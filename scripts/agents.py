"""20201205_Agents_LHA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxIfN96_7dUyEypTynX-sPzZX7X0QxrD
"""

import matplotlib.pyplot as plt

# Standards
import time
import numpy as np
import pandas as pd
import tqdm

# current Logit Model
import statsmodels
import statsmodels.api as sm
import sklearn.metrics

import sklearn as sk 
from agents.Helper import Helper
# Full class definition for the agents.py

class Preparation_Agent:

    def __init__(self, REFIT_df):
        self.input = REFIT_df


    # Stardard Data preprocessing
    def outlier_truncation(self, series, factor=1.5, verbose=0):
    
        q1 = series.quantile(0.25)
        q3 = series.quantile(0.75)
        iqr = q3-q1
    
        lower_bound = q1 - factor*iqr
        upper_bound = q3 + factor*iqr
    
        output = []
        counter = 0
        for item in (tqdm(series, desc=f'[outlier truncation: {series.name}]') if verbose != 0 else series):
            if item > upper_bound:
                output.append(int(upper_bound))
                counter += 1
            elif item < lower_bound:
                output.append(int(lower_bound))
                counter += 1
            else:
                output.append(item)
        print(f'[outlier truncation: {series.name}]: {counter} outliers were truncated.') if verbose != 0 else None 
        return output


    def truncate(self, df, features='all', factor=1.5, verbose=0):
        output = df.copy()
        features = df.select_dtypes(include=['int', 'float']).columns if features == 'all' else features

        for feature in features:
            time.sleep(0.2) if verbose != 0 else None
            row_nn = df[feature] != 0                                                                  # truncate only the values for which the device uses energy
            output.loc[row_nn, feature] = self.outlier_truncation(df.loc[row_nn, feature], factor=factor, verbose=verbose) # Truncatation factor = 1.5 * IQR
            print('\n') if verbose != 0 else None
        return output


    def scale(self, df, features='all', kind='MinMax', verbose=0):
        output = df.copy()
        features = df.select_dtypes(include=['int', 'float']).columns if features == 'all' else features
    
        if kind == 'MinMax':
            from sklearn.preprocessing import MinMaxScaler
        
            scaler = MinMaxScaler()
            output[features] = scaler.fit_transform(df[features])
            print('[MinMaxScaler] Finished scaling the data.') if verbose != 0 else None
        else:
            raise InputError('Chosen scaling method is not available.')
        return output 


    # feature creation
    def get_device_usage(self, df, device, threshold):
        return (df.loc[:, device] > threshold).astype('int')


    def get_last_usage(self, series):

        last_usage = []
        for idx in range(len(series)):
            shift = 1
            if pd.isna(series.shift(periods = 1)[idx]):
                shift = None
            else:
                while series.shift(periods = shift)[idx] == 0:
                    shift += 1
            last_usage.append(shift)
        return last_usage


    def get_last_usages(self, df, features):

        output = pd.DataFrame()
        for feature in features:
            output['periods_since_last_'+feature] = self.get_last_usage(df[feature])
        output.set_index(df.index, inplace=True)
        return output


    def get_activity(self, df, active_appliances, threshold):

        active = pd.DataFrame({appliance: df[appliance] > threshold for appliance in active_appliances})
        return active.apply(any, axis = 1).astype('int')


    def get_time_feature(self, df, features='all'):
    
        functions = {
            'hour': lambda df: df.index.hour, 
            'day_of_week': lambda df: df.index.dayofweek,
            'day_name': lambda df: df.index.day_name().astype('category'),
            'month': lambda df: df.index.month, 
            'month_name': lambda df: df.index.month_name().astype('category')
        }
        if features == 'all':
            output = pd.DataFrame({function[0]: function[1](df) for function in functions.items()})
        else:
            output = pd.DataFrame({function[0]: function[1](df) for function in functions.items() if function[0] in features})
        output.set_index(df.index, inplace=True)
        return output


    def get_time_lags(self, df, features, lags):
    
        output = pd.DataFrame()
        for feature in features:
            for lag in lags:
                output[f'{feature}_lag_{lag}'] = df[feature].shift(periods=lag)
        return output


    def visualize_threshold(self, df, threshold, appliances, figsize=(18,5)):
        # data prep
        for appliance in appliances:
            df[appliance + '_usage'] = self.get_device_usage(df, appliance, threshold)
        df = df.join(self.get_time_feature(df))
        df['activity'] = self.get_activity(df, appliances, threshold)


        usage_cols = [column for column in df.columns if column.endswith('_usage')]
        columns = ['activity'] + usage_cols

        fig, axes = plt.subplots(1,3, figsize=figsize)

        # hour
        hour = df.groupby('hour').mean()[columns]
        hour.plot(ax=axes[0])
        axes[0].set_ylim(-.1, 1.1);
        axes[0].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per hour')

        # week 
        usage_cols = [column for column in df.columns if column.endswith('_usage')]
        week = df.groupby('day_name').mean()[columns]
        week = week.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
        week.plot(ax=axes[1])
        axes[1].set_ylim(-.1, 1.1);
        axes[1].set_xticklabels(['']+list(week.index), rotation=90)
        axes[1].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per day of the week')

        # month
        usage_cols = [column for column in df.columns if column.endswith('_usage')]
        month = df.groupby('month').mean()[columns]
        month.plot(ax=axes[2])
        axes[2].set_ylim(-.1, 1.1);
        axes[2].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per month')


    def validate_thresholds(self, df, thresholds, appliances, figsize=(18,5)):
        
        for threshold in tqdm(thresholds):
            self.visualize_threshold(df, threshold, appliances, figsize)
        time.sleep(0.2)
        print('\n')


    def pipeline_activity(self, df, params):
        
        helper = Helper()
        df = df.copy()
        output = pd.DataFrame()

        # Data cleaning
        df = self.truncate(df, **params['truncate'],)
        df = self.scale(df, **params['scale'])

        # Aggregate to hour level
        df = helper.aggregate(df, **params['aggregate'])

        # Activity feature
        output['activity'] = self.get_activity(df, **params['activity'])
    
        ## Time feature
        output = output.join(self.get_time_feature(df, **params['time']))

        # Activity lags
        output = output.join(self.get_time_lags(output, **params['activity_lag']))

        # Dummy coding
        output = pd.get_dummies(output, drop_first=True)

        return output


    def pipeline_load(self, df, params):
        
        helper = Helper()
        df = df.copy()
        output = pd.DataFrame()

        # Data cleaning
        df = self.truncate(df, **params['truncate'],)
        scaled = self.scale(df, **params['scale'])

        # aggregate
        df = helper.aggregate(df, **params['aggregate'])
        scaled = helper.aggregate(scaled, **params['aggregate'])

        # Get device usage and transform to energy consumption
        for device in params['shiftable_devices']:
            df[device + '_usage'] = self.get_device_usage(scaled, device, **params['device'])
            output[device] = df.apply(lambda timestamp: timestamp[device] * timestamp[device + '_usage'], axis = 1)

        return output, scaled, df


    def pipeline_usage(self, df, params):
        
        helper = Helper()
        df = df.copy()
        output = pd.DataFrame()

        # Data cleaning
        df = self.truncate(df, **params['truncate'],)
        scaled = self.scale(df, **params['scale'])

        # Aggregate to hour level
        scaled = helper.aggregate(scaled, **params['aggregate_hour'])

        # Activity feature
        output['activity'] = self.get_activity(scaled, **params['activity'])

        # Get device usage and transform to energy consumption
        for device in params['shiftable_devices']:
            output[device + '_usage'] = self.get_device_usage(scaled, device, **params['device'])

        # aggregate and convert from mean to binary
        output = helper.aggregate(output, **params['aggregate_day'])
        output = output.apply(lambda x: (x > 0).astype('int'))

        # Last usage
        output = output.join(self.get_last_usages(output, output.columns))
    
        # Time features
        output = output.join(self.get_time_feature(output, **params['time']))

        # lags
        output = output.join(self.get_time_lags(output, ['activity'] + [device+'_usage' for device in params['shiftable_devices']], [1,2,3]))
        output['active_last_2_days'] = ((output.activity_lag_1 == 1) | (output.activity_lag_2 == 1)).astype('int')

        # dummy coding
        output = pd.get_dummies(output, drop_first=True)
        
        return output

class Activity_Agent:

    def __init__(self, activity_input_df):
        self.input = activity_input_df


    def get_Xtest(self, df, date, time_delta='all', target='activity'):
        helper = Helper()
    
        if time_delta == 'all':
            output = df.loc[pd.to_datetime(date):, df.columns != target]
        else:
            df = helper.get_timespan(df, date, time_delta)
            output = df.loc[:, df.columns != target]
        return output


    def get_ytest(self, df, date, time_delta='all', target='activity'):
        helper = Helper()
    
        if time_delta == 'all':
            output = df.loc[pd.to_datetime(date):, target]
        else:
            output = helper.get_timespan(df, date, time_delta)[target]
        return output


    def get_Xtrain(self, df, date, start='2013-11-01', target='activity'):
        if type(start) == int:
            start = pd.to_datetime(date) + pd.Timedelta(days= start)
            start = pd.to_datetime('2013-11-01') if start < pd.to_datetime('2013-11-01') else start
        else:
            start = pd.to_datetime(start)
        end = pd.to_datetime(date) + pd.Timedelta(seconds=-1)
        return df.loc[start:end, df.columns != target]


    def get_ytrain(self, df, date, start='2013-11-01', target='activity'):

        if type(start) == int:
            start = pd.to_datetime(date) + pd.Timedelta(days= start)
            start = pd.to_datetime('2013-11-01') if start < pd.to_datetime('2013-11-01') else start
        else:
            start = pd.to_datetime(start)
        end = pd.to_datetime(date) + pd.Timedelta(seconds=-1)
        return df.loc[start:end, target]


    def train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity'):
        X_train = self.get_Xtrain(df, date, start=train_start, target=target)
        y_train = self.get_ytrain(df, date, start=train_start, target=target)
        X_test = self.get_Xtest(df, date, time_delta=test_delta, target=target)
        y_test = self.get_ytest(df, date, time_delta=test_delta, target=target)
        return X_train, y_train, X_test, y_test


    def fit_smLogit(self, X, y):
        return sm.Logit(y, X).fit(disp=False)


    def fit(self, X, y, model_type):
        if model_type == 'logit':
            model = self.fit_smLogit(X, y)
        else:
            raise InputError('Unknown model type.')
        return model


    def predict(self, model, X):

        if type(model) == statsmodels.discrete.discrete_model.BinaryResultsWrapper:
            y_hat = model.predict(X)
        else:
            raise InputError('Unknown model type.')
        return y_hat


    def auc(self, y_true, y_hat):
        return sklearn.metrics.roc_auc_score(y_true, y_hat)


    def evaluate(self, df, model_type, split_params, predict_start='2014-01-01', predict_end=-1):

        dates = pd.DataFrame(df.index).set_index(df.index)['Time'].apply(lambda date: str(date)[:10]).drop_duplicates()

        predict_start = pd.to_datetime(predict_start)
        predict_end = pd.to_datetime(dates[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)

        dates = dates.loc[predict_start:predict_end]
    
        y_hat_train, y_hat_test = {}, {}
        auc_train, auc_test = {}, {}

        for date in tqdm(dates):
            # train test split
            X_train, y_train, X_test, y_test = self.train_test_split(df, date, **split_params)

            # fit model
            model = self.fit(X_train, y_train, model_type)

            # predict
            y_hat_train.update({date: self.predict(model, X_train)})
            y_hat_test.update({date: self.predict(model, X_test)})

            # evaluate
            auc_train.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})
            auc_test.update({date: self.auc(y_test, list(y_hat_test.values())[-1])})

        return y_hat_train, y_hat_test, auc_train, auc_test


    def plot_model_performance(self, auc_train, auc_test, ylim='default'):
    
        plt.plot(list(auc_train.keys()), list(auc_train.values()))
        plt.plot(list(auc_train.keys()), list(auc_test.values()))
        plt.xticks(list(auc_train.keys()), ' ');
        plt.ylim(ylim) if ylim != 'default' else None


    def evaluate_2(self, df, model_type, split_params, predict_start='2014-01-01', predict_end=-1):
        
        dates = pd.DataFrame(df.index).set_index(df.index)['Time'].apply(lambda date: str(date)[:10]).drop_duplicates()

        predict_start = pd.to_datetime(predict_start)
        predict_end = pd.to_datetime(dates[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)

        dates = dates.loc[predict_start:predict_end]
    
        y_true = []
        y_hat_train = {}
        y_hat_test = []
        auc_train_dict = {}
        auc_test = []

        for date in tqdm(dates):
            # train test split
            #train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity')
            X_train, y_train, X_test, y_test = self.train_test_split(df, date, **split_params)

            # fit model
            model = self.fit(X_train, y_train, model_type)

            # predict
            y_hat_train.update({date: self.predict(model, X_train)})
            y_hat_test += list(self.predict(model, X_test))

            # evaluate train data
            auc_train_dict.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})
        
            y_true += list(y_test)
    
        auc_test = self.auc(y_true, y_hat_test)
        auc_train = np.mean(list(auc_train_dict.values()))

        return auc_train, auc_test, auc_train_dict


    def pipeline(self, df, date, model_type, split_params):
        # train test split
        X_train, y_train, X_test, y_test = self.train_test_split(df, date, **split_params)

        # fit model
        model = self.fit(X_train, y_train, model_type)

        # predict
        return self.predict(model, X_test)

# Load agent
class Load_Agent:

  def __init__(self, load_input_df):
    self.input = load_input_df


  def prove_start_end_date(self, df, date):
    import pandas as pd

    start_date = (df.index[0]).strftime('%Y-%m-%d')
    end_date = date

    if len(df[start_date]) < 24:
      start_date = (pd.to_datetime(start_date) + pd.Timedelta(days = 1)).strftime('%Y-%m-%d')
      df = df[start_date:end_date]
    else: 
      df = df[:end_date]

    if len(df[end_date]) < 24:
      end_new = (pd.to_datetime(end_date) - pd.Timedelta(days = 1)).strftime('%Y-%m-%d')
      df = df[:end_new]
    else: 
      df = df[:end_date]
    return df


  def df_yesterday_date(self, df, date):
    import pandas as pd

    yesterday = (pd.to_datetime(date) - pd.Timedelta(days = 1)).strftime('%Y-%m-%d')
    return df[:yesterday]


  def load_profile_raw(self, df, shiftable_devices):
    import pandas as pd

    hours = [] 
    for hour in range(1,25):
      hours.append('h' + str(hour))
    df_hours = {}

    for idx, appliance in enumerate(shiftable_devices): # delete enumerate if we do not need integers indexes of devices
      df_hours[appliance] = pd.DataFrame(index = None, columns = hours)
      column = df[appliance]

      for i in range(len(column)):

        if (i == 0) and (column[0] > 0):
          df_hours[appliance].loc[0, 'h' + str(1)] = column[0]

        elif (column[i - 1] == 0) and (column[i] > 0):
          for j in range(0, 24): 
            if (i + j) < len(column):
              if (column[i + j] > 0):
                df_hours[appliance].loc[i, 'h' + str(j + 1)] = column[i + j]

    return df_hours


  def load_profile_cleaned(self, df_hours):

    for app in df_hours.keys():
      for i in df_hours[app].index:
        for j in df_hours[app].columns:
          if np.isnan(df_hours[app].loc[i, j]):
            df_hours[app].loc[i, j:] = 0 
    return df_hours


  def load_profile(self, df_hours, shiftable_devices): 
    import pandas as pd

    hours = df_hours[shiftable_devices[0]].columns
    loads = pd.DataFrame(columns = hours)

    for app in df_hours.keys():
      app_mean = df_hours[app].apply(lambda x: x.mean(), axis = 0)
      for hour in app_mean.index:
        loads.loc[app, hour] = app_mean[hour]

    loads = loads.fillna(0)   
    return loads


  def pipeline(self, df, date, shiftable_devices):
        # kann mann dann self benutzen?
        df = self.prove_start_end_date(df, date)
        df = self.df_yesterday_date(df, date)
        df_hours = self.load_profile_raw(df, shiftable_devices)
        df_hours = self.load_profile_cleaned(df_hours)
        loads = self.load_profile(df_hours, shiftable_devices)

        return loads

class Usage_Agent:
    def __init__(self, input_df, device):
        self.input = input_df
        self.device = device


    def train_test_split(self, df, date, train_start='2013-11-01'):
      select_vars =  [self.device + '_usage', self.device+ '_usage_lag_1', self.device+ '_usage_lag_2',	'active_last_2_days']
      df = df[select_vars]
      X_train = df.loc[train_start:date, df.columns != self.device + '_usage']
      y_train = df.loc[train_start:date, df.columns == self.device + '_usage']
      X_test  = df.loc[date, df.columns != self.device + '_usage']
      y_test  = df.loc[date , df.columns == self.device + '_usage']
      return X_train, y_train, X_test, y_test

#################### MINE ###################################################################################################
    def fit_skModels(self,model, X, y):

        return  

    def fit_smLogit(self, X, y):
      return sm.Logit(y, X).fit(disp=False)

    def fit(self, X, y, model_type):
      if model_type == 'logit':
          model = self.fit_smLogit(X, y)
      else:
        raise InputError('Unknown model type.')
      return model

    def predict(self, model, X):
      X = np.array(X)

      if type(model) == statsmodels.discrete.discrete_model.BinaryResultsWrapper:
          y_hat = model.predict(X)
      else:
          raise InputError('Unknown model type.')
      return y_hat

    def pipeline(self, df, date, model_type, train_start):
      X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)
      model = self.fit(X_train, y_train, model_type)
      return self.predict(model, X_test)


    def auc(self, y_true, y_hat):
      import sklearn.metrics
      return sklearn.metrics.roc_auc_score(y_true, y_hat)


    def evaluate(self, df, model_type, train_start, predict_start='2014-01-01', predict_end=-1):
      dates = pd.DataFrame(df.index)
      dates = dates.set_index(df.index)['Time']
      predict_start = pd.to_datetime(predict_start)
      predict_end = pd.to_datetime(dates.iloc[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)
      dates = dates.loc[predict_start:predict_end]
      y_true = []
      y_hat_train = {}
      y_hat_test = []
      auc_train_dict = {}
      auc_test = []

      for date in dates.index:
          # train test split
          #train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity')
          X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)

          # fit model
          model = self.fit(X_train, y_train, model_type)

          # predict
          y_hat_train.update({date: self.predict(model, X_train)})
          y_hat_test += list(self.predict(model, X_test))

          # evaluate train data
          auc_train_dict.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})
        
          y_true += list(y_test)
    
      auc_test = self.auc(y_true, y_hat_test)
      auc_train = np.mean(list(auc_train_dict.values()))

      return auc_train, auc_test, auc_train_dict

# Price Agent
class Price_Agent:
  def __init__(self, Prices_df):
    self.input = Prices_df

  def return_day_ahead_prices(self, Date):
    import pandas as pd
    range = pd.date_range(start=Date, freq = "H", periods=48)
    prices = self.input.loc[range]
    return prices

# BRAUCH MAN NICHT ?!!
#from agents import Activity_Agent, Usage_Agent, Load_Agent, Price_Agent
#from agents import Activity_Agent, Usage_Agent, Load_Agent, Price_Agent
class Recommendation_Agent:

    def __init__(self, activity_input, usage_input, load_input, price_input, shiftable_devices):
        self.activity_input = activity_input
        self.usage_input = usage_input
        self.load_input = load_input
        self.price_input = price_input
        self.shiftable_devices = shiftable_devices

        self.Activity_Agent = Activity_Agent(activity_input)

        #create dicionnary with Usage_Agent for each device
        self.Usage_Agent = {name: Usage_Agent(usage_input , name)  for name in shiftable_devices}

        self.Load_Agent = Load_Agent(load_input)
        self.Price_Agent = Price_Agent(price_input)

    def electricity_prices_from_start_time(self, date):
        prices_48 = self.Price_Agent.return_day_ahead_prices(date)
        prices_from_start_time = pd.DataFrame()

        for i in range(24):
          prices_from_start_time["Price_at_H+"+ str(i)] = prices_48.shift(-i)

        #delete last 24 hours
        prices_from_start_time = prices_from_start_time[:-24]
        return prices_from_start_time

    def cost_by_starting_time(self, date, device):
        #get electriciy prices following every device starting hour with previously defined function
        prices = self.electricity_prices_from_start_time(date)

        #build up table with typical load profile repeated for every hour (see Load_Agent)
        device_load = self.Load_Agent.pipeline(self.load_input, date, self.shiftable_devices).loc[device]
        device_load = pd.concat([device_load] * 24, axis= 1)

        #multiply both tables and aggregate costs for each starting hour
        costs = np.array(prices)*np.array(device_load)
        costs = np.sum(costs, axis = 0)

        #return an array of size 24 containing the total cost at each staring hour.
        return costs

    def recommend_by_device(self, date, device, activity_prob_threshold, usage_prob_threshold):

        #add split params as input
        # IN PARTICULAR --> Specify date to start training
        split_params =  {'train_start': '2013-11-01', 'test_delta': {'days':1, 'seconds':-1}, 'target': 'activity'}

        #compute costs by launching time:
        costs = self.cost_by_starting_time(date, device)

        #compute activity
        activity_probs = self.Activity_Agent.pipeline(self.activity_input, date, 'logit', split_params)

        #set values above threshold to 1. Values below to Inf 
        #(vector will be multiplied by costs, so that hours of little activity likelihood get cost = Inf)
        activity_probs = np.where(activity_probs>= activity_prob_threshold, 1, float("Inf"))

        #add a flag in case all hours have likelihood smaller than threshold
        no_recommend_flag_activity = 0
        if np.min(activity_probs) == float("Inf"):
          no_recommend_flag_activity = 1

        # compute cheapest hour from likely ones
        best_hour = np.argmin(np.array(costs) * np.array(activity_probs))

        # compute likelihood of usage:
        usage_prob = self.Usage_Agent[device].pipeline(self.usage_input , date, "logit", split_params["train_start"]) 
        no_recommend_flag_usage = 0
        if usage_prob < usage_prob_threshold :
          no_recommend_flag_usage = 1

        return {"recommendation_date": [date], "device": [device] ,"best_launch_hour": [best_hour] , "no_recommend_flag_activity" : [no_recommend_flag_activity], "no_recommend_flag_usage" : [no_recommend_flag_usage] }

    def pipeline(self, date, activity_prob_threshold, usage_prob_threshold):
      recommendations_by_device = self.recommend_by_device(date, self.shiftable_devices[0], activity_prob_threshold, usage_prob_threshold)
      recommendations_table = pd.DataFrame.from_dict(recommendations_by_device)

      for device in self.shiftable_devices[1:]:
        recommendations_by_device = self.recommend_by_device(date, device, activity_prob_threshold, usage_prob_threshold)
        recommendations_table = recommendations_table.append(pd.DataFrame.from_dict(recommendations_by_device))
      return recommendations_table