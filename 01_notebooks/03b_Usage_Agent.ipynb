t{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03b_Usage_Agent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python376jvsc74a57bd0071f8e175e610456ffef36eadb93170812a404d84379bd5715daa9aecae9ca07","display_name":"Python 3.7.6 64-bit ('base': conda)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2DRnXoeZJqNX"},"source":["# **Device Usage Agent**"]},{"cell_type":"markdown","metadata":{"id":"HwtXD6qiJx3r"},"source":["The **Device Usage Agent's** functionnality is to estimate the probability that a particular device will be used on the following day. Within the general recommendation framework, this function is used in order to limit unnecessary recommendations that could irritate the user. Whenever a device is unlikely to be used on the next day (estimated likelihood below a certain threshold), no recommendation will be made.\n","\n","In the present notebook, we will describe how these probabilities are estimated in detail and define the **Usage_Agent** class that will be integrated into the recommendation agent.\n","\n","The usage_agent will use a ML-algorithm on features extracted from the household's electricity consumption data in order to predict the likelihood of use of devices on the next day. For instance, at a given day t-1, it will use all available consumption data until day t-1 in order to predict device usage on day t. The features we will use can be divided into 3 categories: \n","1. Whether activity has been detected in the house in the preceding days (activity detected by electricity consumption)\n","2. Whether the to-be-prediced-device has been used in the previous days\n","3. Time dummies.\n","\n","Given the limited number of observations for each household, we will need to restrict the complexity of the ML-Algorithm in use. This is the reason why we will use a logit model with a limited number of features."]},{"cell_type":"markdown","metadata":{"id":"v1xY9gp_4KqO"},"source":["## **1. Load And Preprocess Data**"]},{"cell_type":"markdown","metadata":{"id":"wNx0bpISOLUE"},"source":["This part's only purpose is to load the data used in the  Usage_Agent. This process is described in detail in the Preparation_Agent. \n","\n","**Note: When computing the script with another Household than Household 1 you might need to adapt some parameters**"]},{"cell_type":"markdown","metadata":{"id":"17JxNkMHKa27"},"source":["### **1.1 Initialize And Load Python Scripts**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4PY4GQ73XlD","executionInfo":{"status":"ok","timestamp":1607623203320,"user_tz":-60,"elapsed":876,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"a49e04b6-d03c-44cc-c6ce-70ef1908f99a"},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","#DATA_PATH = '/content/drive/MyDrive/T4_Recommendation-system-for-demand-response-and-load-shifting/02_data/'\n","\n","# load py scripts to Google colab\n","#!cp /content/drive/MyDrive/T4_Recommendation-system-for-demand-response-and-load-shifting/03_scripts/helper_functions.py .\n","#!cp /content/drive/MyDrive/T4_Recommendation-system-for-demand-response-and-load-shifting/03_scripts/agents.py ."],"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","import sklearn as sk \n","import statsmodels\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from Helper import Helper\n","from PreparationAgent import Preparation_Agent"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#sys.path.append('/Users/jana1394/Desktop/Studium/Master/SS2021/APA/xai_energy_efficient_smart_home/scripts/')\n","sys.path.append('/Users/jana1394/Desktop/Studium/Master/SS2021/APA/xai_energy_efficient_smart_home/agents/')\n","sys.path.append('/Users/jana1394/Desktop/Studium/Master/SS2021/APA/xai_energy_efficient_smart_home/02_data')"]},{"cell_type":"code","metadata":{"id":"XSEU7zeN3udE","executionInfo":{"status":"ok","timestamp":1607623227444,"user_tz":-60,"elapsed":24993,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["\n","helper = Helper()\n","DATA_PATH = '/Users/jana1394/Desktop/Studium/Master/SS2021/APA/xai_energy_efficient_smart_home/02_data/'\n","# load household data\n","household = helper.load_household(DATA_PATH, 1)\n","active_appliances = ['Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site']\n","\n","# loading the household data for household 1\n","household = helper.load_household(DATA_PATH, 1)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuV9dYlD4Vkv"},"source":["### **1.2 Set Parameters For Pre-processing Step**"]},{"cell_type":"code","metadata":{"id":"qw4N4Sq45663","executionInfo":{"status":"ok","timestamp":1607623227445,"user_tz":-60,"elapsed":24987,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["truncation_params = {\n","    'features': 'all', \n","    'factor': 1.5, \n","    'verbose': 0\n","}\n","\n","scale_params = {\n","    'features': 'all', \n","    'kind': 'MinMax', \n","    'verbose': 0\n","}\n","\n","aggregate_params = {\n","    'resample_param': '60T'\n","}\n","aggregate_params24_H = {\n","    'resample_param': '24H'\n","}\n","\n","\n","activity_params = {\n","    'active_appliances': ['Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site'],\n","    'threshold': .15\n","}\n","\n","time_params = {\n","    'features': ['hour', 'day_name']\n","}\n","\n","activity_lag_params = {\n","    'features': ['activity'],\n","    'lags': [24, 48, 72]\n","}\n","\n","shiftable_devices = {'Dishwasher', 'Washing Machine'}\n","\n","device = {\n","    'threshold' : .15}\n","\n","activity_pipe_params = {\n","    'truncate': truncation_params,\n","    'scale': scale_params,\n","    'activity': activity_params,\n","    'aggregate_hour': aggregate_params,\n","    'aggregate_day': aggregate_params24_H,\n","    'time': time_params,\n","    'activity_lag': activity_lag_params,\n","    'shiftable_devices' : shiftable_devices,\n","    'device': device\n","}\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsQY8RSZKsqL"},"source":["### **1.3 Pre-process Data For Input In Device_Usage Agent**"]},{"cell_type":"code","metadata":{"id":"AOK_zrFr4aK9","colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"status":"ok","timestamp":1607623276471,"user_tz":-60,"elapsed":73997,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"239cbff3-d8d0-4a8e-a517-d8e30f8aae3e"},"source":["# calling the preparation pipeline\n","prep = Preparation_Agent(household)\n","df = prep.pipeline_usage(household, activity_pipe_params)\n","\n","#display all potential variables for predicting device usage likelihood\n","df.head()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            activity  Dishwasher_usage  Washing Machine_usage  \\\n","Time                                                            \n","2013-10-09         1                 0                      0   \n","2013-10-10         1                 1                      0   \n","2013-10-11         0                 0                      0   \n","2013-10-12         0                 0                      0   \n","2013-10-13         0                 0                      0   \n","\n","            periods_since_last_activity  periods_since_last_Dishwasher_usage  \\\n","Time                                                                           \n","2013-10-09                          NaN                                  NaN   \n","2013-10-10                          1.0                                  2.0   \n","2013-10-11                          1.0                                  1.0   \n","2013-10-12                          2.0                                  2.0   \n","2013-10-13                          3.0                                  3.0   \n","\n","            periods_since_last_Washing Machine_usage  hour  activity_lag_1  \\\n","Time                                                                         \n","2013-10-09                                       NaN     0             NaN   \n","2013-10-10                                       2.0     0             1.0   \n","2013-10-11                                       3.0     0             1.0   \n","2013-10-12                                       4.0     0             0.0   \n","2013-10-13                                       5.0     0             0.0   \n","\n","            activity_lag_2  activity_lag_3  ...  Washing Machine_usage_lag_1  \\\n","Time                                        ...                                \n","2013-10-09             NaN             NaN  ...                          NaN   \n","2013-10-10             NaN             NaN  ...                          0.0   \n","2013-10-11             1.0             NaN  ...                          0.0   \n","2013-10-12             1.0             1.0  ...                          0.0   \n","2013-10-13             0.0             1.0  ...                          0.0   \n","\n","            Washing Machine_usage_lag_2  Washing Machine_usage_lag_3  \\\n","Time                                                                   \n","2013-10-09                          NaN                          NaN   \n","2013-10-10                          NaN                          NaN   \n","2013-10-11                          0.0                          NaN   \n","2013-10-12                          0.0                          0.0   \n","2013-10-13                          0.0                          0.0   \n","\n","            active_last_2_days  day_name_Monday  day_name_Saturday  \\\n","Time                                                                 \n","2013-10-09                   0                0                  0   \n","2013-10-10                   1                0                  0   \n","2013-10-11                   1                0                  0   \n","2013-10-12                   1                0                  1   \n","2013-10-13                   0                0                  0   \n","\n","            day_name_Sunday  day_name_Thursday  day_name_Tuesday  \\\n","Time                                                               \n","2013-10-09                0                  0                 0   \n","2013-10-10                0                  1                 0   \n","2013-10-11                0                  0                 0   \n","2013-10-12                0                  0                 0   \n","2013-10-13                1                  0                 0   \n","\n","            day_name_Wednesday  \n","Time                            \n","2013-10-09                   1  \n","2013-10-10                   0  \n","2013-10-11                   0  \n","2013-10-12                   0  \n","2013-10-13                   0  \n","\n","[5 rows x 23 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>activity</th>\n      <th>Dishwasher_usage</th>\n      <th>Washing Machine_usage</th>\n      <th>periods_since_last_activity</th>\n      <th>periods_since_last_Dishwasher_usage</th>\n      <th>periods_since_last_Washing Machine_usage</th>\n      <th>hour</th>\n      <th>activity_lag_1</th>\n      <th>activity_lag_2</th>\n      <th>activity_lag_3</th>\n      <th>...</th>\n      <th>Washing Machine_usage_lag_1</th>\n      <th>Washing Machine_usage_lag_2</th>\n      <th>Washing Machine_usage_lag_3</th>\n      <th>active_last_2_days</th>\n      <th>day_name_Monday</th>\n      <th>day_name_Saturday</th>\n      <th>day_name_Sunday</th>\n      <th>day_name_Thursday</th>\n      <th>day_name_Tuesday</th>\n      <th>day_name_Wednesday</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-10-09</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-10-10</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-10-11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-10-12</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-10-13</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"aL6F5tn6dTsf"},"source":["## **2.  Constructing the Device Usage Agent**"]},{"cell_type":"markdown","metadata":{"id":"0aU7AQfEYSS6"},"source":["### **2.1 Initialize Agent**"]},{"cell_type":"markdown","metadata":{"id":"T09RUToXR93t"},"source":["First we define the **Usage_Agent class**. It takes as input the data generated by the prep.pipeline_usage function computed above, and the name of the device for which predictions should be made (e.g \"Washing Machine\", \"Dishwasher\"etc...)."]},{"cell_type":"code","metadata":{"id":"hZgoXyrpdVfn","executionInfo":{"status":"ok","timestamp":1607623276472,"user_tz":-60,"elapsed":73988,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["class Usage_Agent:\n","    import pandas as pd\n","\n","    def __init__(self, input_df, device):\n","        self.input = input_df\n","        self.device = device"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gxZlevgn5nrr"},"source":["Here we initialize the agent for the device \"Dishwasher\""]},{"cell_type":"code","metadata":{"id":"Uh-a4Lmi5l_v","executionInfo":{"status":"ok","timestamp":1607623276473,"user_tz":-60,"elapsed":73985,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["import pandas as pd\r\n","Usage_Agent_i = Usage_Agent(df, \"Dishwasher\") "],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dlu2qQ8BYXl9"},"source":["### **2.2 Train_test_split function**"]},{"cell_type":"markdown","metadata":{"id":"AQ_GrCnOVc2X"},"source":["The number of data points available to make a prediction for day t increases by one, each time t increases by one. Therefore, we define a custom train_test_split function that automatically puts all data available until day t-1 (incl.) into the training set. The Data for day t (= prediction day) comes into the test set.\n","\n","In order to limit over-fitting the function also filters out the number of features to be taken into account to train the model. Here these are the following:\n","\n","1. Indicator of device usage at day t-1\n","2. Indicator of device usage at day t-2\n","3. Indicator of activity in the household in the past two days\n"]},{"cell_type":"code","metadata":{"id":"UYOZSfdNVKCd","executionInfo":{"status":"ok","timestamp":1607623276474,"user_tz":-60,"elapsed":73981,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["#date: the day of prediction\n","#train start: the day from which training starts\n","def train_test_split(self, df, date, train_start='2013-11-01'):\n","    #restrict number of variables\n","    select_vars =  [self.device + '_usage', self.device+ '_usage_lag_1', self.device+ '_usage_lag_2',\t'active_last_2_days']\n","    df = df[select_vars]\n","    #spli train and test\n","    X_train = df.loc[train_start:date, df.columns != self.device + '_usage']\n","    y_train = df.loc[train_start:date, df.columns == self.device + '_usage']\n","    X_test  = df.loc[date, df.columns != self.device + '_usage']\n","    y_test  = df.loc[date , df.columns == self.device + '_usage']\n","    return X_train, y_train, X_test, y_test\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'train_test_split', train_test_split)\n","del train_test_split "],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BeEm4w6ZP1n"},"source":["Ouput:"]},{"cell_type":"code","metadata":{"id":"1eQ_lkg_l9-C","executionInfo":{"status":"ok","timestamp":1607623276474,"user_tz":-60,"elapsed":73978,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":[" X_train, y_train, X_test, y_test = Usage_Agent_i.train_test_split(df, \"2014-11-01\", train_start='2013-11-01')"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"UCSNjGPUX3aT","executionInfo":{"status":"ok","timestamp":1607623276476,"user_tz":-60,"elapsed":73976,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"8c49dba4-26f3-4a25-d1b6-0b69675816ba"},"source":["X_train"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            Dishwasher_usage_lag_1  Dishwasher_usage_lag_2  active_last_2_days\n","Time                                                                          \n","2013-11-01                     1.0                     0.0                   1\n","2013-11-02                     0.0                     1.0                   1\n","2013-11-03                     0.0                     0.0                   1\n","2013-11-04                     1.0                     0.0                   1\n","2013-11-05                     0.0                     1.0                   1\n","...                            ...                     ...                 ...\n","2014-10-28                     0.0                     0.0                   1\n","2014-10-29                     0.0                     0.0                   1\n","2014-10-30                     1.0                     0.0                   1\n","2014-10-31                     0.0                     1.0                   1\n","2014-11-01                     0.0                     0.0                   1\n","\n","[366 rows x 3 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dishwasher_usage_lag_1</th>\n      <th>Dishwasher_usage_lag_2</th>\n      <th>active_last_2_days</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-11-01</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-11-02</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-11-03</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-11-04</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-11-05</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014-10-28</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014-10-29</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014-10-30</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014-10-31</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014-11-01</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>366 rows × 3 columns</p>\n</div>"},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iFxM9dUX693","executionInfo":{"status":"ok","timestamp":1607623276477,"user_tz":-60,"elapsed":73968,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"e46df0a4-d69e-4c47-daa2-3d1996eaf48b"},"source":["X_test"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dishwasher_usage_lag_1    0.0\n","Dishwasher_usage_lag_2    0.0\n","active_last_2_days        1.0\n","Name: 2014-11-01 00:00:00, dtype: float64"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"mcO6tljKX9wf","executionInfo":{"status":"ok","timestamp":1607623276478,"user_tz":-60,"elapsed":73963,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"d42133da-e6be-457a-e269-7ef0d08f35e0"},"source":["y_train"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            Dishwasher_usage\n","Time                        \n","2013-11-01                 0\n","2013-11-02                 0\n","2013-11-03                 1\n","2013-11-04                 0\n","2013-11-05                 0\n","...                      ...\n","2014-10-28                 0\n","2014-10-29                 1\n","2014-10-30                 0\n","2014-10-31                 0\n","2014-11-01                 1\n","\n","[366 rows x 1 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dishwasher_usage</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-11-01</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-11-02</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-11-03</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2013-11-04</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-11-05</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014-10-28</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-10-29</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014-10-30</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-10-31</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-11-01</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>366 rows × 1 columns</p>\n</div>"},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIqYYx7AX_7c","executionInfo":{"status":"ok","timestamp":1607623276479,"user_tz":-60,"elapsed":73957,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"6b956628-c21b-4aa7-fa70-a471a0fa1404"},"source":["y_test"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dishwasher_usage    1.0\n","Name: 2014-11-01 00:00:00, dtype: float64"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"Yp_Q2mbNq7SI"},"source":["### **2.3 Fitting Models**"]},{"cell_type":"markdown","metadata":{"id":"901Rnq25Y58c"},"source":["Now that we have the function to perform the split-sampling we can fit the model on training data. For that purpose, we define a Logit-fitting function as follows:"]},{"cell_type":"code","metadata":{"id":"IuwCUQb9q9tH","executionInfo":{"status":"ok","timestamp":1607623276480,"user_tz":-60,"elapsed":73953,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["def fit_smLogit(self, X, y):\n","    import statsmodels.api as sm\n","    return sm.Logit(y, X).fit(disp=False)\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'fit_smLogit', fit_smLogit)\n","del fit_smLogit \n","\n","def fit(self, X, y, model_type):\n","    if model_type == 'logit':\n","        model = self.fit_smLogit(X, y)\n","    else:\n","        raise InputError('Unknown model type.')\n","    return model\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'fit', fit)\n","del fit"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMxMWE5zZ1PE"},"source":["Using this function on the training split, we can train our first model:"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[],"source":["def skModels(self,model, X, y):\n","         # models we want to try\n","    names = [\"knn\", \"linear svm\", \n","    \"rbv svm\", \"gaussian process\",\"descision tree\", \"random forest\", \n","    \"nn\", \"ada boost\",\"nb\", \"qda\", \"logit\"]\n","    classifiers = [ KNeighborsClassifier(3),\n","                    SVC(kernel=\"linear\", C=0.025),\n","                    SVC(gamma=2, C=1),\n","                    GaussianProcessClassifier(1.0 * RBF(1.0)),\n","                    DecisionTreeClassifier(max_depth=5),\n","                    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n","                    MLPClassifier(alpha=1, max_iter=1000),\n","                    AdaBoostClassifier(),\n","                    GaussianNB(),\n","                    QuadraticDiscriminantAnalysis(),\n","                    LogisticRegression()]\n","    new_models = dict(zip(names,classifiers))\n","    if model in new_models.keys():\n","        fitted_model = new_models[model].fit(X,y.values.ravel())\n","        return fitted_model  \n","    else:\n","        return\n"," #add to Activity agent\n","setattr(Usage_Agent, 'skModels', skModels)\n","del skModels \n"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["/Users/jana1394/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n  warnings.warn(\"Variables are collinear\")\n"]}],"source":["usage = Usage_Agent(df, \"Dishwasher\")\n","knn = usage.skModels('knn', X_train, y_train)\n","l_svm = usage.skModels('linear svm', X_train, y_train)\n","rbv_svm = usage.skModels('rbv svm', X_train, y_train)\n","gp = usage.skModels('gaussian process', X_train, y_train)\n","dT = usage.skModels('descision tree', X_train, y_train)\n","rf = usage.skModels('random forest', X_train, y_train) \n","nn = usage.skModels('nn', X_train, y_train)\n","ada = usage.skModels('ada boost', X_train, y_train)\n","nb = usage.skModels('nb', X_train, y_train)\n","qda = usage.skModels('qda', X_train, y_train)\n","logit =usage.skModels('logit', X_train, y_train)\n"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["logit"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38OdZOcTrve-","executionInfo":{"status":"ok","timestamp":1607623277488,"user_tz":-60,"elapsed":74957,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"d79773c9-60b9-4f27-8dba-aa67c27e07b5"},"source":["usage = Usage_Agent(df, \"Dishwasher\")\n","model = usage.fit(X_train, y_train, 'logit')\n","print(model.summary())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["                           Logit Regression Results                           \n","==============================================================================\n","Dep. Variable:       Dishwasher_usage   No. Observations:                  366\n","Model:                          Logit   Df Residuals:                      363\n","Method:                           MLE   Df Model:                            2\n","Date:                Thu, 10 Dec 2020   Pseudo R-squ.:                -0.08789\n","Time:                        18:01:17   Log-Likelihood:                -215.79\n","converged:                       True   LL-Null:                       -198.36\n","Covariance Type:            nonrobust   LLR p-value:                     1.000\n","==========================================================================================\n","                             coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------------------\n","Dishwasher_usage_lag_1    -1.3136      0.359     -3.656      0.000      -2.018      -0.609\n","Dishwasher_usage_lag_2    -0.3116      0.296     -1.053      0.292      -0.892       0.269\n","active_last_2_days        -0.5561      0.172     -3.241      0.001      -0.892      -0.220\n","==========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dishwasher_usage_lag_1    0.0\n","Dishwasher_usage_lag_2    0.0\n","active_last_2_days        1.0\n","Name: 2014-11-01 00:00:00, dtype: float64"]},"metadata":{},"execution_count":55}],"source":["X_test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"a0iwlva2Hy-n","executionInfo":{"status":"ok","timestamp":1607623277490,"user_tz":-60,"elapsed":74954,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"7d2d46a3-c8ce-4345-e153-83c3ec54b4ce"},"source":["y_train"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dishwasher_usage</th>\n","    </tr>\n","    <tr>\n","      <th>Time</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2013-11-01</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2013-11-02</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2013-11-03</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2013-11-04</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2013-11-05</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2014-10-28</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2014-10-29</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2014-10-30</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2014-10-31</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2014-11-01</th>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>366 rows × 1 columns</p>\n","</div>"],"text/plain":["            Dishwasher_usage\n","Time                        \n","2013-11-01                 0\n","2013-11-02                 0\n","2013-11-03                 1\n","2013-11-04                 0\n","2013-11-05                 0\n","...                      ...\n","2014-10-28                 0\n","2014-10-29                 1\n","2014-10-30                 0\n","2014-10-31                 0\n","2014-11-01                 1\n","\n","[366 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"Y_my8fyn4d-k"},"source":["Once the model is fitted to the training data, a prediction can be made for the test day. This prediction function is defined in the following:"]},{"cell_type":"code","metadata":{"id":"HVtiJ92utWbo","executionInfo":{"status":"ok","timestamp":1607623277491,"user_tz":-60,"elapsed":74947,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["def predict(self, model, X):\n","    import statsmodels\n","    import numpy as np\n","    X = np.array(X)\n","\n","    if type(model) == statsmodels.discrete.discrete_model.BinaryResultsWrapper:\n","        y_hat = model.predict(X)\n","    else:\n","        raise InputError('Unknown model type.')\n","    return y_hat\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'predict', predict)\n","del predict"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbbWXy2Ftd5v","executionInfo":{"status":"ok","timestamp":1607623277492,"user_tz":-60,"elapsed":74944,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"b9a38e75-e08e-42fc-b694-f5434f9f7705"},"source":["#compute prediction at day t (see date used for split sampling)\n","import numpy as np\n","y_hat = usage.predict(model, X_test)\n","y_hat"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.36444631])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["def skpredict(self, model, X):\n","    X = np.array(X).reshape(-1,3)\n","    y_hat = model.predict(X)\n","   \n","    return X, y_hat\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'skpredict', skpredict)\n","del skpredict"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'predict'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-135-78bd33c273dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-130-aebcd5cddf4c>\u001b[0m in \u001b[0;36mskpredict\u001b[0;34m(self, model, X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mskpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"]}],"source":["y_hat_new = usage.skpredict(logit, X_test)"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0])"]},"metadata":{},"execution_count":133}],"source":["y_hat_new"]},{"cell_type":"markdown","metadata":{"id":"PLOHQvFv_GXI"},"source":["### **2.4 Pipeline**"]},{"cell_type":"markdown","metadata":{"id":"uuQrieip1S6X"},"source":["Finally, we wrap up all the previously defined functions into the **pipeline** function. This allows to generate a prediction by simply inputting :\r\n","* the pre-processed usage data\r\n","* the prediction date\r\n","* the model type (limited to logit for now)\r\n","* the date at which the model has started to train\r\n"]},{"cell_type":"code","metadata":{"id":"_i8kTixg_Nvu","executionInfo":{"status":"ok","timestamp":1607623277492,"user_tz":-60,"elapsed":74939,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["def pipeline(self, df, date, model_type, train_start):\n","    X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n","\n","    # fit model\n","    model = self.fit(X_train, y_train, model_type)\n","\n","    # predict\n","    return self.predict(model, X_test)\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'pipeline', pipeline)\n","del pipeline"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAexapts5BYv"},"source":["A prediction for the \"2013-12-08\" based on the data starting on the '2013-11-01' can finally be made for the device with which we initialized the class (here: \"Dishwasher\")"]},{"cell_type":"code","metadata":{"id":"CXUmgFdK_Z03","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607623277493,"user_tz":-60,"elapsed":74936,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"dd3f42a0-89fe-437d-b00f-cde1b4bb9030"},"source":["date = \"2013-12-08\"\n","train_start = '2013-11-01'\n","usage.pipeline(df, date, 'logit', train_start)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.55439791])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"F5cE1DjO6ojI"},"source":["### **2.5 Model Evaluation**\r\n","\r\n","Finally, we want to assess the accuracy of our model before using it in the recommendation agent. \r\n","\r\n","A drawback to our approach is that we are not able to apply conventional model evaluation techniques to our model. We will train our model for each day to account for newly available information. Hence, we have different train and test sets for each day and for each day different performance metric based on the respective data sets. Therefore, we created our own evaluation function. \r\n","\r\n","Our evaluation function will build a model, fit the model and predict the target for each day for a given prediction period. For each day and fitted model it will calculate a performance metric on the train data. We chose the Area Under the Receiver Operating Characteristic Curve (AUC) as performance metric for our binary classification task. As in our case the test data is only the current date to be predicted, we calculate the test AUC over the usage probabilities of all day after all days have been predicted. To summarize the train AUC in one score, we apply an average over all calculated train AUC scores (Note: This approach is the same as for the activity predictions)."]},{"cell_type":"code","metadata":{"id":"GumQPIhDOM5m","executionInfo":{"status":"ok","timestamp":1607623277494,"user_tz":-60,"elapsed":74932,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["def auc(self, y_true, y_hat):\n","    import sklearn.metrics\n","    return sklearn.metrics.roc_auc_score(y_true, y_hat)\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'auc', auc)\n","del auc\n","\n","def evaluate(self, df, model_type, train_start, predict_start='2014-01-01', predict_end=-1):\n","    import pandas as pd\n","    import numpy as np\n","    dates = pd.DataFrame(df.index)\n","    dates = dates.set_index(df.index)['Time']\n","    predict_start = pd.to_datetime(predict_start)\n","    predict_end = pd.to_datetime(dates.iloc[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)\n","    dates = dates.loc[predict_start:predict_end]\n","    y_true = []\n","    y_hat_train = {}\n","    y_hat_test = []\n","    auc_train_dict = {}\n","    auc_test = []\n","\n","    for date in dates.index:\n","        # train test split\n","        #train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity')\n","        X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n","\n","        # fit model\n","        model = self.fit(X_train, y_train, model_type)\n","\n","        # predict\n","        y_hat_train.update({date: self.predict(model, X_train)})\n","        y_hat_test += list(self.predict(model, X_test))\n","\n","        # evaluate train data\n","        auc_train_dict.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})\n","        \n","        y_true += list(y_test)\n","    \n","    auc_test = self.auc(y_true, y_hat_test)\n","    auc_train = np.mean(list(auc_train_dict.values()))\n","\n","    return auc_train, auc_test, auc_train_dict\n","\n","\n","# add to Activity agent\n","setattr(Usage_Agent, 'evaluate', evaluate)\n","del evaluate"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YYtHJpERPikJ"},"source":["Finally, we can evaluate the simple Logit model for the \"Dishwasher\", for instance for all predictions after the \"2014-08-01\".  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqaxt7-b9SfE","executionInfo":{"status":"ok","timestamp":1607623282273,"user_tz":-60,"elapsed":79708,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}},"outputId":"86e00cfb-e9fc-4ba2-cc23-a33c7a1c2755"},"source":["auc_train, auc_test, auc_train_dict = usage.evaluate(df, \"logit\", '2013-11-01', predict_start='2014-08-01', predict_end= -1)\r\n","print(\"mean_auc_on_train = \"+ str(auc_train) + \" | test_auc = \" + str(auc_test))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["mean_auc_on_train = 0.4878626283941594 | test_auc = 0.5342367726642515\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_nf8rKaVRNW-"},"source":["As can be seen above, the model's performance is quite disappointing. It is not surprising that we do not have a very high accuracy, given the little amount of data we have. However, there must be potential for improvment. A first step in that direction would be a proper feature selection methodology taking into account different devices and households. Moreover, there has been a large decrease in model accuracy after changing the pre-processing pipeline methodology. Therefore, it seems that the model is sensitive to the way we detect the devices' activity. In the next steps we should investigate how and why these pre-processing steps impact the model's performance."]},{"cell_type":"markdown","metadata":{"id":"qYF4hPeeDFtA"},"source":["## **Appendix A1: Complete Usage Agent**"]},{"cell_type":"code","metadata":{"id":"FllI0166G0lz","executionInfo":{"status":"ok","timestamp":1607623282274,"user_tz":-60,"elapsed":79706,"user":{"displayName":"Felix Germaine","photoUrl":"","userId":"07033484937606028650"}}},"source":["class Usage_Agent:\n","    import pandas as pd\n","\n","    def __init__(self, input_df, device):\n","        self.input = input_df\n","        self.device = device\n","\n","\n","    def train_test_split(self, df, date, train_start='2013-11-01'):\n","      select_vars =  [self.device + '_usage', self.device+ '_usage_lag_1', self.device+ '_usage_lag_2',\t'active_last_2_days']\n","      df = df[select_vars]\n","      X_train = df.loc[train_start:date, df.columns != self.device + '_usage']\n","      y_train = df.loc[train_start:date, df.columns == self.device + '_usage']\n","      X_test  = df.loc[date, df.columns != self.device + '_usage']\n","      y_test  = df.loc[date , df.columns == self.device + '_usage']\n","      return X_train, y_train, X_test, y_test\n","\n","    def fit_smLogit(self, X, y):\n","      import statsmodels.api as sm\n","      return sm.Logit(y, X).fit(disp=False)\n","\n","    def fit(self, X, y, model_type):\n","      if model_type == 'logit':\n","          model = self.fit_smLogit(X, y)\n","      else:\n","        raise InputError('Unknown model type.')\n","      return model\n","\n","    def predict(self, model, X):\n","      import statsmodels\n","      import numpy as np\n","      X = np.array(X)\n","\n","      if type(model) == statsmodels.discrete.discrete_model.BinaryResultsWrapper:\n","          y_hat = model.predict(X)\n","      else:\n","          raise InputError('Unknown model type.')\n","      return y_hat\n","\n","    def pipeline(self, df, date, model_type, train_start):\n","      X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n","      model = self.fit(X_train, y_train, model_type)\n","      return self.predict(model, X_test)\n","\n","\n","    def auc(self, y_true, y_hat):\n","      import sklearn.metrics\n","      return sklearn.metrics.roc_auc_score(y_true, y_hat)\n","\n","\n","    def evaluate(self, df, model_type, train_start, predict_start='2014-01-01', predict_end=-1):\n","      import pandas as pd\n","      import numpy as np\n","      dates = pd.DataFrame(df.index)\n","      dates = dates.set_index(df.index)['Time']\n","      predict_start = pd.to_datetime(predict_start)\n","      predict_end = pd.to_datetime(dates.iloc[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)\n","      dates = dates.loc[predict_start:predict_end]\n","      y_true = []\n","      y_hat_train = {}\n","      y_hat_test = []\n","      auc_train_dict = {}\n","      auc_test = []\n","\n","      for date in dates.index:\n","          # train test split\n","          #train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity')\n","          X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n","\n","          # fit model\n","          model = self.fit(X_train, y_train, model_type)\n","\n","          # predict\n","          y_hat_train.update({date: self.predict(model, X_train)})\n","          y_hat_test += list(self.predict(model, X_test))\n","\n","          # evaluate train data\n","          auc_train_dict.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})\n","        \n","          y_true += list(y_test)\n","    \n","      auc_test = self.auc(y_true, y_hat_test)\n","      auc_train = np.mean(list(auc_train_dict.values()))\n","\n","      return auc_train, auc_test, auc_train_dict"],"execution_count":23,"outputs":[]}]}