{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performance Evaluation Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of our recommender system, we will analyze the system regarding the individual agents’ performance, the cold start problem, the sensitivity to changes in the recommendation hyperparameter and potential energy cost savings. We will introduce a further agent called Evaluation Agent which will perform all evaluation actions.\n",
    "\n",
    "We will perform our evaluation analysis for households 1 to 10 in the REFIT: Electrical Load Measurements data (Murray et at., 2019), to validate our evaluation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Preparing the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from agents import Performance_Evaluation_Agent\n",
    "from helper_functions import Helper\n",
    "\n",
    "helper = Helper()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "EXPORT_PATH = '../export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **2. Preparations for Evaluating the Performance of our Recommender System**\n",
    "### **2.1 Determining User Input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to use our recommender system and evaluate its performance, we need to specify the required user inputs (i.e. active appliances, shiftable devices and the consumption threshold). For specifying which of the devices in the respective household will be determined as active appliances and shiftable devices, we look at the description of the devices provided in the readme file and categorize the devices according to our definitions of the categories. Furthermore, we validate that the used devices do not contain any noise in their consumption data and remove devices which contain noise, i.e. consume energy constantly over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House 1\n",
      "0.Aggregate, 1.Fridge, 2.Chest Freezer, 3.Upright Freezer, 4.Tumble Dryer,\n",
      "5.Washing Machine, 6.Dishwasher, 7.Computer Site, 8.Television Site, 9.Electric Heater\n",
      "\t!NOTES\n",
      "\t\t0. October 2014, numerous light bulbs changed to LEDs.\n",
      "\t\t7.a Desktop Computer\n",
      "\t\t7.b Computer Monitor\n",
      "\n",
      "House 2\n",
      "0.Aggregate, 1.Fridge-Freezer, 2.Washing Machine, 3.Dishwasher, 4.Television,\n",
      "5.Microwave, 6.Toaster, 7.Hi-Fi, 8.Kettle, 9.Oven Extractor Fan\n",
      "\t!NOTES\n",
      "\n",
      "House 3\n",
      "0.Aggregate, 1.Toaster, 2.Fridge-Freezer, 3.Freezer, 4.Tumble Dryer,\n",
      "5.Dishwasher, 6.Washing Machine, 7.Television, 8.Microwave, 9.Kettle\n",
      "\t!NOTES\n",
      "\n",
      "House 4\n",
      "0.Aggregate, 1.Fridge, 2.Freezer, 3.Fridge-Freezer, 4.Washing Machine (1),\n",
      "5.Washing Machine (2), 6.Computer Site, 7.Television Site, 8.Microwave, 9.Kettle\n",
      "\t!NOTES\n",
      "\t\t6.a Desktop Computer\n",
      "\t\t6.b Computer Monitor\n",
      "\t\t6.c Scanner\n",
      "\t\t6.d Printer\n",
      "\t\t6.e Router\n",
      "\t\t6. Change in signature 01/02/2015\n",
      "\t\t7.a Television\n",
      "\t\t7.b DVD Player\n",
      "\t\t7.c VHS Player\n",
      "\t\t7. Change in signature 19/12/2014\n",
      "\n",
      "House 5\n",
      "0.Aggregate, 1.Fridge-Freezer, 2.Tumble Dryer, 3.Washing Machine, 4.Dishwasher,\n",
      "5.Computer Site, 6.Television Site, 7.Combination Microwave, 8.Kettle, 9.Toaster\n",
      "\t!NOTES\n",
      "\t\t2. Dehumidifier added on 21/11/2014\n",
      "\t\t5.a Desktop Computer\n",
      "\t\t5.b Computer Monitor\n",
      "\t\t5.c Printer\n",
      "\t\t5.d Speakers\n",
      "\t\t5. Change in signature 27/01/2015\n",
      "\t\t6.a Television w/DVD Player\n",
      "\t\t6.b Set-top Box\n",
      "\t\t6.c Games Console\n",
      "\n",
      "House 6\n",
      "0.Aggregate, 1.Freezer (Utility Room), 2.Washing Machine, 3.Dishwasher, 4.MJY Computer,\n",
      "5.Television Site, 6.Microwave, 7.Kettle, 8.Toaster, 9.PGM Computer\n",
      "\t!NOTES\n",
      "\t\t4.a Desktop Computer\n",
      "\t\t4.b Computer Monitor\n",
      "\t\t4.c Computer Monitor\n",
      "\t\t4.d Printer\n",
      "\t\t5.a Television\n",
      "\t\t5.b Set-top Box\n",
      "\t\t5.c PC\n",
      "\t\t5.d DVD Player\n",
      "\t\t9.a Desktop Computer\n",
      "\t\t9.b Computer Monitor\n",
      "\t\t9.c Printer\n",
      "\t\t9.d Shredder\n",
      "\n",
      "House 7\n",
      "0.Aggregate, 1.Fridge, 2.Freezer (Garage), 3.Freezer, 4.Tumble Dryer,\n",
      "5.Washing Machine, 6.Dishwasher, 7.Television Site, 8.Toaster, 9.Kettle\n",
      "\t!NOTES\n",
      "\t\t3. Change in signature 24/11/13\n",
      "\t\t6. Change in signature 20/05/14\n",
      "\t\t7.a Television\n",
      "\t\t7.b Speakers\n",
      "\t\t7.c Telephone\n",
      "\n",
      "House 8\n",
      "0.Aggregate, 1.Fridge, 2.Freezer, 3.Dryer, 4.Washing Machine,\n",
      "5.Toaster, 6.Computer, 7.Television Site, 8.Microwave, 9.Kettle\n",
      "\t!NOTES\n",
      "\t\t1. Change in Fridge Nov 6th\n",
      "\t\t5.a Toaster\n",
      "\t\t5.b DAB Radio\n",
      "\t\t7.a Television\n",
      "\t\t7.b DVD Player\n",
      "\t\t7.c VHS Player\n",
      "\t\t7.d Sound Bar\n",
      "\n",
      "House 9\n",
      "0.Aggregate, 1.Fridge-Freezer, 2.Washer Dryer, 3.Washing Machine, 4.Dishwasher,\n",
      "5.Television Site, 6.Microwave, 7.Kettle, 8.Hi-Fi, 9.Electric Heater\n",
      "\t!NOTES\n",
      "\t\t5.a Television\n",
      "\t\t5.b Sky Box\n",
      "\t\t5.c DVD Player\n",
      "\n",
      "House 10\n",
      "0.Aggregate, 1.Magimix (Blender), 2.Freezer, 3.Chest Freezer (In Garage), 4.Fridge-Freezer,\n",
      "5.Washing Machine, 6.Dishwasher, 7.Television Site, 8.Microwave, 9. Kenwood KMix\n",
      "\t!NOTES\n",
      "\t\t1. Changed from Fridge to Blender on 17/06/2014\n",
      "\t\t2. Changed from Freezer to Toaster on 25/06/2014.\n",
      "\t\t3. Post April 2015, included a second Freezer (both located in Garage)\n",
      "\t\t4. Changed from Whirlpool ART 500-9/G/1 to AEG SKZ71800F0 March 2015 (Both Fridge-Freezer)\n",
      "\t\t7.a. TV\n",
      "\t\t7.b. DVD Player\n",
      "\t\t7.c. Set-top Box\n",
      "\t\t7.d. Router\n",
      "\t\t7.e. Network storage External power supply\n",
      "\t\t7.f. Laptop external power supply\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readme = DATA_PATH+'REFIT_Readme.txt'\n",
    "readme = helper.load_txt(readme)\n",
    "start = readme.rfind('House 1\\n')\n",
    "end = readme.find('House 11\\n')\n",
    "print(readme[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Shiftable Devices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual input, determined using information provided in the readme\n",
    "# validated in the next step using the validate thresholds functionality\n",
    "shiftable_devices = {\n",
    "    1: ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n",
    "    2: ['Washing Machine', 'Dishwasher'],\n",
    "    3: ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n",
    "    4: ['Washing Machine (1)', 'Washing Machine (2)'],\n",
    "    5: ['Tumble Dryer'], # , 'Washing Machine' --> consumes energy constantly; , 'Dishwasher' --> noise at 3am\n",
    "    6: ['Washing Machine', 'Dishwasher'],\n",
    "    7: ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n",
    "    8: ['Washing Machine'], # 'Dryer' --> consumes constantly\n",
    "    9: ['Washer Dryer', 'Washing Machine', 'Dishwasher'], \n",
    "    10: ['Washing Machine'] #'Dishwasher'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Active Appliances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1611869179830,
     "user": {
      "displayName": "Leon H",
      "photoUrl": "",
      "userId": "10150017554198726068"
     },
     "user_tz": -60
    },
    "id": "XH42w1_HyeMs"
   },
   "outputs": [],
   "source": [
    "# manual input, determined using information provided in the readme\n",
    "# validated in the next step using the validate thresholds functionality\n",
    "active_appliances = {\n",
    "    1: deepcopy(shiftable_devices[1]) + ['Television Site', 'Computer Site'],\n",
    "    2: deepcopy(shiftable_devices[2]) + ['Television', 'Microwave', 'Toaster', 'Hi-Fi', 'Kettle'],\n",
    "    3: deepcopy(shiftable_devices[3]) + ['Toaster', 'Television', 'Microwave', 'Kettle'],\n",
    "    4: deepcopy(shiftable_devices[4]) + ['Television Site', 'Kettle'], #'Microwave', 'Computer Site' --> consume energy constantly \n",
    "    5: deepcopy(shiftable_devices[5]) + ['Television Site', 'Combination Microwave', 'Kettle', 'Toaster'], # 'Computer Site', --> consumes energy constantly\n",
    "    6: deepcopy(shiftable_devices[6]) + ['MJY Computer', 'Kettle', 'Toaster'], #, 'PGM Computer', 'Television Site' 'Microwave' --> consume energy constantly \n",
    "    7: deepcopy(shiftable_devices[7]) + ['Television Site', 'Toaster', 'Kettle'],\n",
    "    8: deepcopy(shiftable_devices[8]) + ['Toaster', 'Kettle'], # 'Television Site', 'Computer' --> consume energy constantly\n",
    "    9: deepcopy(shiftable_devices[9]) + ['Microwave', 'Kettle'], #'Television Site', 'Hi-Fi' --> consume energy constantly\n",
    "    10: deepcopy(shiftable_devices[10]) + ['Magimix (Blender)', 'Microwave'] # 'Television Site' --> consume energy constantly\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Energy Consumption Threshold**\n",
    "\n",
    "Our Preparation Agent will require the energy consumption threshold, which will determine if a device was used in a given period. This threshold will allow to reduce the impact of noise in the data. We will determine the optimal thresholds for the households using the Preparation Agent’s validate thresholds method. To demonstrate how noise in the consumption data occurs, we call the validate thresholds method for household 1. The consumption data regarding the Television Site in household 1 seems to contain daily noise around 3 am. We will choose the optimal threshold, such that the noise is removed from the data.\n",
    "\n",
    "Furthermore, we will create our initial evaluation configuration file for each household which will contain the specified user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the thresholds for household 1 to show noise in the data\n",
    "household_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the config including the user input\n",
    "config =  {'data': {'household': deepcopy(household_id)}}\n",
    "config['user_input'] = {\n",
    "    'shiftable_devices': deepcopy(shiftable_devices[config['data']['household']]),\n",
    "    'active_appliances': deepcopy(active_appliances[config['data']['household']])\n",
    "}\n",
    "# ada acts weird, logit is alright, random forest is alright, knn is okay, xgboost ok\n",
    "# initializing the evaluation agent\n",
    "evaluation = Performance_Evaluation_Agent(DATA_PATH, 'logit', config, load_data=True, weather_sel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-70721216f1d7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Data-Preparation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mdf_th\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreparation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpreparation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mdf_th\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreparation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_th\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mdf_th\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhelper\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maggregate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_th\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'60T'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\xai_energy_efficient_smart_home\\code\\agents.py\u001B[0m in \u001B[0;36mtruncate\u001B[1;34m(self, df, features, factor, verbose)\u001B[0m\n\u001B[0;32m     73\u001B[0m                 \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m             )  # truncate only the values for which the device uses energy\n\u001B[1;32m---> 75\u001B[1;33m             output.loc[row_nn, feature] = self.outlier_truncation(\n\u001B[0m\u001B[0;32m     76\u001B[0m                 \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrow_nn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfactor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfactor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m             )  # Truncatation factor = 1.5 * IQR\n",
      "\u001B[1;32m~\\Documents\\xai_energy_efficient_smart_home\\code\\agents.py\u001B[0m in \u001B[0;36moutlier_truncation\u001B[1;34m(self, series, factor, verbose)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m                 \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m         print(\n\u001B[0;32m     57\u001B[0m             \u001B[1;34mf\"[outlier truncation: {series.name}]: {counter} outliers were truncated.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "preparation = evaluation.preparation\n",
    "\n",
    "# Data-Preparation\n",
    "df_th = preparation.truncate(preparation.input)\n",
    "df_th = preparation.scale(df_th)\n",
    "df_th = helper.aggregate(df_th, '60T')\n",
    "\n",
    "# Graphical analysis of candidate thresholds\n",
    "thresholds = [0] + list(np.geomspace(.01, .4, 5))\n",
    "preparation.validate_thresholds(df_th, thresholds, config['user_input']['active_appliances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    1: 0.15,\n",
    "    2: 0.01,\n",
    "    3: 0.01, \n",
    "    4: 0.01, \n",
    "    5: 0.025,\n",
    "    6: 0.065, \n",
    "    7: 0.01, \n",
    "    8: 0.01, # washing machine over night\n",
    "    9: 0.01, \n",
    "    10: 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **2.2 Running our Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to analyze the performance of our recommender system, we need to calculate all outputs of all our agents and all recommendations possible based on the available data. To conveniently compute these outputs and recommendations for the households, we added a pipeline method to the Evaluation Agent which allows to run every agent of our recommender system for every available date iteratively. We will demonstrate its functionality by creating the recommendations for household 3. \n",
    "\n",
    "Additionally, we will use a further method of the Evaluation Agent to receive the default configuration for evaluating our recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the config including the user input\n",
    "config =  {'data': {'household': deepcopy(household_id)}}\n",
    "config['user_input'] = {\n",
    "    'shiftable_devices': deepcopy(shiftable_devices[config['data']['household']]),\n",
    "    'active_appliances': deepcopy(active_appliances[config['data']['household']]),\n",
    "    'threshold': deepcopy(thresholds[config['data']['household']])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'household': 3},\n 'user_input': {'shiftable_devices': ['Tumble Dryer',\n   'Washing Machine',\n   'Dishwasher'],\n  'active_appliances': ['Tumble Dryer',\n   'Washing Machine',\n   'Dishwasher',\n   'Toaster',\n   'Television',\n   'Microwave',\n   'Kettle'],\n  'threshold': 0.01},\n 'preparation': {'activity': {'truncate': {'features': 'all',\n    'factor': 1.5,\n    'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'aggregate': {'resample_param': '60T'},\n   'activity': {'active_appliances': ['Tumble Dryer',\n     'Washing Machine',\n     'Dishwasher',\n     'Toaster',\n     'Television',\n     'Microwave',\n     'Kettle'],\n    'threshold': 0.01},\n   'time': {'features': ['hour', 'day_name']},\n   'activity_lag': {'features': ['activity'], 'lags': [24, 48, 72]}},\n  'usage': {'truncate': {'features': 'all', 'factor': 1.5, 'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'activity': {'active_appliances': ['Tumble Dryer',\n     'Washing Machine',\n     'Dishwasher',\n     'Toaster',\n     'Television',\n     'Microwave',\n     'Kettle'],\n    'threshold': 0.01},\n   'aggregate_hour': {'resample_param': '60T'},\n   'aggregate_day': {'resample_param': '24H'},\n   'time': {'features': ['hour', 'day_name']},\n   'shiftable_devices': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n   'device': {'threshold': 0.01}},\n  'load': {'truncate': {'features': 'all', 'factor': 1.5, 'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'aggregate': {'resample_param': '60T'},\n   'shiftable_devices': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n   'device': {'threshold': 0.01}}}}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the evaluation agent\n",
    "evaluation = Performance_Evaluation_Agent(DATA_PATH, 'logit',config, xai = False)\n",
    "evaluation.get_default_config('preparation')\n",
    "evaluation.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[evaluation agent] Finished preparing the data for the activity agent.\n",
      "[evaluation agent] Finished preparing the data for the usage agent.\n",
      "[evaluation agent] Finished preparing the data for the load agent.\n"
     ]
    }
   ],
   "source": [
    "evaluation.pipeline('preparation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                     activity  hour  activity_lag_24  activity_lag_48  \\\nTime                                                                    \n2013-09-29 23:00:00         0    23              0.0              0.0   \n2013-09-30 00:00:00         0     0              0.0              0.0   \n2013-09-30 01:00:00         0     1              0.0              1.0   \n2013-09-30 02:00:00         0     2              0.0              1.0   \n2013-09-30 03:00:00         0     3              0.0              0.0   \n\n                     activity_lag_72  day_name_Monday  day_name_Saturday  \\\nTime                                                                       \n2013-09-29 23:00:00              0.0                0                  0   \n2013-09-30 00:00:00              0.0                1                  0   \n2013-09-30 01:00:00              0.0                1                  0   \n2013-09-30 02:00:00              0.0                1                  0   \n2013-09-30 03:00:00              0.0                1                  0   \n\n                     day_name_Sunday  day_name_Thursday  day_name_Tuesday  \\\nTime                                                                        \n2013-09-29 23:00:00                1                  0                 0   \n2013-09-30 00:00:00                0                  0                 0   \n2013-09-30 01:00:00                0                  0                 0   \n2013-09-30 02:00:00                0                  0                 0   \n2013-09-30 03:00:00                0                  0                 0   \n\n                     day_name_Wednesday  \nTime                                     \n2013-09-29 23:00:00                   0  \n2013-09-30 00:00:00                   0  \n2013-09-30 01:00:00                   0  \n2013-09-30 02:00:00                   0  \n2013-09-30 03:00:00                   0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>activity</th>\n      <th>hour</th>\n      <th>activity_lag_24</th>\n      <th>activity_lag_48</th>\n      <th>activity_lag_72</th>\n      <th>day_name_Monday</th>\n      <th>day_name_Saturday</th>\n      <th>day_name_Sunday</th>\n      <th>day_name_Thursday</th>\n      <th>day_name_Tuesday</th>\n      <th>day_name_Wednesday</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-09-29 23:00:00</th>\n      <td>0</td>\n      <td>23</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-09-30 00:00:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-09-30 01:00:00</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-09-30 02:00:00</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013-09-30 03:00:00</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.df['activity'][100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            activity  Tumble Dryer_usage  Washing Machine_usage  \\\nTime                                                              \n2014-01-03         1                   1                      1   \n2014-01-04         1                   1                      0   \n2014-01-05         1                   1                      1   \n2014-01-06         1                   0                      0   \n2014-01-07         1                   0                      0   \n\n            Dishwasher_usage  periods_since_last_activity  \\\nTime                                                        \n2014-01-03                 1                          1.0   \n2014-01-04                 1                          1.0   \n2014-01-05                 1                          1.0   \n2014-01-06                 0                          1.0   \n2014-01-07                 1                          1.0   \n\n            periods_since_last_Tumble Dryer_usage  \\\nTime                                                \n2014-01-03                                    1.0   \n2014-01-04                                    1.0   \n2014-01-05                                    1.0   \n2014-01-06                                    1.0   \n2014-01-07                                    2.0   \n\n            periods_since_last_Washing Machine_usage  \\\nTime                                                   \n2014-01-03                                       1.0   \n2014-01-04                                       1.0   \n2014-01-05                                       2.0   \n2014-01-06                                       1.0   \n2014-01-07                                       2.0   \n\n            periods_since_last_Dishwasher_usage  hour  activity_lag_1  ...  \\\nTime                                                                   ...   \n2014-01-03                                  2.0     0             1.0  ...   \n2014-01-04                                  1.0     0             1.0  ...   \n2014-01-05                                  1.0     0             1.0  ...   \n2014-01-06                                  1.0     0             1.0  ...   \n2014-01-07                                  2.0     0             1.0  ...   \n\n            Dishwasher_usage_lag_1  Dishwasher_usage_lag_2  \\\nTime                                                         \n2014-01-03                     0.0                     1.0   \n2014-01-04                     1.0                     0.0   \n2014-01-05                     1.0                     1.0   \n2014-01-06                     1.0                     1.0   \n2014-01-07                     0.0                     1.0   \n\n            Dishwasher_usage_lag_3  active_last_2_days  day_name_Monday  \\\nTime                                                                      \n2014-01-03                     0.0                   1                0   \n2014-01-04                     1.0                   1                0   \n2014-01-05                     0.0                   1                0   \n2014-01-06                     1.0                   1                1   \n2014-01-07                     1.0                   1                0   \n\n            day_name_Saturday  day_name_Sunday  day_name_Thursday  \\\nTime                                                                \n2014-01-03                  0                0                  0   \n2014-01-04                  1                0                  0   \n2014-01-05                  0                1                  0   \n2014-01-06                  0                0                  0   \n2014-01-07                  0                0                  0   \n\n            day_name_Tuesday  day_name_Wednesday  \nTime                                              \n2014-01-03                 0                   0  \n2014-01-04                 0                   0  \n2014-01-05                 0                   0  \n2014-01-06                 0                   0  \n2014-01-07                 1                   0  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>activity</th>\n      <th>Tumble Dryer_usage</th>\n      <th>Washing Machine_usage</th>\n      <th>Dishwasher_usage</th>\n      <th>periods_since_last_activity</th>\n      <th>periods_since_last_Tumble Dryer_usage</th>\n      <th>periods_since_last_Washing Machine_usage</th>\n      <th>periods_since_last_Dishwasher_usage</th>\n      <th>hour</th>\n      <th>activity_lag_1</th>\n      <th>...</th>\n      <th>Dishwasher_usage_lag_1</th>\n      <th>Dishwasher_usage_lag_2</th>\n      <th>Dishwasher_usage_lag_3</th>\n      <th>active_last_2_days</th>\n      <th>day_name_Monday</th>\n      <th>day_name_Saturday</th>\n      <th>day_name_Sunday</th>\n      <th>day_name_Thursday</th>\n      <th>day_name_Tuesday</th>\n      <th>day_name_Wednesday</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-01-03</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-01-04</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-01-05</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-01-06</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2014-01-07</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.df['usage'][100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                     Tumble Dryer  Washing Machine  Dishwasher\nTime                                                          \n2013-10-09 18:00:00           0.0              0.0         0.0\n2013-10-09 19:00:00           0.0              0.0         0.0\n2013-10-09 20:00:00           0.0              0.0         0.0\n2013-10-09 21:00:00           0.0              0.0         0.0\n2013-10-09 22:00:00           0.0              0.0         0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tumble Dryer</th>\n      <th>Washing Machine</th>\n      <th>Dishwasher</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-10-09 18:00:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-10-09 19:00:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-10-09 20:00:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-10-09 21:00:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-10-09 22:00:00</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.df['load'][335:340]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Creating all recommendations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'household': 3,\n  'first_date': '2013-09-26',\n  'last_date': '2015-06-01',\n  'start_dates': {'activity': '2013-09-29',\n   'usage': '2013-09-28',\n   'load': '2013-09-26',\n   'usage_tumble_dryer': '2013-09-28',\n   'usage_washing_machine': '2013-09-28',\n   'usage_dishwasher': '2013-09-28',\n   'combined': '2013-09-29'}},\n 'user_input': {'shiftable_devices': ['Tumble Dryer',\n   'Washing Machine',\n   'Dishwasher'],\n  'active_appliances': ['Tumble Dryer',\n   'Washing Machine',\n   'Dishwasher',\n   'Toaster',\n   'Television',\n   'Microwave',\n   'Kettle'],\n  'threshold': 0.01},\n 'preparation': {'activity': {'truncate': {'features': 'all',\n    'factor': 1.5,\n    'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'aggregate': {'resample_param': '60T'},\n   'activity': {'active_appliances': ['Tumble Dryer',\n     'Washing Machine',\n     'Dishwasher',\n     'Toaster',\n     'Television',\n     'Microwave',\n     'Kettle'],\n    'threshold': 0.01},\n   'time': {'features': ['hour', 'day_name']},\n   'activity_lag': {'features': ['activity'], 'lags': [24, 48, 72]}},\n  'usage': {'truncate': {'features': 'all', 'factor': 1.5, 'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'activity': {'active_appliances': ['Tumble Dryer',\n     'Washing Machine',\n     'Dishwasher',\n     'Toaster',\n     'Television',\n     'Microwave',\n     'Kettle'],\n    'threshold': 0.01},\n   'aggregate_hour': {'resample_param': '60T'},\n   'aggregate_day': {'resample_param': '24H'},\n   'time': {'features': ['hour', 'day_name']},\n   'shiftable_devices': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n   'device': {'threshold': 0.01}},\n  'load': {'truncate': {'features': 'all', 'factor': 1.5, 'verbose': 0},\n   'scale': {'features': 'all', 'kind': 'MinMax', 'verbose': 0},\n   'aggregate': {'resample_param': '60T'},\n   'shiftable_devices': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n   'device': {'threshold': 0.01}}},\n 'activity': {'model_type': 'logit',\n  'split_params': {'train_start': '2013-09-29',\n   'test_delta': {'days': 1, 'seconds': -1},\n   'target': 'activity'}},\n 'usage': {'model_type': 'logit', 'train_start': '2013-09-28'},\n 'usage_tumble_dryer': {'model_type': 'logit', 'train_start': '2013-09-28'},\n 'usage_washing_machine': {'model_type': 'logit', 'train_start': '2013-09-28'},\n 'usage_dishwasher': {'model_type': 'logit', 'train_start': '2013-09-28'},\n 'load': {'shiftable_devices': ['Tumble Dryer',\n   'Washing Machine',\n   'Dishwasher']}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.get_default_config(['activity', 'usage', 'load'])\n",
    "evaluation.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent:\t\tload\n",
      "progress: \t614/614\n",
      "time:\t\t[21:28<00:00]\n",
      "\n",
      "                         h1          h2          h3         h4         h5  \\\n",
      "Tumble Dryer     712.256488  341.869906  116.022231  59.712949  33.156928   \n",
      "Washing Machine  146.987549   86.275832   35.139014  17.643711   9.037951   \n",
      "Dishwasher       117.363011  160.292822    9.845558   2.183648   0.497150   \n",
      "\n",
      "                        h6        h7        h8       h9       h10  ...  h15  \\\n",
      "Tumble Dryer     11.954180  5.738580  1.444243  0.00000  0.000000  ...  0.0   \n",
      "Washing Machine   3.771159  1.192733  1.450442  0.46657  0.487798  ...  0.0   \n",
      "Dishwasher        0.403743  0.000000  0.000000  0.00000  0.000000  ...  0.0   \n",
      "\n",
      "                 h16  h17  h18  h19  h20  h21  h22  h23  h24  \n",
      "Tumble Dryer     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "Washing Machine  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "Dishwasher       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "evaluation.pipeline(['activity','usage','load'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.pipeline('recommendation', activity_threshold=0.625, usage_threshold=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                              device  best_launch_hour  \\\nrecommendation_date                                      \n2013-09-29              Tumble Dryer                 0   \n2013-09-29           Washing Machine                 0   \n2013-09-29                Dishwasher                 0   \n2013-09-30              Tumble Dryer                 1   \n2013-09-30           Washing Machine                 2   \n...                              ...               ...   \n2015-05-31           Washing Machine                 7   \n2015-05-31                Dishwasher                 7   \n2015-06-01              Tumble Dryer                21   \n2015-06-01           Washing Machine                21   \n2015-06-01                Dishwasher                14   \n\n                     no_recommend_flag_activity  no_recommend_flag_usage  \\\nrecommendation_date                                                        \n2013-09-29                                    1                        0   \n2013-09-29                                    1                        0   \n2013-09-29                                    1                        0   \n2013-09-30                                    0                        0   \n2013-09-30                                    0                        0   \n...                                         ...                      ...   \n2015-05-31                                    0                        0   \n2015-05-31                                    0                        0   \n2015-06-01                                    0                        0   \n2015-06-01                                    0                        0   \n2015-06-01                                    0                        0   \n\n                     recommendation  \nrecommendation_date                  \n2013-09-29                      NaN  \n2013-09-29                      NaN  \n2013-09-29                      NaN  \n2013-09-30                      1.0  \n2013-09-30                      2.0  \n...                             ...  \n2015-05-31                      7.0  \n2015-05-31                      7.0  \n2015-06-01                     21.0  \n2015-06-01                     21.0  \n2015-06-01                     14.0  \n\n[1833 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>device</th>\n      <th>best_launch_hour</th>\n      <th>no_recommend_flag_activity</th>\n      <th>no_recommend_flag_usage</th>\n      <th>recommendation</th>\n    </tr>\n    <tr>\n      <th>recommendation_date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-09-29</th>\n      <td>Tumble Dryer</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2013-09-29</th>\n      <td>Washing Machine</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2013-09-29</th>\n      <td>Dishwasher</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2013-09-30</th>\n      <td>Tumble Dryer</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2013-09-30</th>\n      <td>Washing Machine</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2015-05-31</th>\n      <td>Washing Machine</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>2015-05-31</th>\n      <td>Dishwasher</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>2015-06-01</th>\n      <td>Tumble Dryer</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>2015-06-01</th>\n      <td>Washing Machine</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>2015-06-01</th>\n      <td>Dishwasher</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1833 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.output['recommendation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation.output['recommendation']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.dump(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **3. Evaluating the Performance of our Recommender System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goals for evaluating our recommender systems performance are to measure the performance of the individual agents (i.e. Activity Agent, Usage Agent, Load Agent), to quantify the cold start problem of our recommender system, to analyze the sensitivity of our system to changes in the recommendation hyperparameter (i.e. activity threshold and usage threshold) as well as to calculate the potential energy cost savings for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Evaluating the Performance of the Individual Agents** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the performance the Activity Agent, the Usage Agent and the Load Agent, we will use the agents’ own evaluation methods. Hence, evaluation method of the Activity Agent and the Usage Agent will calculate the Area under the ROC curve (i.e. AUC score). It will use all daily predictions received from running the pipeline from above, combine the predictions to one dataset and use this dataset to calculate the AUC scores. For the Load Agent, its evaluation method will extract all load profiles for the actual devices runs in the data and compare them to the respective typical load profile which is available at the date of the actual run. To summarize the performance the evaluation method will calculate the mean squared error for all device runs and will aggregate the scores to a single score by averaging over all device runs (i.e. MSE score). After finishing the evaluation of the individual agents’ performance, we will receive one AUC score for the Activity Agent per household as well as one usage AUC score and one load MSE score per shiftable device of the respective household.\n",
    "\n",
    "All necessary steps to calculate the performance of the individual agents can be called using a single method of the Evaluation Agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_id = 3\n",
    "\n",
    "config = json.load(open(EXPORT_PATH + str(household_id) + '_config.json', 'r'))\n",
    "files = ['df.pkl', 'output.pkl']\n",
    "files = [f\"{EXPORT_PATH}{household_id}_{file}\" for file in files]\n",
    "\n",
    "# initializing the agent\n",
    "evaluation = Performance_Evaluation_Agent(DATA_PATH,'logit' ,config, load_data=True, load_files=files, xai=True)\n",
    "evaluation.init_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/518 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explainability approaches are being evaluated.\n",
      "The start time is:1627240013.2549047\n",
      "0\n",
      "[0.025941682227521426]\n",
      "1\n",
      "[0.025941682227521426, 0.10019796102334465]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/518 [00:03<27:52,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383]\n",
      "The time passed is: 3.196448564529419\n",
      "<class 'float'>\n",
      "[3.196448564529419]\n",
      "The explainability approaches are being evaluated.\n",
      "The start time is:1627240016.4792786\n",
      "0\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207]\n",
      "1\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/518 [00:06<27:35,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357]\n",
      "The time passed is: 3.162541389465332\n",
      "<class 'float'>\n",
      "[3.196448564529419, 3.162541389465332]\n",
      "The explainability approaches are being evaluated.\n",
      "The start time is:1627240019.672737\n",
      "0\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612]\n",
      "1\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612, 0.02766276087694552]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/518 [00:09<27:43,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612, 0.02766276087694552, 0.026828917711847103]\n",
      "The time passed is: 3.2243757247924805\n",
      "<class 'float'>\n",
      "[3.196448564529419, 3.162541389465332, 3.2243757247924805]\n",
      "The explainability approaches are being evaluated.\n",
      "The start time is:1627240022.9240406\n",
      "0\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612, 0.02766276087694552, 0.026828917711847103, 0.2861832254303942]\n",
      "1\n",
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612, 0.02766276087694552, 0.026828917711847103, 0.2861832254303942, 0.41573428934590256]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/518 [00:13<28:16,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025941682227521426, 0.10019796102334465, 0.07830983601658383, 0.2308317099165207, 0.10986886001032951, 0.01902755380553357, 0.0004977026589472612, 0.02766276087694552, 0.026828917711847103, 0.2861832254303942, 0.41573428934590256, 0.13076975247261716]\n",
      "The time passed is: 3.380336046218872\n",
      "<class 'float'>\n",
      "[3.196448564529419, 3.162541389465332, 3.2243757247924805, 3.380336046218872]\n",
      "The explainability approaches are being evaluated.\n",
      "The start time is:1627240026.3382823\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/518 [00:15<33:24,  3.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-5174549a7417>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mevaluation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_agent_scores\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\xai_energy_efficient_smart_home\\code\\agents.py\u001B[0m in \u001B[0;36mget_agent_scores\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1725\u001B[0m             \u001B[0magent_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'_'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1726\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0magent_type\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'activity'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1727\u001B[1;33m                 \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mauc_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0mMSEE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1728\u001B[0m                 \u001B[0mscores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'activity_auc'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mauc_test\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1729\u001B[0m                 \u001B[0mscores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'MSEE'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMSEE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\xai_energy_efficient_smart_home\\code\\agents.py\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(self, df, model_type, split_params, predict_start, predict_end, return_errors, weather_sel, xai)\u001B[0m\n\u001B[0;32m    606\u001B[0m                         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlocal\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    607\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 608\u001B[1;33m                         \u001B[0mexp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexplainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain_instance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_row\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mX_test\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlocal\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredict_fn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    609\u001B[0m                         \u001B[1;31m#print(exp)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    610\u001B[0m                         \u001B[1;31m#print(exp.local_pred)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\lime\\lime_tabular.py\u001B[0m in \u001B[0;36mexplain_instance\u001B[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001B[0m\n\u001B[0;32m    338\u001B[0m             \u001B[1;31m# Preventative code: if sparse, convert to csr format if not in csr format already\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    339\u001B[0m             \u001B[0mdata_row\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_row\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtocsr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 340\u001B[1;33m         \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minverse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__data_inverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_row\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_samples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    341\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    342\u001B[0m             \u001B[1;31m# Note in sparse case we don't subtract mean since data would become dense\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\lime\\lime_tabular.py\u001B[0m in \u001B[0;36m__data_inverse\u001B[1;34m(self, data_row, num_samples)\u001B[0m\n\u001B[0;32m    548\u001B[0m             \u001B[0minverse\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minverse_column\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiscretizer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m             \u001B[0minverse\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiscretizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mundiscretize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minverse\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[0minverse\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_row\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minverse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\lime\\discretize.py\u001B[0m in \u001B[0;36mundiscretize\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    142\u001B[0m                 )\n\u001B[0;32m    143\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m                 ret[:, feature] = self.get_undiscretize_values(\n\u001B[0m\u001B[0;32m    145\u001B[0m                     \u001B[0mfeature\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mret\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m                 )\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\lime\\discretize.py\u001B[0m in \u001B[0;36mget_undiscretize_values\u001B[1;34m(self, feature, values)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mminz\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 127\u001B[1;33m         ret[np.where(min_max_unequal)] = scipy.stats.truncnorm.rvs(\n\u001B[0m\u001B[0;32m    128\u001B[0m             \u001B[0mminz\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmin_max_unequal\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m             \u001B[0mmaxz\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmin_max_unequal\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001B[0m in \u001B[0;36mrvs\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m   1058\u001B[0m             \u001B[0mvals\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_rvs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1059\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1060\u001B[1;33m             \u001B[0mvals\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_rvs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1061\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1062\u001B[0m         \u001B[0mvals\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvals\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mscale\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\u001B[0m in \u001B[0;36m_rvs\u001B[1;34m(self, a, b, size, random_state)\u001B[0m\n\u001B[0;32m   7846\u001B[0m                 idx = tuple((it.multi_index[j] if not bc[j] else slice(None))\n\u001B[0;32m   7847\u001B[0m                             for j in range(-len(size), 0))\n\u001B[1;32m-> 7848\u001B[1;33m                 \u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_rvs_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mit\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumsamples\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7849\u001B[0m                 \u001B[0mit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miternext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7850\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\u001B[0m in \u001B[0;36m_rvs_scalar\u001B[1;34m(self, a, b, numsamples, random_state)\u001B[0m\n\u001B[0;32m   7862\u001B[0m         \u001B[1;31m# Calculate some rvs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7863\u001B[0m         \u001B[0mU\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muniform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhigh\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 7864\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ppf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mU\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7865\u001B[0m         \u001B[0mrvs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize1d\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7866\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mrvs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\u001B[0m in \u001B[0;36m_ppf\u001B[1;34m(self, q, a, b)\u001B[0m\n\u001B[0;32m   7739\u001B[0m         \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0matleast_1d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0matleast_1d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7740\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 7741\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0m_truncnorm_ppf_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7742\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7743\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\u001B[0m in \u001B[0;36m_truncnorm_ppf_scalar\u001B[1;34m(q, a, b)\u001B[0m\n\u001B[0;32m   7587\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7588\u001B[0m                 \u001B[0mna\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_norm_cdf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_norm_cdf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 7589\u001B[1;33m                 \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcond_inner\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_norm_ppf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqinner\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnb\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mna\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1.0\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mqinner\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7590\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misinf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7591\u001B[0m             np.place(out, cond_inner,\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mplace\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\xai_energy_efficient_smart_home\\lib\\site-packages\\numpy\\lib\\function_base.py\u001B[0m in \u001B[0;36mplace\u001B[1;34m(arr, mask, vals)\u001B[0m\n\u001B[0;32m   1746\u001B[0m                         \"not {name}\".format(name=type(arr).__name__))\n\u001B[0;32m   1747\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1748\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_insert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1749\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1750\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "evaluation.get_agent_scores()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem vlt: läuft gar nicht weil xai nicht als true erkannt?\n",
    "# woher kommt 0?\n",
    "# this is what is returned by get_agent_scores:\n",
    "evaluation.agent_scores\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# idea: erweitern von agents.py um explainability aspect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Converting agent scores to tabular structure for comparison with further households**\n",
    "\n",
    "As each household has different shiftable devices, we will assign each shiftable device in each household with an integer index. Using these indices, we are able to create an overview on the individual agent scores over different households. To connect the indices with the actual device name we will create a legend for the shiftable devices using our helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device                       0                    1           2\nhousehold                                                      \n1                 Tumble Dryer      Washing Machine  Dishwasher\n2              Washing Machine           Dishwasher           -\n3                 Tumble Dryer      Washing Machine  Dishwasher\n4          Washing Machine (1)  Washing Machine (2)           -\n5                 Tumble Dryer                    -           -\n6              Washing Machine           Dishwasher           -\n7                 Tumble Dryer      Washing Machine  Dishwasher\n8              Washing Machine                    -           -\n9                 Washer Dryer      Washing Machine  Dishwasher\n10             Washing Machine                    -           -",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>device</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n    <tr>\n      <th>household</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Tumble Dryer</td>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tumble Dryer</td>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Washing Machine (1)</td>\n      <td>Washing Machine (2)</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tumble Dryer</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Tumble Dryer</td>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Washing Machine</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Washer Dryer</td>\n      <td>Washing Machine</td>\n      <td>Dishwasher</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Washing Machine</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.shiftable_device_legend(EXPORT_PATH).fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          activity_auc usage_auc             load_mse                \n                     -         0     1     2        0       1       2\nhousehold                                                            \n3                 0.83      0.71  0.68  0.71  24907.5  890.33  314.84",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>activity_auc</th>\n      <th colspan=\"3\" halign=\"left\">usage_auc</th>\n      <th colspan=\"3\" halign=\"left\">load_mse</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>-</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n    <tr>\n      <th>household</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0.83</td>\n      <td>0.71</td>\n      <td>0.68</td>\n      <td>0.71</td>\n      <td>24907.5</td>\n      <td>890.33</td>\n      <td>314.84</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_scores = {}\n",
    "agent_scores[household_id] = evaluation.agent_scores_to_summary()\n",
    "helper.concat_household_scores(agent_scores).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluating the Sensitivity of our Recommender System to Changes in the Recommendation Hyperparameter\n",
    "\n",
    "The ultimate goal of evaluating our framework is to analyze the potential energy cost savings for our user. To evaluate the cost savings for each recommendation, we will start by determining whether a recommendation provided by framework would be acceptable for the user. A recommendation is considered acceptable, if both the activity target and the device usage target are positive for the recommended starting hour and device. Furthermore, we will calculate the cost savings for the acceptable recommendations. These cost savings are determined by the cost for running the device according to our recommendation and the cost for the relevant run which the user performed without receiving our recommendation. If the user ran the device multiple times on the day for which a recommendation was provided, the relevant run was defined as the run for which the starting time is closest to the recommended starting hour. As we use industry day-ahead energy prices in our recommender system, the absolute savings do not represent a realistic prediction of cost savings on the household level. However, the relative saving which compares the energy costs while accepting the recommendation to the energy costs without the recommendation is a more appropriate to measure our recommender system’s performance.\n",
    "\n",
    "Since we calculate the cost savings for each recommendation, we will aggregate our evaluation results to summarize the performance of our recommender system to a few metrics. The performance will be determined by the number of recommendations provided, the acceptance rate of the provided recommendations, the total savings and the relative savings.\n",
    "\n",
    "For creating our recommendations, we need to specify the two hyperparameter, i.e. activity threshold and usage threshold. We tune the parameters using a grid search over candidate thresholds. We will use the results from the grid search to analyze the sensitivity of the recommendation timing and the performance metrics to changes in the hyperparameter as well as determining optimal values for these parameters. We defined the optimal hyperparameter as the parameter combination which will lead to the highest total savings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Savings for Given Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.evaluate(0.625, 0.125)\n",
    "list(evaluation.results.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.results_to_summary().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search over Candidate Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 8\n",
    "activity_thresholds = np.linspace(1/(steps), 1, steps)[:-1]\n",
    "usage_thresholds = np.linspace(1/steps, 1, steps)[:-1]\n",
    "print(f\"[grid search] candidate thresholds {activity_thresholds}\\n\")\n",
    "time.sleep(0.3)\n",
    "evaluation.grid_search(activity_thresholds, usage_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.results_to_summary().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Sensitivity of the Timing of Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the results from the grid search to analyze the changes in recommendation timing for different hyperparameter. As the device usage is predicted on a daily level, changes in the usage threshold will not lead to changes in the hours in which the recommendation will be made. Therefore, we will analyze the changes in recommendation timings for a constant usage threshold and a changing activity threshold. \n",
    "\n",
    "Additionally, we will compare the timing of the recommendations to the average energy price per hour and the average activity of the user per hour to identify potential patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the recommendation timings to average activity per hour and average price per hour\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "prices = pd.DataFrame(evaluation.price.input)\n",
    "prices['hour'] = list(prices.index.map(lambda x: x.hour))\n",
    "prices.groupby(by='hour').mean().plot(ax=axes[0])\n",
    "axes[0].set(title='average price per hour', ylabel='average price');\n",
    "\n",
    "activity = evaluation.activity.input[['hour', 'activity']]\n",
    "activity.groupby(by='hour').mean().plot(ax=axes[1])\n",
    "axes[1].set(title='average activity per hour', ylabel='average avtivity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Sensitivity of our Performance Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = int(np.ceil(np.sqrt(evaluation.results_to_summary().shape[1])))\n",
    "n_rows = int(np.ceil(evaluation.results_to_summary().shape[1] / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18,10))\n",
    "axes = axes.reshape(-1,)\n",
    "\n",
    "columns = list(evaluation.results_to_summary().columns)\n",
    "for i in range(axes.shape[0]):\n",
    "    try:\n",
    "        col = columns[i]\n",
    "        sensitivity = evaluation.get_sensitivity(col)\n",
    "        sns.heatmap(sensitivity, ax=axes[i])\n",
    "        axes[i].set(title=f\"heatmap: {col}\")\n",
    "    except IndexError:\n",
    "        axes[i].remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Determining the Optimal Recommendation Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = evaluation.get_optimal_thresholds()\n",
    "optimal_thresholds_index = evaluation.thresholds_to_index()\n",
    "optimal_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results = results.append(evaluation.optimal_result_to_summary())\n",
    "results = results[['activity_threshold', 'usage_threshold', 'acceptable', 'n_recommendations', 'relative_savings_mean', 'total_savings']]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing of optimal recommendations\n",
    "evaluation.results[optimal_thresholds_index]['recommendation'].hist();\n",
    "plt.ylabel('recommendation count')\n",
    "plt.xlabel('hour of the day')\n",
    "plt.title('Timing of optimal recommendations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.dump(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **4. Performing the Evaluation on Multiple Households**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our findings, we will perform the evaluation steps presented above on households 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold start: metrics\n",
    "euclidean = scipy.spatial.distance.euclidean\n",
    "magnitude = lambda v: np.sqrt(np.dot(v.values.reshape(-1), v.values.reshape(-1)))\n",
    "norm_euclidean = lambda y_true, y_hat: euclidean(y_true, y_hat) / magnitude(y_true)\n",
    "\n",
    "metrics =     {\n",
    "    'activity': 'auc',\n",
    "    'usage': 'auc',\n",
    "    'load': 'norm_euclidean'\n",
    "}\n",
    "\n",
    "# cold start: tolerance values\n",
    "tolerance = [0.05, 0.1, 0.15, 0.2, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 8\n",
    "activity_thresholds = np.linspace(1/(steps), 1, steps)[:-1]\n",
    "usage_thresholds = np.linspace(1/steps, 1, steps)[:-1]\n",
    "print(f\"[grid search] candidate thresholds {list(activity_thresholds)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "households = shiftable_devices.keys()\n",
    "\n",
    "agent_scores = {}\n",
    "cold_start_days = {}\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for household_id in households:\n",
    "    \n",
    "    # creating the configuration file\n",
    "    config =  {'data': {'household': deepcopy(household_id)}}\n",
    "    config['user_input'] = {\n",
    "        'shiftable_devices': deepcopy(shiftable_devices[config['data']['household']]),\n",
    "        'active_appliances': deepcopy(active_appliances[config['data']['household']]),\n",
    "        'threshold': deepcopy(thresholds[config['data']['household']])\n",
    "    }\n",
    "\n",
    "    # initializing the evaluation agent\n",
    "    evaluation = Evaluation_Agent(DATA_PATH, config)\n",
    "\n",
    "    # running the pipeline\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # preparing the input data\n",
    "    evaluation.get_default_config('preparation')\n",
    "    evaluation.pipeline('preparation')\n",
    "    \n",
    "    # creating all recommendations\n",
    "    evaluation.get_default_config(['activity', 'usage', 'load'])\n",
    "    evaluation.pipeline(['activity', 'usage', 'load'])\n",
    "\n",
    "    # storing intermediary results\n",
    "    evaluation.dump(EXPORT_PATH)\n",
    "    \n",
    "    # evaluating the performance of the induvidual agents\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    evaluation.get_agent_scores()\n",
    "    agent_scores[household_id] = evaluation.agent_scores_to_summary()\n",
    "    \n",
    "    # evaluating the performance of the induvidual agents\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    evaluation.get_cold_start_scores(fn={'load': norm_euclidean})\n",
    "    evaluation.get_cold_start_days(tolerance_values=tolerance)\n",
    "    cold_start_days[household_id] = evaluation.cold_start_to_summary(tolerance)\n",
    "    \n",
    "    # evaluating the performance of the framework + parameter tuning\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    evaluation.grid_search(activity_thresholds, usage_thresholds)\n",
    "    results = results.append(evaluation.optimal_result_to_summary())\n",
    "    \n",
    "    # storing the results\n",
    "    evaluation.dump(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = helper.shiftable_device_legend(EXPORT_PATH)\n",
    "legend.fillna('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance of the Individual Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_PATH+'agent-scores.pkl', 'wb') as file:\n",
    "    pickle.dump(agent_scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores_summary = helper.concat_household_scores(agent_scores).round(2).fillna('-')\n",
    "agent_scores_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Cold Start Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_start_days_df = helper.concat_household_scores(cold_start_days)\n",
    "\n",
    "tolerance_index = 2\n",
    "print(f\"[cold start] tolerance value: {tolerance[tolerance_index]}\")\n",
    "\n",
    "for i in range(cold_start_days_df.shape[0]):\n",
    "    for j in range(cold_start_days_df.shape[1]):\n",
    "        try:\n",
    "            cold_start_days_df.iloc[i,j] = cold_start_days_df.iloc[i,j][tolerance_index]\n",
    "        except TypeError:\n",
    "            pass\n",
    "cold_start_days_df.fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_PATH+'cold-start-days.pkl', 'wb') as file:\n",
    "    pickle.dump(cold_start_days, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Perfomance of our Recommender System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[['activity_threshold', 'usage_threshold', 'acceptable', 'n_recommendations', 'relative_savings_mean', 'total_savings']]\n",
    "results.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_PATH+'results.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **Reference List**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Murray, D., Stankovic, L., & Stankovic, V. (2017). An electrical load measurements datasetof united kingdom households from a two-year longitudinal study [data set]. *Scientific Data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **Appendix A1: Complete Evaluation Agent Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation_Agent:\n",
    "    def __init__(self, DATA_PATH, config, load_data=True, load_files=None):\n",
    "        import agents\n",
    "        from helper_functions import Helper\n",
    "        import pandas as pd\n",
    "\n",
    "        helper = Helper()\n",
    "\n",
    "        self.config = config\n",
    "        self.preparation = (agents.Preparation_Agent(helper.load_household(DATA_PATH, config[\"data\"][\"household\"]))\n",
    "            if load_data\n",
    "            else None\n",
    "        )\n",
    "        self.price = (\n",
    "            agents.Price_Agent(helper.create_day_ahead_prices_df(DATA_PATH, \"Day-ahead Prices_201501010000-201601010000.csv\"))\n",
    "            if load_data\n",
    "            else None\n",
    "        )\n",
    "        self.activity = None\n",
    "        self.load = None\n",
    "        for device in self.config[\"user_input\"][\"shiftable_devices\"]:\n",
    "            name = (\"usage_\"+ device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "            exec(f\"self.{name} = None\")\n",
    "        self.recommendation = None\n",
    "        self.df = {}\n",
    "        self.output = {}\n",
    "        self.errors = {}\n",
    "        self.agent_scores = {} \n",
    "        self.cold_start_scores = {}\n",
    "        #self.true_loads = None\n",
    "        self.results = {}\n",
    "        self.cold_start_days = pd.DataFrame()\n",
    "        if load_files != None:\n",
    "            self.load_from_drive(load_files)\n",
    "\n",
    "    # helper: loading and storing intermediary results and further helper\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def _load_object(self, filename):\n",
    "        import pickle\n",
    "        import json\n",
    "        import yaml\n",
    "\n",
    "        # using a command dict as a if-list\n",
    "        commands = {\n",
    "            \"pkl\": f\"pickle.load(open('{filename}', 'rb'))\",\n",
    "            \"json\": f\"json.load(open('{filename}', 'r'))\",\n",
    "            \"yaml\": f\"yaml.load(open('{filename}', 'r'), Loader = yaml.Loader)\",\n",
    "        }\n",
    "\n",
    "        *_, name, ftype = filename.split(\".\")\n",
    "        name = name[name.rfind(\"_\") + 1 :]\n",
    "        obj = eval(commands[ftype])\n",
    "        self[name] = obj\n",
    "\n",
    "    def load_from_drive(self, files):\n",
    "        files = [files] if type(files) != list else files\n",
    "        for filename in files:\n",
    "            self._load_object(filename)\n",
    "\n",
    "    def dump(self, EXPORT_PATH):\n",
    "        import json\n",
    "        import yaml\n",
    "        import pickle\n",
    "\n",
    "        # storing the current configuration\n",
    "        json.dump(self.config, open(EXPORT_PATH + str(self.config[\"data\"][\"household\"]) + \"_config.json\",\"w\"), indent=4)\n",
    "\n",
    "        # storing the prepared data\n",
    "        if self.df != {}:\n",
    "            pickle.dump(self.df, open(EXPORT_PATH + str(self.config[\"data\"][\"household\"]) + \"_df.pkl\", \"wb\"))\n",
    "\n",
    "        # storing the agents' output\n",
    "        if self.output != {}:\n",
    "            pickle.dump(self.output, open(EXPORT_PATH + str(self.config[\"data\"][\"household\"]) + \"_output.pkl\", \"wb\"))\n",
    "            \n",
    "        # storing the results\n",
    "        if self.results != {}:\n",
    "            pickle.dump(self.results, open(EXPORT_PATH + str(self.config[\"data\"][\"household\"]) + \"_results.pkl\", \"wb\"))\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return eval(f\"self.{item}\")\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        exec(f\"self.{key} = value\")\n",
    "\n",
    "    def _format_time(self, seconds):\n",
    "        return \"{:02.0f}\".format(seconds // 60) + \":\" + \"{:02.0f}\".format(seconds % 60)\n",
    "    \n",
    "    def _get_agent_names(self):\n",
    "        devices = self.config[\"user_input\"][\"shiftable_devices\"]\n",
    "        names = [\"activity\", \"load\"] + [\"usage_\"+ str(device).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower() for device in devices]\n",
    "        return names\n",
    "\n",
    "    \n",
    "    # creating the default configuration\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def get_default_config(self, agents):\n",
    "        if type(agents) != list:\n",
    "            agents = [agents]\n",
    "        \n",
    "        agents = [agent.lower() for agent in agents]\n",
    "        for agent in agents:\n",
    "            exec(f\"self._get_default_{agent}_config()\")     \n",
    "            \n",
    "    def _get_default_preparation_config(self):\n",
    "        from copy import deepcopy\n",
    "\n",
    "        # preparation\n",
    "        self.config[\"preparation\"] = {}\n",
    "        ## preparation: activity agent\n",
    "        self.config[\"preparation\"][\"activity\"] = {\n",
    "            \"truncate\": {\"features\": \"all\", \"factor\": 1.5, \"verbose\": 0},\n",
    "            \"scale\": {\"features\": \"all\", \"kind\": \"MinMax\", \"verbose\": 0},\n",
    "            \"aggregate\": {\"resample_param\": \"60T\"},\n",
    "            \"activity\": {\n",
    "                \"active_appliances\": deepcopy(self.config[\"user_input\"][\"active_appliances\"]),\n",
    "                \"threshold\": deepcopy(self.config[\"user_input\"][\"threshold\"]),\n",
    "            },\n",
    "            \"time\": {\"features\": [\"hour\", \"day_name\"]},\n",
    "            \"activity_lag\": {\"features\": [\"activity\"], \"lags\": [24, 48, 72]},\n",
    "        }\n",
    "        ## preparation: usage agent\n",
    "        self.config[\"preparation\"][\"usage\"] = {\n",
    "            \"truncate\": {\"features\": \"all\", \"factor\": 1.5, \"verbose\": 0},\n",
    "            \"scale\": {\"features\": \"all\", \"kind\": \"MinMax\", \"verbose\": 0},\n",
    "            \"activity\": {\n",
    "                \"active_appliances\": deepcopy(self.config[\"user_input\"][\"active_appliances\"]),\n",
    "                \"threshold\": deepcopy(self.config[\"user_input\"][\"threshold\"]),\n",
    "            },\n",
    "            \"aggregate_hour\": {\"resample_param\": \"60T\"},\n",
    "            \"aggregate_day\": {\"resample_param\": \"24H\"},\n",
    "            \"time\": {\"features\": [\"hour\", \"day_name\"]},\n",
    "            \"shiftable_devices\": deepcopy(self.config[\"user_input\"][\"shiftable_devices\"]),\n",
    "            \"device\": {\"threshold\": deepcopy(self.config[\"user_input\"][\"threshold\"])},\n",
    "        }\n",
    "        ## preparation: load agent\n",
    "        self.config[\"preparation\"][\"load\"] = {\n",
    "            \"truncate\": {\"features\": \"all\", \"factor\": 1.5, \"verbose\": 0},\n",
    "            \"scale\": {\"features\": \"all\", \"kind\": \"MinMax\", \"verbose\": 0},\n",
    "            \"aggregate\": {\"resample_param\": \"60T\"},\n",
    "            \"shiftable_devices\": deepcopy(self.config[\"user_input\"][\"shiftable_devices\"]),\n",
    "            \"device\": {\"threshold\": deepcopy(self.config[\"user_input\"][\"threshold\"])},\n",
    "        }\n",
    "\n",
    "    def _get_default_activity_config(self):\n",
    "        from copy import deepcopy\n",
    "        \n",
    "        if (self.activity == None):\n",
    "            self.init_agents()\n",
    "        self._get_dates()\n",
    "        self.config[\"activity\"] = {\n",
    "            \"model_type\": \"logit\",\n",
    "            \"split_params\": {\n",
    "                \"train_start\": deepcopy(self.config[\"data\"][\"start_dates\"][\"activity\"]),\n",
    "                \"test_delta\": {\"days\": 1, \"seconds\": -1},\n",
    "                \"target\": \"activity\",\n",
    "            },\n",
    "        }\n",
    "        \n",
    "    def _get_default_load_config(self):\n",
    "        from copy import deepcopy\n",
    "        \n",
    "        if (self.load == None):\n",
    "            self.init_agents()\n",
    "        self._get_dates()\n",
    "        self.config[\"load\"] = {\n",
    "            \"shiftable_devices\": deepcopy(self.config[\"user_input\"][\"shiftable_devices\"])\n",
    "        }\n",
    "        \n",
    "    def _get_default_usage_config(self):\n",
    "        from copy import deepcopy\n",
    "        \n",
    "        if (self.activity == None) | (self.load == None):\n",
    "            self.init_agents()\n",
    "        self._get_dates()\n",
    "        self.config[\"usage\"] = {\n",
    "            \"model_type\": \"logit\",\n",
    "            \"train_start\": deepcopy(self.config[\"data\"][\"start_dates\"][\"usage\"]),\n",
    "        }\n",
    "        for device in self.config[\"user_input\"][\"shiftable_devices\"]:\n",
    "            name = (\"usage_\"+ device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "            self.config[name] = self.config[\"usage\"]\n",
    "            self.config[\"data\"][\"start_dates\"][name] = self.config[\"data\"][\"start_dates\"][\"usage\"]\n",
    "            \n",
    "    # extracting the available dates in the data\n",
    "    def get_first_date(self, df):\n",
    "        import pandas as pd\n",
    "\n",
    "        first_data = df.index.to_series()[0]\n",
    "        return (first_data + pd.Timedelta(\"1D\")).replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    def get_last_date(self, df):\n",
    "        import pandas as pd\n",
    "\n",
    "        last_data = df.index.to_series()[-1]\n",
    "        return (last_data - pd.Timedelta(\"1D\")).replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    def get_min_start_date(self, df):\n",
    "        df = df.dropna()\n",
    "        return df.loc[df.index.hour == 0, :].index[0]\n",
    "\n",
    "    def _get_dates(self):\n",
    "        import numpy as np\n",
    "\n",
    "        # first and last date in the data\n",
    "        self.config[\"data\"][\"first_date\"] = str(self.get_first_date(self.preparation.input))[:10]\n",
    "        self.config[\"data\"][\"last_date\"] = str(self.get_last_date(self.preparation.input))[:10]\n",
    "        # start dates\n",
    "        start_dates = {}\n",
    "        for agent, data in self.df.items():\n",
    "            start_dates[agent] = self.get_min_start_date(data)\n",
    "        start_dates[\"combined\"] = np.max(list(start_dates.values()))\n",
    "        self.config[\"data\"][\"start_dates\"] = {\n",
    "            key: str(value)[:10] for key, value in start_dates.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    # running the pipeline\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def pipeline(self, agents, **kwargs):\n",
    "        # converting single agent to list\n",
    "        if type(agents) != list:\n",
    "            agents = [agents]\n",
    " \n",
    "        agents = [agent.lower() for agent in agents]\n",
    "        \n",
    "        if 'preparation' in agents:\n",
    "            self._prepare(**kwargs)\n",
    "        if 'activity' in agents:\n",
    "            self._pipeline_activity_usage_load('activity', **kwargs)\n",
    "        if 'usage' in agents:\n",
    "            usage_agents = [\"usage_\"+ device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower() for device in self.config[\"user_input\"][\"shiftable_devices\"]]\n",
    "            for agent in usage_agents:\n",
    "                self._pipeline_activity_usage_load(agent, **kwargs)\n",
    "        if 'load' in agents:\n",
    "            self._pipeline_activity_usage_load('load', **kwargs)\n",
    "        if 'recommendation' in agents:\n",
    "            self._get_recommendations(**kwargs)\n",
    "            \n",
    "    def init_agents(self):\n",
    "        import agents\n",
    "\n",
    "        # initialize the agents\n",
    "        self.activity = agents.Activity_Agent(self.df[\"activity\"])\n",
    "        self.load = agents.Load_Agent(self.df[\"load\"])\n",
    "\n",
    "        # initialize usage agents for the shiftable devices: agent = usage_name\n",
    "        for device in self.config[\"user_input\"][\"shiftable_devices\"]:\n",
    "            name = (\"usage_\"+ device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "            exec(f'self.{name} = Usage_Agent(self.df[\"usage\"], \"{device}\")')\n",
    "            self.df[name] = self.df[\"usage\"]\n",
    "\n",
    "        self.recommendation = agents.Recommendation_Agent(\n",
    "            self.df[\"activity\"],\n",
    "            self.df[\"usage\"],\n",
    "            self.df[\"load\"],\n",
    "            self.price.input,\n",
    "            self.config[\"user_input\"][\"shiftable_devices\"],\n",
    "        )\n",
    "            \n",
    "    def _prepare(self, agent=\"all\"):\n",
    "        lines = {\n",
    "            \"activity\": 'self.df[\"activity\"] = self.preparation.pipeline_activity(self.preparation.input, self.config[\"preparation\"][\"activity\"])',\n",
    "            \"usage\": 'self.df[\"usage\"] = self.preparation.pipeline_usage(self.preparation.input, self.config[\"preparation\"][\"usage\"])',\n",
    "            \"load\": 'self.df[\"load\"] ,_,_ = self.preparation.pipeline_load(self.preparation.input, self.config[\"preparation\"][\"load\"])',\n",
    "        }\n",
    "        if agent == \"all\":\n",
    "            for agent in [\"activity\", \"usage\", \"load\"]:\n",
    "                exec(lines[agent])\n",
    "                print(f\"[evaluation agent] Finished preparing the data for the {agent} agent.\")\n",
    "        else:\n",
    "            exec(lines[agent])\n",
    "            print(f\"[evaluation agent] Finished preparing the data for the {agent} agent.\")\n",
    "\n",
    "    def _pipeline_activity_usage_load(self, agent, verbose=1):\n",
    "        import pandas as pd\n",
    "        from IPython.display import clear_output\n",
    "        import time\n",
    "\n",
    "        self.output[agent] = {}\n",
    "        self.errors[agent] = {}\n",
    "\n",
    "        # init agents\n",
    "        if (self.activity == None) | (self.load == None):\n",
    "            self.init_agents()\n",
    "\n",
    "        # determining the dates\n",
    "        dates = self.df[agent].index.to_series()\n",
    "        start = pd.to_datetime(self.config[\"data\"][\"start_dates\"][agent])\n",
    "        end = pd.to_datetime(self.config[\"data\"][\"last_date\"]).replace(\n",
    "            hour=23, minute=59, second=59\n",
    "        )\n",
    "        dates = dates[(dates >= start) & (dates <= end)].resample(\"1D\").count()\n",
    "        dates = [str(date)[:10] for date in list(dates.index)]\n",
    "\n",
    "        # pipeline funtion\n",
    "        start = time.time() if verbose >= 1 else None\n",
    "        for date in dates:\n",
    "            try:\n",
    "                self.output[agent][date] = eval(f'self.{agent}.pipeline(self.{agent}.input, \"{date}\", **self.config[\"{agent}\"])')\n",
    "                # verbose\n",
    "                if verbose >= 1:\n",
    "                    clear_output(wait=True)\n",
    "                    elapsed = time.time() - start\n",
    "                    remaining = (elapsed / (len(dates)) * (len(dates) - (dates.index(date) + 1)))\n",
    "                    print(f\"agent:\\t\\t{agent}\")\n",
    "                    print(f\"progress: \\t{dates.index(date)+1}/{len(dates)}\")\n",
    "                    print(f\"time:\\t\\t[{self._format_time(elapsed)}<{self._format_time(remaining)}]\\n\")\n",
    "                    print(self.output[agent][date])\n",
    "            except Exception as e:\n",
    "                self.errors[agent][date] = type(e).__name__\n",
    "\n",
    "    def _get_recommendations(\n",
    "        self, activity_threshold, usage_threshold, dates: tuple = \"all\"\n",
    "    ):\n",
    "        import numpy as np\n",
    "        from IPython.display import clear_output\n",
    "\n",
    "        # determining dates\n",
    "        start = (\n",
    "            self.config[\"data\"][\"start_dates\"][\"combined\"]\n",
    "            if dates == \"all\"\n",
    "            else dates[0]\n",
    "        )\n",
    "        end = self.config[\"data\"][\"last_date\"] if dates == \"all\" else dates[1]\n",
    "        dates = np.arange(\n",
    "            np.datetime64(start),\n",
    "            np.datetime64(end) + np.timedelta64(1, \"D\"),\n",
    "            np.timedelta64(1, \"D\"),\n",
    "        )\n",
    "        dates = [str(date) for date in dates]\n",
    "\n",
    "        # creating recommendations\n",
    "        self.errors[\"recommendation\"] = {}\n",
    "        self.output[\"recommendation\"] = {}\n",
    "        for date in dates:\n",
    "            try:\n",
    "                self.output[\"recommendation\"][date] = self.recommendation.pipeline(\n",
    "                    date, activity_threshold, usage_threshold, evaluation=self.output\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.errors[\"recommendation\"][date] = e\n",
    "\n",
    "        # merging the recommendations into one dataframe\n",
    "        df = list(self.output[\"recommendation\"].values())[0]\n",
    "\n",
    "        for idx in range(1, len(self.output[\"recommendation\"].values())):\n",
    "            df = df.append(list(self.output[\"recommendation\"].values())[idx])\n",
    "        df.set_index(\"recommendation_date\", inplace=True)\n",
    "        self.output[\"recommendation\"] = df\n",
    "        clear_output()\n",
    "\n",
    "        \n",
    "    # individual agent scores\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def get_agent_scores(self):\n",
    "        scores = {}\n",
    "        scores['activity_auc'] = None\n",
    "        scores['usage_auc'] = {}\n",
    "        scores['load_mse'] = {}\n",
    "\n",
    "        agents = self._get_agent_names()\n",
    "        for agent in agents:\n",
    "            agent_type = agent.split('_')[0]\n",
    "    \n",
    "            if agent_type == 'activity':\n",
    "                _, auc_test, _ = self[agent].evaluate(self[agent].input, **self.config[agent])\n",
    "                scores['activity_auc'] = auc_test\n",
    "            if agent_type == 'usage':\n",
    "                _, auc_test, _ = self[agent].evaluate(self[agent].input, **self.config[agent])\n",
    "                scores['usage_auc'][self[agent].device] = auc_test\n",
    "            if agent_type == 'load':\n",
    "                try:\n",
    "                    scores['load_mse'] = self.load.evaluate(**self.config['load'], evaluation=self.output['load'])\n",
    "                except KeyError:\n",
    "                    scores['load_mse'] = self.load.evaluate(**self.config['load'])\n",
    "        self.agent_scores = scores\n",
    "        return scores\n",
    "    \n",
    "    def agent_scores_to_summary(self, scores='default'):\n",
    "        import pandas as pd\n",
    "\n",
    "        if scores == 'default':\n",
    "            scores = self.agent_scores\n",
    "\n",
    "        summary = {}\n",
    "        summary['activity_auc'] = pd.DataFrame()\n",
    "        summary['usage_auc'] = pd.DataFrame()\n",
    "        summary['load_mse'] = pd.DataFrame()\n",
    "\n",
    "        household_id = self.config['data']['household']\n",
    "        devices = self.config['user_input']['shiftable_devices']\n",
    "\n",
    "        # activity \n",
    "        summary['activity_auc'].loc[household_id, '-'] = scores['activity_auc']\n",
    "        # usage\n",
    "        i = 0\n",
    "        for device in devices:\n",
    "            summary['usage_auc'].loc[household_id, i] = scores['usage_auc'][device]\n",
    "            i += 1\n",
    "        #load\n",
    "        i = 0\n",
    "        for device in devices:\n",
    "            summary['load_mse'].loc[household_id, i] = scores['load_mse'][device]\n",
    "            i += 1\n",
    "        \n",
    "        summary['activity_auc'].index.name = 'household'\n",
    "        summary['usage_auc'].index.name = 'household'\n",
    "        summary['load_mse'].index.name = 'household'\n",
    "        summary['usage_auc'].columns.name = 'device'\n",
    "        summary['load_mse'].columns.name = 'device'\n",
    "        return summary\n",
    "    \n",
    "    \n",
    "    # cold start: predict on all data\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def predict_all(self, agent, **kwargs):\n",
    "        agent_type = agent.split(\"_\")[0]\n",
    "        return eval(f\"self._predict_all_{agent_type}(agent, **kwargs)\")\n",
    "    \n",
    "    def _predict_all_load(self, agent, device):\n",
    "        y_hat = {\n",
    "            date: profiles.loc[device, :]\n",
    "            for date, profiles in self.output[agent].items()\n",
    "        }\n",
    "        return y_hat\n",
    "    \n",
    "    def _predict_all_activity(self, agent):\n",
    "        return self._predict_all_activity_usage(agent)\n",
    "\n",
    "    def _predict_all_usage(self, agent):\n",
    "        return self._predict_all_activity_usage(agent)\n",
    "\n",
    "    def _predict_all_activity_usage(self, agent):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        y_hat = {}\n",
    "        # intitializing the error dict\n",
    "        try:\n",
    "            self.errors[\"evaluation\"]\n",
    "        except KeyError:\n",
    "            self.errors[\"evaluation\"] = {}\n",
    "\n",
    "        try:\n",
    "            self.errors[\"evaluation\"][agent]\n",
    "        except KeyError:\n",
    "            self.errors[\"evaluation\"][agent] = {}\n",
    "\n",
    "        # determining the dates\n",
    "        dates = np.arange(\n",
    "            np.datetime64(self.config[\"data\"][\"start_dates\"][agent]),\n",
    "            np.datetime64(self.config[\"data\"][\"last_date\"]) + np.timedelta64(1, \"D\"),\n",
    "            np.timedelta64(1, \"D\"),\n",
    "        )\n",
    "        start = dates[0]\n",
    "        end = dates[-1] + pd.Timedelta(days=1, seconds=-1)\n",
    "\n",
    "        # creating X_test\n",
    "        X_test, _, _, _ = self[agent].train_test_split(\n",
    "            self[agent].input,\n",
    "            dates[-1] + np.timedelta64(1, \"D\"),\n",
    "            train_start=self.config[\"data\"][\"start_dates\"][agent],\n",
    "        )\n",
    "\n",
    "        # creating predictions\n",
    "        for date in dates:\n",
    "            X_train, y_train, _, _ = self[agent].train_test_split(\n",
    "                self[agent].input,\n",
    "                date,\n",
    "                train_start=self.config[\"data\"][\"start_dates\"][agent],\n",
    "            )\n",
    "            try:\n",
    "                model = self[agent].fit(X_train, y_train, \"logit\")\n",
    "                y_hat[date] = self[agent].predict(model, X_test)\n",
    "            except Exception as e:\n",
    "                self.errors[\"evaluation\"][agent][date] = type(e).__name__\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    # cold start: calculate cold start scores\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def get_cold_start_scores(self, fn: dict = \"default\"):\n",
    "        from IPython.display import clear_output\n",
    "\n",
    "        scores = {}\n",
    "        fn = {} if fn == \"default\" else fn\n",
    "        \n",
    "        # activity-agent\n",
    "        scores[\"activity\"] = self._get_cold_start_score(\"activity\", fn=fn.get(\"activity\", \"default\"))\n",
    "        clear_output()\n",
    "\n",
    "        for device in self.config[\"user_input\"][\"shiftable_devices\"]:\n",
    "            name = device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "            # usage agent\n",
    "            scores[\"usage_\" + name] = self._get_cold_start_score(\"usage_\" + name, fn=fn.get(\"usage\", \"default\"))\n",
    "            # load agent\n",
    "            scores[\"load_\" + name] = self._get_cold_start_score(\"load\", fn=fn.get(\"load\", \"default\"), device=device)\n",
    "            clear_output()\n",
    "        self.cold_start_scores = scores\n",
    "    \n",
    "    def _get_cold_start_score(self, agent, fn=\"default\", **kwargs):\n",
    "        import sklearn.metrics\n",
    "        import numpy as np\n",
    "\n",
    "        agent_type = agent.split(\"_\")[0]\n",
    "        # specifying the correct score function\n",
    "        fn_dict = {\n",
    "            \"activity\": f\"self.{agent}.auc\",\n",
    "            \"usage\": f\"self.{agent}.auc\",\n",
    "            \"load\": \"sklearn.metrics.mean_squared_error\",\n",
    "        }\n",
    "        fn = eval(fn_dict[agent_type]) if fn == \"default\" else fn\n",
    "\n",
    "        # specifying the correct y_true, y_hat\n",
    "        y_dict = {\n",
    "            \"activity\": \"self[agent].train_test_split(self[agent].input, date=np.datetime64(self.config['data']['last_date'])+np.timedelta64(1, 'D'), train_start=self.config['data']['start_dates'][agent])\",\n",
    "            \"usage\": \"self[agent].train_test_split(self[agent].input, date=np.datetime64(self.config['data']['last_date'])+np.timedelta64(1, 'D'), train_start=self.config['data']['start_dates'][agent])\",\n",
    "            \"load\": \"list(self.output['load'].values())[-1].loc[kwargs['device'], :]\",\n",
    "        }\n",
    "        y_true = eval(y_dict[agent_type])\n",
    "        y_true = y_true if agent_type == \"load\" else y_true[1]\n",
    "        y_hat = self.predict_all(agent, **kwargs)\n",
    "\n",
    "        # calculating the scores\n",
    "        scores = {}\n",
    "        for date, pred in y_hat.items():\n",
    "            scores[date] = fn(y_true, pred)\n",
    "        return scores\n",
    "\n",
    "    def cold_start_scores_to_df(self):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        scores_df = pd.DataFrame()\n",
    "        # convert dicts into dataframe\n",
    "        for key in self.cold_start_scores.keys():\n",
    "            for date, score in self.cold_start_scores[key].items():\n",
    "                scores_df.loc[str(date), key] = score\n",
    "\n",
    "        # sort the dataframe\n",
    "        cols = (\n",
    "            [\"activity\"]\n",
    "            + [col for col in scores_df if col.startswith(\"usage\")]\n",
    "            + [col for col in scores_df if col.startswith(\"load\")]\n",
    "        )\n",
    "        scores_df.index = scores_df.index.map(np.datetime64)\n",
    "        scores_df = scores_df[cols].sort_index()\n",
    "        return scores_df\n",
    "    \n",
    "    def get_cold_start_days(self, tolerance_values):\n",
    "        import pandas as pd\n",
    "\n",
    "        self.cold_start_days = pd.DataFrame({\"tolerance\": []}).set_index(\"tolerance\")\n",
    "        scores_df = self.cold_start_scores_to_df()\n",
    "        tolerance_fn = {\n",
    "            \"activity\": \"scores_df[agent].max() * (1 - tolerance[agent_type])\",\n",
    "            \"usage\": \"scores_df[agent].max() * (1 - tolerance[agent_type])\",\n",
    "            \"load\": \"tolerance['load']\",\n",
    "        }\n",
    "\n",
    "        # agent coldstart days\n",
    "        for tolerance in tolerance_values:\n",
    "            tolerance = {\"activity\": tolerance, \"usage\": tolerance, \"load\": tolerance}\n",
    "\n",
    "            for agent in scores_df.columns:\n",
    "                agent_type = agent.split(\"_\")[0]\n",
    "\n",
    "                done = False\n",
    "                day = 0\n",
    "                while not done:\n",
    "                    day += 1\n",
    "                    tolerance_value = eval(tolerance_fn[agent_type])\n",
    "                    if agent_type == \"load\":\n",
    "                        done = all(scores_df[agent].values[day - 1 :] < tolerance_value)\n",
    "                    else:\n",
    "                        done = all(scores_df[agent].values[day - 1 :] > tolerance_value)\n",
    "                self.cold_start_days.loc[tolerance[agent_type], agent] = day\n",
    "        # framework cold start days\n",
    "        self.cold_start_days['framework'] = self.cold_start_days.max(axis=1)\n",
    "    \n",
    "    \n",
    "    def cold_start_to_summary(self, tolerance_values='all'):\n",
    "        import pandas as pd\n",
    "        \n",
    "        if tolerance_values == 'all':\n",
    "            tolerance_values = list(self.cold_start_days.index)\n",
    "        \n",
    "        household_id = self.config['data']['household']\n",
    "        devices = self.config['user_input']['shiftable_devices']\n",
    "\n",
    "        summary = {}\n",
    "        summary['activity'] = {}\n",
    "        summary['usage'] = {}\n",
    "        summary['load'] = {}\n",
    "        summary['framework'] = {}\n",
    "\n",
    "        # activity agent\n",
    "        summary['activity']['-'] = {}  # '-': placeholder for device\n",
    "        summary['activity']['-'][household_id] = self.cold_start_days['activity'][tolerance_values].astype(int).to_list()\n",
    "        # usage agent\n",
    "        i = 0\n",
    "        for device in devices:\n",
    "            name = 'usage_' + device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "            summary['usage'][i] = {}\n",
    "            summary['usage'][i][household_id] = self.cold_start_days[name][tolerance_values].astype(int).to_list()\n",
    "            i += 1\n",
    "\n",
    "        # load agent\n",
    "        i = 0\n",
    "        for device in devices:\n",
    "            name = 'load_' + device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "            summary['load'][i] = {}\n",
    "            summary['load'][i][household_id] = self.cold_start_days[name][tolerance_values].astype(int).to_list()\n",
    "            i += 1\n",
    "    \n",
    "        # framework\n",
    "        summary['framework']['-'] = {}  # '-': placeholder for device\n",
    "        summary['framework']['-'][household_id] = self.cold_start_days['framework'][tolerance_values].astype(int).to_list()\n",
    "\n",
    "        # converting the format\n",
    "        for key, value in summary.items():\n",
    "            summary[key] = pd.DataFrame(value)\n",
    "            summary[key].columns.name = 'device'\n",
    "            summary[key].index.name = 'household'\n",
    "        return summary\n",
    "    \n",
    "    # cold start: visualizations\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def _plot_axs(self, axs, y, x=None, legend=None, **kwargs):\n",
    "        axs.plot(x, y) if x != None else axs.plot(y)\n",
    "        axs.set(**kwargs)\n",
    "        axs.legend(legend) if legend != None else None\n",
    "\n",
    "    def visualize_cold_start(self, metrics_name: dict, tolerance: dict=None, figsize=(18, 5)):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        scores_df = self.cold_start_scores_to_df()\n",
    "        fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "        # activity\n",
    "        self._plot_axs(\n",
    "            axs[0],\n",
    "            x=range(1, scores_df.shape[0] + 1),\n",
    "            y=scores_df[\"activity\"],\n",
    "            title=f\"[activity] {metrics_name['activity']}\",\n",
    "        )\n",
    "        legend = ['activity']\n",
    "        if tolerance != None: \n",
    "            tolerance_value = scores_df[\"activity\"].max() * (1 - tolerance[\"activity\"])\n",
    "            color = axs[0].lines[-1].get_color()\n",
    "            axs[0].plot([tolerance_value] * scores_df.shape[0], \"--\", c=color)\n",
    "            legend.append([f\"tolerance@{tolerance['activity']}\"])\n",
    "        axs[0].legend(legend)\n",
    "        axs[0].set_xlabel(\"days\")\n",
    "\n",
    "        # usage\n",
    "        usage_agents = [agent for agent in scores_df.columns if agent.find(\"usage\") != -1]\n",
    "        legend = []\n",
    "        for agent in usage_agents:\n",
    "            self._plot_axs(axs[1],\n",
    "                x=range(1, scores_df.shape[0] + 1),\n",
    "                y=scores_df[agent],\n",
    "                title=f\"[usage] {metrics_name['usage']}\",\n",
    "            )\n",
    "            legend += [agent]\n",
    "            if tolerance != None:\n",
    "                tolerance_value = scores_df[agent].max() * (1 - tolerance[\"usage\"])\n",
    "                color = axs[1].lines[-1].get_color()\n",
    "                axs[1].plot([tolerance_value] * scores_df.shape[0], \"--\", c=color)\n",
    "                legend += [f\"tolerance_{agent.replace('usage_', '')}@{tolerance['usage']}\"]\n",
    "        axs[1].legend(legend)\n",
    "        axs[1].set_xlabel(\"days\")\n",
    "\n",
    "        # load\n",
    "        load_agents = [agent for agent in scores_df.columns if agent.find(\"load\") != -1]\n",
    "        legend = []\n",
    "        for agent in load_agents:\n",
    "            self._plot_axs(\n",
    "                axs[2],\n",
    "                x=range(1, scores_df.shape[0] + 1),\n",
    "                y=scores_df[agent],\n",
    "                title=f\"[load] {metrics_name['load']}\",\n",
    "            )\n",
    "            legend += [agent]\n",
    "        if tolerance != None:\n",
    "            axs[2].plot([tolerance[\"load\"]] * scores_df.shape[0], \"--\", c=\"black\")\n",
    "            legend += [f\"tolerance@{tolerance['load']}\"]\n",
    "        axs[2].legend(legend)\n",
    "        axs[2].set_xlabel(\"days\")\n",
    "\n",
    "    \n",
    "    # evaluation: calculate costs per device run\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def calculate_cost(self, date, hour, load):\n",
    "        import numpy as np\n",
    "\n",
    "        if np.isnan(hour):\n",
    "            return np.nan\n",
    "        else:\n",
    "            price_idx = self.price.input.index.values\n",
    "            prices = self.price.input.values\n",
    "\n",
    "            dt = np.datetime64(date) + np.timedelta64(int(hour), \"h\")\n",
    "            # getting the correct position for the load in the load array\n",
    "            i = np.where(price_idx == dt)[0][0]\n",
    "\n",
    "            # reshaping the load array and calculating the costs\n",
    "            before = np.zeros(i)\n",
    "            after = np.zeros(prices.shape[0] - load.shape[0] - before.shape[0])\n",
    "            load = np.hstack([before, load, after])\n",
    "            return np.dot(load, prices)\n",
    "\n",
    "    def _get_usage(self, device, date):\n",
    "        return self.df[\"usage\"].loc[date, device + \"_usage\"]\n",
    "\n",
    "    def _get_activity(self, date, hour):\n",
    "        import numpy as np\n",
    "\n",
    "        if np.isnan(hour):\n",
    "            return np.nan\n",
    "        else:\n",
    "            dt = np.datetime64(date) + np.timedelta64(int(hour), \"h\")\n",
    "            return self.activity.input.loc[dt, \"activity\"]\n",
    "\n",
    "    def _get_starting_times(self, device):\n",
    "        import numpy as np\n",
    "\n",
    "        # extracts hours in which the device is turned on,\n",
    "        # conditional on that the device was turned off the hour before\n",
    "        times = self.df[\"load\"][device].index.to_numpy()\n",
    "        hour = self.df[\"load\"][device].values\n",
    "        before = np.insert(hour, 0, 0)[:-1]\n",
    "        return times[(before == 0) & (hour != 0)]\n",
    "\n",
    "    def _get_starting_hours(self, device, date):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        times = self._get_starting_times(device)\n",
    "        date = np.datetime64(date) if type(date) != np.datetime64 else date\n",
    "        times = times[(times >= date) & (times < date + np.timedelta64(1, \"D\"))]\n",
    "        hours = (\n",
    "            pd.Series(times).apply(lambda x: x.hour).to_numpy()\n",
    "            if times.shape[0] != 0\n",
    "            else np.nan\n",
    "        )\n",
    "        return hours\n",
    "\n",
    "    def _get_load(self, true_loads, device, date, hour):\n",
    "        import numpy as np\n",
    "\n",
    "        try:\n",
    "            dt = np.datetime64(date) + np.timedelta64(hour, \"h\")\n",
    "        # if hour == NaN, return zero load profile\n",
    "        except ValueError:\n",
    "            return np.zeros(24)\n",
    "        try:\n",
    "            return true_loads[device].loc[dt].values\n",
    "        except KeyError as ke:\n",
    "            # return a zero load profile if the datetime index was not found\n",
    "            if str(ke).split(\"(\")[0] == \"numpy.datetime64\":\n",
    "                return np.zeros(24)\n",
    "            # in any other case raise the key error\n",
    "            else:\n",
    "                raise ke\n",
    "\n",
    "\n",
    "    # evaluation: performance metrics\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def evaluate(self, activity_threshold, usage_threshold):\n",
    "        name = f\"activity: {activity_threshold}; usage: {usage_threshold}\"\n",
    "        #self._get_recommendations(activity_threshold, usage_threshold)\n",
    "        self.pipeline('recommendation', activity_threshold=activity_threshold, usage_threshold=usage_threshold, dates='all')\n",
    "        self.results[name] = self._evaluate()\n",
    "\n",
    "    def _evaluate(self):\n",
    "        import numpy as np\n",
    "\n",
    "        df = self.output[\"recommendation\"].copy()\n",
    "        \n",
    "        # usage and activity target\n",
    "        df[\"usage_true\"] = df.apply(lambda row: self._get_usage(row[\"device\"], row.name), axis=1)\n",
    "        df[\"activity_true\"] = df.apply(lambda row: self._get_activity(row.name, row[\"recommendation\"]), axis=1)\n",
    "        df[\"acceptable\"] = df[\"usage_true\"] * df[\"activity_true\"]\n",
    "\n",
    "        # starting times\n",
    "        df[\"starting_times\"] = df.apply(\n",
    "            lambda row: self._get_starting_hours(row[\"device\"], row.name), axis=1\n",
    "        )\n",
    "        df[\"relevant_start\"] = abs(df[\"starting_times\"] - df[\"recommendation\"])\n",
    "        df.loc[df[\"starting_times\"].notna(), \"relevant_start\"] = df[\n",
    "            df[\"starting_times\"].notna()\n",
    "        ].apply(\n",
    "            lambda row: row[\"starting_times\"][np.argmin(row[\"relevant_start\"])], axis=1\n",
    "        )\n",
    "\n",
    "        # actual loads\n",
    "        true_loads = self.load.get_true_loads(self.config[\"user_input\"][\"shiftable_devices\"])\n",
    "        df[\"load\"] = df.apply(lambda row: self._get_load(true_loads, row[\"device\"], row.name, row[\"relevant_start\"]), axis=1)\n",
    "\n",
    "        # calculating costs\n",
    "        df[\"cost_no_recommendation\"] = df.apply(lambda row: self.calculate_cost(row.name, row[\"relevant_start\"], row[\"load\"]), axis=1)\n",
    "        df[\"cost_recommendation\"] = df.apply(lambda row: self.calculate_cost(row.name, row[\"recommendation\"], row[\"load\"]),axis=1)\n",
    "        df[\"savings\"] = df[\"cost_no_recommendation\"] - df[\"cost_recommendation\"]\n",
    "        df[\"relative_savings\"] = df[\"savings\"] / df[\"cost_no_recommendation\"]\n",
    "\n",
    "        return df[\n",
    "            [\n",
    "                \"device\",\n",
    "                \"recommendation\",\n",
    "                \"acceptable\",\n",
    "                \"relevant_start\",\n",
    "                \"cost_no_recommendation\",\n",
    "                \"cost_recommendation\",\n",
    "                \"savings\",\n",
    "                \"relative_savings\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def _result_to_summary(self, result):\n",
    "        return {\n",
    "            \"n_recommendations\": result[\"recommendation\"].count(),\n",
    "            \"acceptable\": result[\"acceptable\"].mean(),\n",
    "            \"total_savings\": (result[\"acceptable\"] * result[\"savings\"]).sum(),\n",
    "            \"relative_savings_mean\": result[\"relative_savings\"].mean(),\n",
    "            \"relative_savings_median\": result[\"relative_savings\"].median(),\n",
    "        }\n",
    "\n",
    "    def results_to_summary(self):\n",
    "        import pandas as pd\n",
    "\n",
    "        summary = {\n",
    "            name: self._result_to_summary(result)\n",
    "            for name, result in self.results.items()\n",
    "        }\n",
    "        return pd.DataFrame.from_dict(summary, orient=\"index\")\n",
    "    \n",
    "\n",
    "    # evaluation: grid search and sensitivity\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def grid_search(self, activity_thresholds, usage_thresholds):\n",
    "        import itertools\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        # updating the config \n",
    "        try:\n",
    "            self.config['evaluation']\n",
    "        except:\n",
    "            self.config['evaluation'] = {}\n",
    "        \n",
    "        self.config['evaluation']['grid_search'] = {}\n",
    "        self.config['evaluation']['grid_search']['activity_thresholds'] = list(activity_thresholds)\n",
    "        self.config['evaluation']['grid_search']['usage_thresholds'] = list(usage_thresholds)\n",
    "        \n",
    "        # testing candidate thresholds\n",
    "        iterator = itertools.product(activity_thresholds, usage_thresholds)\n",
    "        for thresholds in tqdm(list(iterator)):\n",
    "            self.evaluate(thresholds[0], thresholds[1])\n",
    "\n",
    "    def get_sensitivity(self, target):\n",
    "        import pandas as pd\n",
    "\n",
    "        df = self.results_to_summary()\n",
    "        sensitivity = pd.DataFrame()\n",
    "        for threshold_name in df.index:\n",
    "            thresholds = threshold_name.split(\"; \")\n",
    "            activity_threshold, usage_threshold = [th.split(\": \")[1] for th in thresholds]\n",
    "            sensitivity.loc[activity_threshold, usage_threshold] = df.loc[threshold_name, target]\n",
    "        # sort and name rows and columns\n",
    "        sensitivity = sensitivity.loc[sorted(sensitivity.index), :]\n",
    "        sensitivity = sensitivity.loc[:, sorted(sensitivity.columns)]\n",
    "        sensitivity.index.name = \"activity_threshold\"\n",
    "        sensitivity.columns.name = \"usage_threshold\"\n",
    "        return sensitivity\n",
    "    \n",
    "    def get_optimal_thresholds(self):\n",
    "        df = self.results_to_summary()\n",
    "        result = df.sort_values(by='total_savings').iloc[-1, :]\n",
    "        thresholds = result.name.split('; ')\n",
    "        thresholds = [threshold.split(': ') for threshold in thresholds]\n",
    "        thresholds = {f\"{threshold}_threshold\": value for threshold, value in thresholds}\n",
    "        self.config['evaluation']['grid_search']['optimal_thresholds'] = thresholds\n",
    "        return thresholds\n",
    "        \n",
    "    def thresholds_to_index(self, activity_threshold='optimal', usage_threshold='optimal'):\n",
    "        if activity_threshold == 'optimal':\n",
    "            activity_threshold = self.config['evaluation']['grid_search']['optimal_thresholds']['activity_threshold']\n",
    "        if usage_threshold == 'optimal':\n",
    "            usage_threshold = self.config['evaluation']['grid_search']['optimal_thresholds']['usage_threshold']\n",
    "        return f\"activity: {activity_threshold}; usage: {usage_threshold}\"\n",
    "    \n",
    "    def optimal_result_to_summary(self):\n",
    "        import pandas as pd\n",
    "        optimal_thresholds = self.get_optimal_thresholds()\n",
    "        optimal_thresholds_index = self.thresholds_to_index()\n",
    "        result = self.results_to_summary().loc[optimal_thresholds_index,:]\n",
    "        result = result.append(pd.Series(optimal_thresholds))\n",
    "        result.name = self.config['data']['household']\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "name": "conda-env-xai_energy_efficient_smart_home-py",
   "language": "python",
   "display_name": "Python [conda env:xai_energy_efficient_smart_home] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}