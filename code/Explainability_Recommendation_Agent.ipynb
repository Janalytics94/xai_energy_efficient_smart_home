{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1ejom0Bv-lN"
   },
   "source": [
    "# **Recommendation Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XQKtNbRwLoR"
   },
   "source": [
    "The **Recommendation Agent** is the final agent in the recommendation process. It orchestrates how the **Activity Agent**, **Usage Agent**, **Load Agent** and **Price Agent** interact in order to deliver a recommendation at a particular date for a given household (see the dedicated notebooks in order to see the definition of these agents in detail).\n",
    "\n",
    "The recommendation agent works as follows:\n",
    "\n",
    "1. It requests the outputs of the **Activity Agent**, **Usage Agent**, **Load Agent** and **Price Agent**:\n",
    "\n",
    "\n",
    "*   The Activity Agent returns the probability that persons are present and in an \"active state\" in the house at each given hour of the day.\n",
    "*   The Usage Agent returns the probability that a given to-be-recommended-device will be used on the next day.\n",
    "*   The Load Agent returns a typical load profile for each to-be-recommended-device.\n",
    "*   The Price Agent returns the day-ahead-prices for the next 48 hours.\n",
    "\n",
    "2. It then computes the cost associated with launching the devices at each hour of the next day (based on the devices' typical load profiles and the day ahead electricity prices).\n",
    "\n",
    "3. Finally it recommends the cheapest launching hour among the set of hours at which users are likely present and active **[Probability(Present&Active)> Threshold]**. A recommendation  for a given device is made only if the user is likely enough to use the device on the next day **[Probability(Device_Usage)> Threshold]**\n",
    "\n",
    "\n",
    "In the present notebook, we will build this **Recommendation Agent** step by step. For that purpose, we will first load and preprocess the required data with the preprocessing agents. Then, we will iteratively add functions to the **Recommendation Agent class** in order to finally build a function entitled \"Pipeline\", which ouputs the desired recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwYZuk7-e7XK"
   },
   "source": [
    "## **1. Load and Pre-process Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iop0mvXH0a56"
   },
   "source": [
    "This part's only purpose is to load the data used in the recommendation agent. This process is described in detail in the Preparation Agent.  **[You might need to adapt some parameters when applying the script to another household than household 1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uRIFoyczkdD"
   },
   "source": [
    "### **1.1 Initialize and load python scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzS_qOg8teXQ"
   },
   "outputs": [],
   "source": [
    "# loading necessary libraries\n",
    "#import knn as knn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "import shap as shap\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from helper_functions import Helper\n",
    "from agents import Preparation_Agent\n",
    "\n",
    "# Gammli\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "\n",
    "from gammli.gammli import GAMMLI\n",
    "from gammli.dataReader import data_initialize\n",
    "from gammli.utils import local_visualize\n",
    "from gammli.utils import global_visualize_density\n",
    "from gammli.utils import feature_importance_visualize\n",
    "from gammli.utils import plot_trajectory\n",
    "from gammli.utils import plot_regularization\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "helper = Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ML Models\n",
    "\n",
    "# More ML Models\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sky4JWfvzctI"
   },
   "source": [
    "### **1.2 Set Params**\n",
    "\n",
    "**Note:** For the full detail of the parameters and the preprocessing agents take a look at the Preparation Agent's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCjWp7PIAeSZ"
   },
   "outputs": [],
   "source": [
    "# load household data for Household 1\n",
    "household = helper.load_household(DATA_PATH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "active_appliances = ['Toaster', 'Tumble Dryer', 'Dishwasher', 'Washing Machine','Television', 'Microwave', 'Kettle']\n",
    "shiftable_devices = ['Tumble Dryer', 'Washing Machine', 'Dishwasher']\n",
    "#model_types = ['logit', 'knn', 'ada', 'random forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CP1f_6EvAygB"
   },
   "outputs": [],
   "source": [
    "#activity params\n",
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "aggregate_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "\n",
    "activity_params = {\n",
    "    'active_appliances': active_appliances,\n",
    "    'threshold': threshold \n",
    "}\n",
    "\n",
    "time_params = {\n",
    "    'features': ['hour', 'day_name']\n",
    "}\n",
    "\n",
    "activity_lag_params = {\n",
    "    'features': ['activity'],\n",
    "    'lags': [24, 48, 72]\n",
    "}\n",
    "\n",
    "activity_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'activity': activity_params,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params\n",
    "}\n",
    "\n",
    "#load agent\n",
    "device_params = {\n",
    "    'threshold': threshold\n",
    "}\n",
    "\n",
    "load_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'shiftable_devices': shiftable_devices, \n",
    "    'device': device_params\n",
    "}\n",
    "\n",
    "#usage agent\n",
    "\n",
    "device = {\n",
    "    'threshold' : threshold}\n",
    "\n",
    "aggregate_params24_H = {\n",
    "    'resample_param': '24H'\n",
    "}\n",
    "\n",
    "usage_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'activity': activity_params,\n",
    "    'aggregate_hour': aggregate_params,\n",
    "    'aggregate_day': aggregate_params24_H,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params,\n",
    "    'shiftable_devices' : shiftable_devices,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf06_sA30KqX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#calling the preparation pipeline\n",
    "#prep = Preparation_Agent(household)\n",
    "#activity_df = prep.pipeline_activity(household, activity_pipe_params)\n",
    "#load_df, _, _ = prep.pipeline_load(household, load_pipe_params)\n",
    "#usage_df = prep.pipeline_usage(household, usage_pipe_params)\n",
    "\n",
    "#load price data\n",
    "#FILE_PATH = '/content/drive/MyDrive/T4_Recommendation-system-for-demand-response-and-load-shifting/02_data/'\n",
    "#price_df = helper.create_day_ahead_prices_df(DATA_PATH, 'Day-ahead Prices_201501010000-201601010000.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#activity_df.to_pickle('../data/processed_pickle/activity_df.pkl')\n",
    "#load_df,_, _ .to_pickle('../data/processed_pickle/load_df.pkl')\n",
    "#usage_df.to_pickle('../data/processed_pickle/usage_df.pkl')\n",
    "#price_df.to_pickle('../data/processed_pickle/price_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load pickle data\n",
    "activity_df = pd.read_pickle('../data/processed_pickle/activity_df.pkl')\n",
    "load_df = pd.read_pickle('../data/processed_pickle/load_df.pkl')\n",
    "usage_df = pd.read_pickle('../data/processed_pickle/usage_df.pkl')\n",
    "price_df = pd.read_pickle('../data/processed_pickle/price_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCMDOvjkNAfz"
   },
   "source": [
    "## **2. Constructing the Recommendation Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aub0_x9N1pVm"
   },
   "source": [
    "### **2.1 Initiliaze Agent**\n",
    "In a first step, the Recommendation Agent is initialized with the preprocessed data and the name of the shiftable devices (those for which we want to make predictions). All Agents are initialized accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8wrHrBUNCAI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from agents import Activity_Agent, Usage_Agent, Load_Agent, Price_Agent\n",
    "class Recommendation_Agent:\n",
    "    import pandas as pd\n",
    "\n",
    "    def __init__(self, activity_input, usage_input, load_input, price_input, shiftable_devices, model_type):\n",
    "        self.activity_input = activity_input\n",
    "        self.usage_input = usage_input\n",
    "        self.load_input = load_input\n",
    "        self.price_input = price_input\n",
    "        self.shiftable_devices = shiftable_devices\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.Activity_Agent = Activity_Agent(activity_input)\n",
    "\n",
    "        #create dicionnary with Usage_Agent for each device\n",
    "        self.Usage_Agent = {name: Usage_Agent(usage_input , name)  for name in shiftable_devices}\n",
    "\n",
    "        self.Load_Agent = Load_Agent(load_input)\n",
    "        self.Price_Agent = Price_Agent(price_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFcXh324pqjD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#initialize the Recommendation Agent with the required inputs\n",
    "recommend = Recommendation_Agent(activity_df, usage_df, load_df, price_df, shiftable_devices,model_type='knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0qrt4R13ean"
   },
   "source": [
    "### **2.2 Compute Usage Cost For Every Starting Time (each of the 24 hours of the day) For A Given Device**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6ZD4Ztk2dWx"
   },
   "source": [
    "This function computes the cost associated with launching a to-be-recommended-device at each hour of the next day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIiczCma4OS4"
   },
   "source": [
    "#### **2.2.1 Electricity Prices For 24 hours After Hypothetical Starting Time**\n",
    "First we build up a function which gives the electricity price for the 24 hours following a hypothetical starting time. \n",
    "\n",
    "*   The column \"Price_at_H+0\" gives the electricity price for the 24 hours after 00:00:00\n",
    "*   The column \"Price_at_H+1\" gives the electricity price for the 24 hours following 01:00:00 ( \"Price_at_H+0\" shifted by one hour)\n",
    "* The column \"Price_at_H+2\" gives the electricity price for the 24 hours following 02:00:00 ( \"Price_at_H+0\" shifted by two hours)\n",
    "*....\n",
    "\n",
    "This function first requests the **day-ahead electricity prices** for the next 48h from the Price_Agent. Then it arranges the prices as described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8IYAFSAmsUc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def electricity_prices_from_start_time(self, date):\n",
    "  import pandas as pd\n",
    "  prices_48 = self.Price_Agent.return_day_ahead_prices(date)\n",
    "  prices_from_start_time = pd.DataFrame()\n",
    "\n",
    "  for i in range(24):\n",
    "    prices_from_start_time[\"Price_at_H+\"+ str(i)] = prices_48.shift(-i)\n",
    "\n",
    "  #delete last 24 hours\n",
    "  prices_from_start_time = prices_from_start_time[:-24]\n",
    "  return prices_from_start_time\n",
    "\n",
    "# add to Activity agent\n",
    "setattr(Recommendation_Agent, 'electricity_prices_from_start_time', electricity_prices_from_start_time)\n",
    "del electricity_prices_from_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1607623110074,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "76HM7yq1tIs6",
    "outputId": "47b0afe4-4011-4c5e-ff4c-549826e15a21",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "recommend.electricity_prices_from_start_time(\"2014-02-20\")\n",
    "#H0 prices for next 24 hours start at 00:00\n",
    "#H1 prices for next 24 hours start at 01:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RwCKzj96MNC"
   },
   "source": [
    "#### **2.2.2 Device Launching Cost By Hour Of The Day**\n",
    "We compute the cost of operating the device at every given hour by multiplying the **day ahead electricity price** with the device's **typical load profile**. The latter typical load profile is generated by the **Load Agent** (for details see the Load Agent's notebook).\n",
    "\n",
    "As an output, we get the typical costs of operating the device for all possible 24 staring times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The Recommendation Agent's pipeline functions can be sped up by providing the predecessing agents' outputs directly. This Functionality will be used to evaluate the recommender systems performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw6_cZkTxcGN",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def cost_by_starting_time(self, date, device, evaluation=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # get electriciy prices following every device starting hour with previously defined function\n",
    "    prices = self.electricity_prices_from_start_time(date)\n",
    "\n",
    "    # build up table with typical load profile repeated for every hour (see Load_Agent)\n",
    "    if not evaluation:\n",
    "        device_load = self.Load_Agent.pipeline(self.load_input, date, self.shiftable_devices).loc[device]\n",
    "    else:\n",
    "        # get device load for one date\n",
    "        device_load = evaluation[\"load\"][date].loc[device]\n",
    "\n",
    "    device_load = pd.concat([device_load] * 24, axis=1)\n",
    "\n",
    "    # multiply both tables and aggregate costs for each starting hour\n",
    "    costs = np.array(prices) * np.array(device_load)\n",
    "    costs = np.sum(costs, axis=0)\n",
    "\n",
    "    # return an array of size 24 containing the total cost at each staring hour.\n",
    "    return costs\n",
    "\n",
    "setattr(Recommendation_Agent, 'cost_by_starting_time', cost_by_starting_time)\n",
    "del cost_by_starting_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB-CZm3N3cGA"
   },
   "source": [
    "As can be seen below, the output returns the cost associated with starting the \"Washing machine\" on each hour of the \"2014-02-20\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1607623110516,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "d8YIwxgo1TQp",
    "outputId": "5414778a-785b-421e-eaf2-f2b04ff196a3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "recommend.cost_by_starting_time(\"2014-02-20\", \"Washing Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dsiW7ba9CpI"
   },
   "source": [
    "### **2.3 Starting Time Recommendation For Each Device**\n",
    "\n",
    "We create a function that gives a starting time recommendation for a given device.\n",
    "\n",
    "1.   The function loads the **device usage costs associated with each starting time** (see above function).\n",
    "2.   In order **not** to recommend to start the device at an hour where the person is not home or at sleep, we exclude hours that have an  **activity probability below a certain threshold**. These probabilities are computed with the **Activity Agent**.\n",
    "3. In order **not** to make a recommendation when the household is unlikely to use the device on the next day anyway, we set a **device usage probability threshold** under which no recommendation is made. These usage probabilities are computed with the **Usage Agent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXU_oAlpBMjZ"
   },
   "source": [
    "The function outputs a dictionary with the best starting time, among the hours at which activity is likely enough. The output additionaly contains **\"no_recommend\"** flags, in order to signal that no recommendation should be made when :\n",
    "* There is no hour of the day where activity is likely enough (all hours of the day have an activity probability below the set threshold), then the **\"no_recommend_flag_activity\"** turns from 0 to 1.\n",
    "* The device usage is unlikely (below the set threshold), then the **\"no_recommend_flag_usage\"** turns from 0 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vmE_0iKrHfS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#return cheapest launching hour from the set of hours satifsfying:  probability of activity > threshold\n",
    "def recommend_by_device(self, date, device, model_type, activity_prob_threshold, usage_prob_threshold, evaluation=False):\n",
    "    import numpy as np\n",
    "\n",
    "    split_params = {\n",
    "        \"train_start\": \"2013-11-01\",\n",
    "        \"test_delta\": {\"days\": 1, \"seconds\": -1},\n",
    "        \"target\": \"activity\",\n",
    "    }\n",
    "\n",
    "    # compute costs by launching time:\n",
    "    costs = self.cost_by_starting_time(date, device, evaluation=evaluation)\n",
    "\n",
    "    # compute activity probabilities\n",
    "    if not evaluation:\n",
    "        activity_probs = self.Activity_Agent.pipeline(self.activity_input, date, model_type ,split_params)\n",
    "    else:\n",
    "        # get activity probs for date\n",
    "        activity_probs = evaluation[\"activity\"][date]\n",
    "\n",
    "    # set values above threshold to 1. Values below to Inf\n",
    "    # (vector will be multiplied by costs, so that hours of little activity likelihood get cost = Inf)\n",
    "    activity_probs = np.where(activity_probs >= activity_prob_threshold, 1, float(\"Inf\"))\n",
    "\n",
    "    # add a flag in case all hours have likelihood smaller than threshold\n",
    "    no_recommend_flag_activity = 0\n",
    "    if np.min(activity_probs) == float(\"Inf\"):\n",
    "        no_recommend_flag_activity = 1\n",
    "\n",
    "    # compute cheapest hour from likely ones\n",
    "    best_hour = np.argmin(np.array(costs) * np.array(activity_probs))\n",
    "\n",
    "    # compute likelihood of usage:\n",
    "    if not evaluation:\n",
    "        usage_prob = self.Usage_Agent[device].pipeline(self.usage_input, date, model_type, split_params[\"train_start\"])\n",
    "    else:\n",
    "        # get usage probs\n",
    "        name = \"usage_\" + device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "        usage_prob = evaluation[name][date]\n",
    "\n",
    "    no_recommend_flag_usage = 0\n",
    "    if usage_prob < usage_prob_threshold:\n",
    "        no_recommend_flag_usage = 1\n",
    "\n",
    "    return {\n",
    "        \"recommendation_date\": [date],\n",
    "        \"device\": [device],\n",
    "        \"best_launch_hour\": [best_hour],\n",
    "        \"no_recommend_flag_activity\": [no_recommend_flag_activity],\n",
    "        \"no_recommend_flag_usage\": [no_recommend_flag_usage],\n",
    "        \"recommendation\": [best_hour if (no_recommend_flag_activity == 0 and no_recommend_flag_usage == 0)else np.nan]\n",
    "    }\n",
    "\n",
    "setattr(Recommendation_Agent, 'recommend_by_device', recommend_by_device)\n",
    "del recommend_by_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3287,
     "status": "ok",
     "timestamp": 1607623112044,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "lluZpc8F8et9",
    "outputId": "312686b2-ae7e-44de-ee97-05321b9a1d87",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "recommend.recommend_by_device(\"2014-08-21\", \"Dishwasher\", 'knn', 0.3, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Create Recommendation Function For Entire Household**\n",
    "Finally, we wrap up the \"recommend_by_device\" function that makes a recommendation for each device, into the \"pipeline\" function that will make recommendations for all shiftable devices within a household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdezBYJB9eLF",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(self, date, model_type, activity_prob_threshold, usage_prob_threshold, evaluation=False):\n",
    "    import pandas as pd\n",
    "\n",
    "    recommendations_by_device = self.recommend_by_device(date, self.shiftable_devices[0], model_type, activity_prob_threshold,              usage_prob_threshold, evaluation=evaluation)\n",
    "    recommendations_table = pd.DataFrame.from_dict(recommendations_by_device)\n",
    "\n",
    "    for device in self.shiftable_devices[1:]:\n",
    "        recommendations_by_device = self.recommend_by_device(date, device, model_type, activity_prob_threshold, usage_prob_threshold, evaluation=evaluation)\n",
    "        recommendations_table = recommendations_table.append(pd.DataFrame.from_dict(recommendations_by_device))\n",
    "    return recommendations_table\n",
    "\n",
    "setattr(Recommendation_Agent, 'pipeline', pipeline)\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeYj-a8Yt9Rt"
   },
   "source": [
    "We can then generate a recommendation for a household by specifying:\n",
    "1. The day to be recommended\n",
    "2. The \"activity_prob_threshold\" (hours that have a smaller probability of household activity are not considered for recommendation)\n",
    "3. The \"usage_probability_threshold\" (devices that have a smaller probability of usage are not considered for recommendation)\n",
    "\n",
    "**Note**:  It remains to be investigated at which value to set these thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6068,
     "status": "ok",
     "timestamp": 1607623114836,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "OgkOjpNEMZTS",
    "outputId": "ba08901e-f70f-497e-852d-dff9836dabb4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "recommend.pipeline(date = \"2015-02-15\", model_type= \"random forest\", activity_prob_threshold = 0.4,  usage_prob_threshold = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGUEXy4TuzYZ"
   },
   "source": [
    "Finally, the system recommends to the user the \"best_launch_hour\" on the \"recommendation_date\" if both the \"no_recommend_flag_activity\" and \"no_recommend_flag_usage\" are at 0 for the given device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Continue here:\n",
    "# TO DO: get out specific models from rec agents; already extraction possible?\n",
    "#\n",
    "\n",
    "#TO DO: more than just model_type as setting?\n",
    "# e.g. pickle final models & hyperparameters from file?\n",
    "\n",
    "#TO DO: streamline s.t. recommendation pipeline is only called with one call\n",
    "\n",
    "#What do we want to see at the end?\n",
    "#overview of different explainers & their performance on metrics\n",
    "#we need: function that implements explainer on individual agent\n",
    "# for local prediction from recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Examplarily\n",
    "#Activity Agent  with shap\n",
    "# what we need to get out model, Xtest,location of rec?\n",
    "# problem:\n",
    "\n",
    "\n",
    "import shap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "date = '2014-01-01'\n",
    "activity = Activity_Agent(activity_df)\n",
    "X_test = activity.get_Xtest(activity_df, date)\n",
    "X_test\n",
    "\n",
    "\n",
    "#date = '2014-01-01'\n",
    "#activity = Activity_Agent(activity_df)\n",
    "y_test = activity.get_ytest(activity_df, date)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#date = '2014-01-01'\n",
    "\n",
    "activity = Activity_Agent(activity_df)\n",
    "X_train = activity.get_Xtrain(activity_df, date)\n",
    "X_train\n",
    "\n",
    "activity = Activity_Agent(activity_df)\n",
    "y_train = activity.get_ytrain(activity_df, date)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = activity.train_test_split(activity_df, date)\n",
    "activity_df.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability Evaluation BOARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Explainability of Activity Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_model_knn = activity.fit_knn(X_train,y_train)\n",
    "pred_model_rf = activity.fit_random_forest(X_train,y_train)\n",
    "pred_model_ada = activity.fit_ADA(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test_i = np.array(X_test.iloc[0]).reshape(1,-1)\n",
    "X_test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_knn = pred_model_knn.predict(X_test_i)\n",
    "y_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# that means we can get an error estimate for the knn model. how is that different with the shap model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictive_models = [pred_model_knn, pred_model_rf, pred_model_ada]\n",
    "predictive_models\n",
    "#y_test.head(20)#.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### LIME Function\n",
    "from lime import lime_tabular\n",
    "import statistics\n",
    "\n",
    "local = 9 #the instance we want to explain\n",
    "n_iter = 1\n",
    "i = 0\n",
    "\n",
    "data = {'Explainability Model': [],\n",
    "        'Predictive Model': [],\n",
    "        'Classifier': [],\n",
    "        'Run Duration': [],\n",
    "        'MAEE': [],\n",
    "        'MSEE': []}\n",
    "exp_eval_df = pd.DataFrame(data)\n",
    "\n",
    "for pred_model in predictive_models:\n",
    "    classifier = pred_model\n",
    "    print(classifier)\n",
    "    \n",
    "    print(pred_model)\n",
    "    print(str(pred_model))\n",
    "    \n",
    "    if \"KNeighbors\" in str(pred_model):\n",
    "        predictive_model = \"KNN\"\n",
    "    elif \"Random\" in str(pred_model):\n",
    "        predictive_model = \"Random Forest\"\n",
    "    elif \"Ada\" in str(pred_model):\n",
    "        predictive_model = \"AdaBoost\"\n",
    "    else:\n",
    "        predictive_model = \"Unknown model\"\n",
    "        \n",
    "    print(predictive_model)\n",
    "    \n",
    "    #predicted activity by pred model\n",
    "    X_test_i = np.array(X_test.iloc[local]).reshape(1,-1)\n",
    "    y_pred_i = pred_model.predict(X_test_i)\n",
    "    print(y_pred_i[0])\n",
    "\n",
    "    # LIME\n",
    "    explainability_model = 'LIME'\n",
    "    start_time = time.time()\n",
    "    explainer = lime_tabular.LimeTabularExplainer(training_data = np.array(X_train),\n",
    "                                              mode = \"regression\",\n",
    "                                              feature_names = X_train.columns,\n",
    "                                              categorical_features = [0])\n",
    "\n",
    "    exp = explainer.explain_instance(data_row = X_test.iloc[local], #changed to somewhere where activity= 1\n",
    "                                predict_fn = pred_model.predict)\n",
    "\n",
    "    exp.show_in_notebook(show_table = True)\n",
    "    end_time = time.time()\n",
    "    difference_time = end_time - start_time\n",
    "        \n",
    "    #compute MSEE:\n",
    "    y_expl_i = exp.local_pred\n",
    "    #print(y_expl_i)\n",
    "    #SEE = (y_pred_i - y_expl_i)**2 #squared prediction error for this computation (repetition necessary for MSEE)\n",
    "    \n",
    "    rep = 0\n",
    "\n",
    "    exp_list_abs = []\n",
    "    exp_list_squ = []\n",
    "    for rep in range(n_iter): #number of iterations for computing the diffferent lime models \n",
    " \n",
    "        exp = explainer.explain_instance(data_row = X_test.iloc[local], #changed to somewhere where activity= 1 \n",
    "                                predict_fn = pred_model.predict)\n",
    "        exp_list_abs.append(y_pred_i-exp.local_pred)\n",
    "        exp_list_squ.append((y_pred_i-exp.local_pred)**2)\n",
    "\n",
    "    exp_np_abs =np.array(exp_list_abs)\n",
    "    exp_np_squ =np.array(exp_list_squ)\n",
    "    #exp_list_abs.to_numpy()\n",
    "    #exp_list_abs.flatten()\n",
    "    #print(exp_list_abs.flatten())\n",
    "           \n",
    "    MAEE = statistics.mean(exp_np_abs.flatten())\n",
    "    MSEE = statistics.mean(exp_np_squ.flatten())\n",
    "    print(MAEE)\n",
    "    \n",
    "    #MSEE = mean(exp_list_squ[0])\n",
    "    #print(MSEE)\n",
    "    \n",
    "    #exp.as_list()\n",
    "    exp_eval_df.loc[i+1] = [explainability_model, predictive_model, classifier, difference_time, MAEE, MSEE]\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "    \n",
    "    # SHAP\n",
    "\n",
    "    explainability_model = 'SHAP'\n",
    "    start_time = time.time()\n",
    "\n",
    "    explainer = shap.KernelExplainer(pred_model.predict_proba, X_train)\n",
    "    shap_values = explainer.shap_values(X_test.iloc[local,:])\n",
    "    display(shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[local,:]))\n",
    "\n",
    "    end_time = time.time()\n",
    "    difference_time = end_time - start_time\n",
    "    \n",
    "    #compute MSEE:\n",
    "    rep = 0\n",
    "    \n",
    "    shap_list_abs = []\n",
    "    shap_list_squ = []\n",
    "    \n",
    "    for rep in range(n_iter):\n",
    "        explainer = shap.KernelExplainer(pred_model.predict_proba, X_train)\n",
    "        shap_values = explainer.shap_values(X_test.iloc[local,:])\n",
    "        print(shap_values)\n",
    "        # first array = contribution to class 0\n",
    "        # second array = contribution to class 1\n",
    "        contribution_to_class_1 = np.array(shap_values).sum(axis=1)[1] # the red part of the diagram\n",
    "        print(contribution_to_class_1)\n",
    "        base_value = explainer.expected_value[1] # the mean prediction\n",
    "        print(base_value)\n",
    "        y_expl_i = base_value + contribution_to_class_1\n",
    "        print(y_expl_i)\n",
    "        SEE = (y_pred_i[0] - y_expl_i)**2 #squared prediction error for this computation (repetition necessary for MSEE)\n",
    "    \n",
    "        shap_list_abs.append(y_pred_i-y_expl_i)\n",
    "        shap_list_squ.append((y_pred_i-y_expl_i)**2)\n",
    "\n",
    "        print(shap_list_abs)\n",
    "    \n",
    "    shap_np_abs =np.array(shap_list_abs)\n",
    "    shap_np_squ =np.array(shap_list_squ)\n",
    "\n",
    "    MAEE = statistics.mean(shap_np_abs.flatten())\n",
    "    MSEE = statistics.mean(shap_np_squ.flatten())\n",
    "    print(MAEE)\n",
    "\n",
    "    exp_eval_df.loc[i+1] = [explainability_model, predictive_model, classifier, difference_time, MAEE, MSEE]\n",
    "\n",
    "    i = i+1\n",
    "    \n",
    "    print(exp_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable Booster Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "ebm = ExplainableBoostingClassifier()\n",
    "#ebm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "interpret.glassbox.ebm.ebm.ExplainableBoostingClassifier"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "type(ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "set_visualize_provider(InlineProvider())\n",
    "from interpret import show\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_local = ebm.explain_local(X_test, y_test)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gammli -  Generalized Additive Modeling with Manifest and Latent Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "task_type = \"Classification\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "for i in columns:\n",
    "    meta_info[i]={'type': 'continues','source':'user'}\n",
    "meta_info['hour']={'type': 'continues','source':'user'}\n",
    "meta_info['activity_lag_24']={'type': 'continues','source':'item'}\n",
    "meta_info['activity_lag_48']={'type': 'continues','source':'item'}\n",
    "meta_info['day_name_Monday']={'type': 'continues','source':'item'}\n",
    "meta_info['day_name_Saturday']={'type': 'continues','source':'item'}\n",
    "meta_info['day_name_Sunday']={'type': 'continues','source':'item'}\n",
    "meta_info['day_name_Thursday']={'type': 'continues','source':'item'}\n",
    "meta_info['day_name_Wednesday']={'type': 'continues','source':'item'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Gammli\n",
    "tr_x, tr_Xi, tr_y, tr_idx, te_x, te_Xi, te_y, val_x, val_Xi, val_y, val_idx, meta_info, model_info,sy,sy_t = data_initialize(X_train,X_test,meta_info,task_type ,'warm', random_state, True)\n",
    "model = GAMMLI(wc='warm',model_info=model_info, meta_info=meta_info, subnet_arch=[20, 10],interact_arch=[20, 10],activation_func=tf.tanh, batch_size=min(500, int(0.2*tr_x.shape[0])), lr_bp=0.001, auto_tune=False,\n",
    "               interaction_epochs=1000,main_effect_epochs=1000,tuning_epochs=200,loss_threshold_main=0.01,loss_threshold_inter=0.1,\n",
    "              verbose=True, early_stop_thres=20,interact_num=10,n_power_iterations=5,n_oversamples=10, u_group_num=10, i_group_num=10, reg_clarity=10, lambda_=5,\n",
    "              mf_training_iters=200,change_mode=False,convergence_threshold=0.0001,max_rank=3,interaction_restrict='intra', si_approach ='als')\n",
    "model.fit(tr_x, val_x, tr_y, val_y, tr_Xi, val_Xi, tr_idx, val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability of Usage Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "usage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "date = '2014-11-01' #other date than activity\n",
    "Usage_Agent_i = Usage_Agent(usage_df, \"Dishwasher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = Usage_Agent_i.train_test_split(usage_df, \"2014-11-01\", train_start='2013-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "usage = Usage_Agent(usage_df, \"Dishwasher\")\n",
    "model = usage.fit_knn(X_train, np.ravel(y_train)) # Change it to the model you want to use\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pred_model_knn = usage.fit_knn(X_train,y_train)\n",
    "pred_model_rf = usage.fit_random_forest(X_train,y_train)\n",
    "pred_model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pred_model_knn = usage.fit_knn(X_train,np.ravel(y_train))\n",
    "pred_model_rf = usage.fit_random_forest(X_train,np.ravel(y_train))\n",
    "pred_model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predictive_models = [pred_model_knn, pred_model_rf]\n",
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#compute prediction at day t (see date used for split sampling)\n",
    "import numpy as np\n",
    "y_hat = usage.predict(model, X_test)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test#.iloc[2]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### LIME Function\n",
    "from lime import lime_tabular\n",
    "\n",
    "i = 0\n",
    "\n",
    "for pred_model in predictive_models:\n",
    "    predictive_model = pred_model\n",
    "\n",
    "    # LIME\n",
    "    explainability_model = 'LIME'\n",
    "    start_time = time.time()\n",
    "    explainer = lime_tabular.LimeTabularExplainer(training_data = np.array(X_train),\n",
    "                                              mode = \"regression\",\n",
    "                                              feature_names = X_train.columns,\n",
    "                                              categorical_features = [0])\n",
    "\n",
    "    exp = explainer.explain_instance(data_row = X_test, #removed iloc \n",
    "                                predict_fn = pred_model.predict)\n",
    "\n",
    "    exp.show_in_notebook(show_table = True)\n",
    "    end_time = time.time()\n",
    "    difference_time = end_time - start_time\n",
    "    #exp.as_list()\n",
    "    exp_eval_df.loc[i+1] = [explainability_model, predictive_model, start_time, end_time, difference_time]\n",
    "\n",
    "    # SHAP\n",
    "\n",
    "    explainability_model = 'SHAP'\n",
    "    start_time = time.time()\n",
    "\n",
    "    explainer = shap.KernelExplainer(pred_model.predict_proba, X_train)\n",
    "    print(explainer)\n",
    "    shap_values = explainer.shap_values(X_test)#.iloc[0,:]) #removed iloc\n",
    "    print(shap_values)\n",
    "    display(shap.force_plot(explainer.expected_value[1], shap_values[1], X_test))#removed iloc, display necessary to show multiple\n",
    "    \n",
    "    end_time = time.time()\n",
    "    difference_time = end_time - start_time\n",
    "\n",
    "    exp_eval_df.loc[i+2] = [explainability_model, predictive_model, start_time, end_time, difference_time]\n",
    "\n",
    "    i = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainale Boosting Classifier - Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = ExplainableBoostingClassifier()\n",
    "ebm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ebm_local = ebm.explain_local(X_test, y_test)\n",
    "#show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Appendix A1: Complete Recommendation Agent Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GG0aBAe4ODM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Recommendation_Agent:\n",
    "    def __init__(\n",
    "        self, activity_input, usage_input, load_input, price_input, shiftable_devices\n",
    "    ):\n",
    "        self.activity_input = activity_input\n",
    "        self.usage_input = usage_input\n",
    "        self.load_input = load_input\n",
    "        self.price_input = price_input\n",
    "        self.shiftable_devices = shiftable_devices\n",
    "        self.Activity_Agent = Activity_Agent(activity_input)\n",
    "        # create dicionnary with Usage_Agent for each device\n",
    "        self.Usage_Agent = {\n",
    "            name: Usage_Agent(usage_input, name) for name in shiftable_devices\n",
    "        }\n",
    "        self.Load_Agent = Load_Agent(load_input)\n",
    "        self.Price_Agent = Price_Agent(price_input)\n",
    "\n",
    "    # calculating costs\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def electricity_prices_from_start_time(self, date):\n",
    "        import pandas as pd\n",
    "\n",
    "        prices_48 = self.Price_Agent.return_day_ahead_prices(date)\n",
    "        prices_from_start_time = pd.DataFrame()\n",
    "        for i in range(24):\n",
    "            prices_from_start_time[\"Price_at_H+\" + str(i)] = prices_48.shift(-i)\n",
    "        # delete last 24 hours\n",
    "        prices_from_start_time = prices_from_start_time[:-24]\n",
    "        return prices_from_start_time\n",
    "\n",
    "    def cost_by_starting_time(self, date, device, evaluation=False):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        # get electriciy prices following every device starting hour with previously defined function\n",
    "        prices = self.electricity_prices_from_start_time(date)\n",
    "        # build up table with typical load profile repeated for every hour (see Load_Agent)\n",
    "        if not evaluation:\n",
    "            device_load = self.Load_Agent.pipeline(\n",
    "                self.load_input, date, self.shiftable_devices\n",
    "            ).loc[device]\n",
    "        else:\n",
    "            # get device load for one date\n",
    "            device_load = evaluation[\"load\"][date].loc[device]\n",
    "        device_load = pd.concat([device_load] * 24, axis=1)\n",
    "        # multiply both tables and aggregate costs for each starting hour\n",
    "        costs = np.array(prices) * np.array(device_load)\n",
    "        costs = np.sum(costs, axis=0)\n",
    "        # return an array of size 24 containing the total cost at each staring hour.\n",
    "        return costs\n",
    "    \n",
    "    # creating recommendations\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def recommend_by_device(\n",
    "        self,\n",
    "        date,\n",
    "        device,\n",
    "        model_type,\n",
    "        activity_prob_threshold,\n",
    "        usage_prob_threshold,\n",
    "        evaluation=False,\n",
    "    ):\n",
    "        import numpy as np\n",
    "\n",
    "        # add split params as input\n",
    "        # IN PARTICULAR --> Specify date to start training\n",
    "        split_params = {\n",
    "            \"train_start\": \"2013-11-01\",\n",
    "            \"test_delta\": {\"days\": 1, \"seconds\": -1},\n",
    "            \"target\": \"activity\",\n",
    "        }\n",
    "        # compute costs by launching time:\n",
    "        costs = self.cost_by_starting_time(date, device, evaluation=evaluation)\n",
    "        # compute activity probabilities\n",
    "        if not evaluation:\n",
    "            activity_probs = self.Activity_Agent.pipeline(self.activity_input, date, model_type, split_params)\n",
    "        else:\n",
    "            # get activity probs for date\n",
    "            activity_probs = evaluation[\"activity\"][date]\n",
    "\n",
    "        # set values above threshold to 1. Values below to Inf\n",
    "        # (vector will be multiplied by costs, so that hours of little activity likelihood get cost = Inf)\n",
    "        activity_probs = np.where(activity_probs >= activity_prob_threshold, 1, float(\"Inf\"))\n",
    "\n",
    "        # add a flag in case all hours have likelihood smaller than threshold\n",
    "        no_recommend_flag_activity = 0\n",
    "        if np.min(activity_probs) == float(\"Inf\"):\n",
    "            no_recommend_flag_activity = 1\n",
    "\n",
    "        # compute cheapest hour from likely ones\n",
    "        best_hour = np.argmin(np.array(costs) * np.array(activity_probs))\n",
    "\n",
    "        # compute likelihood of usage:\n",
    "        if not evaluation:\n",
    "            usage_prob = self.Usage_Agent[device].pipeline(self.usage_input, date, model_type, split_params[\"train_start\"])\n",
    "        else:\n",
    "            # get usage probs\n",
    "            name = (\"usage_\"+ device.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "            usage_prob = evaluation[name][date]\n",
    "\n",
    "        no_recommend_flag_usage = 0\n",
    "        if usage_prob < usage_prob_threshold:\n",
    "            no_recommend_flag_usage = 1\n",
    "\n",
    "        return {\n",
    "            \"recommendation_date\": [date],\n",
    "            \"device\": [device],\n",
    "            \"best_launch_hour\": [best_hour],\n",
    "            \"no_recommend_flag_activity\": [no_recommend_flag_activity],\n",
    "            \"no_recommend_flag_usage\": [no_recommend_flag_usage],\n",
    "            \"recommendation\": [\n",
    "                best_hour\n",
    "                if (no_recommend_flag_activity == 0 and no_recommend_flag_usage == 0)\n",
    "                else np.nan\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    # vizualizing the recommendations\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def recommendations_on_date_range(\n",
    "        self, date_range, activity_prob_threshold=0.6, usage_prob_threshold=0.5\n",
    "    ):\n",
    "        import pandas as pd\n",
    "\n",
    "        recommendations = []\n",
    "        for date in date_range:\n",
    "            recommendations.append(self.pipeline(date, activity_prob_threshold, usage_prob_threshold))\n",
    "            output = pd.concat(recommendations)\n",
    "        return output\n",
    "\n",
    "    def visualize_recommendations_on_date_range(self, recs):\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for device in recs[\"device\"].unique():\n",
    "            plot_device = recs[recs[\"device\"] == device]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=plot_device[\"recommendation_date\"],\n",
    "                    y=plot_device[\"recommendation\"],\n",
    "                    mode=\"lines\",\n",
    "                    name=device,\n",
    "                )\n",
    "            )\n",
    "        fig.show()\n",
    "\n",
    "    def histogram_recommendation_hour(self, recs):\n",
    "        import seaborn as sns\n",
    "\n",
    "        ax = sns.displot(recs, x=\"recommendation\", binwidth=1)\n",
    "        ax.set(xlabel=\"Hour of Recommendation\", ylabel=\"counts\")\n",
    "    \n",
    "    # pipeline function: create recommendations\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def pipeline(self, date, activity_prob_threshold, usage_prob_threshold, evaluation=False):\n",
    "        import pandas as pd\n",
    "\n",
    "        recommendations_by_device = self.recommend_by_device(\n",
    "            date,\n",
    "            self.shiftable_devices[0],\n",
    "            activity_prob_threshold,\n",
    "            usage_prob_threshold,\n",
    "            evaluation=evaluation,\n",
    "        )\n",
    "        recommendations_table = pd.DataFrame.from_dict(recommendations_by_device)\n",
    "\n",
    "        for device in self.shiftable_devices[1:]:\n",
    "            recommendations_by_device = self.recommend_by_device(\n",
    "                date,\n",
    "                device,\n",
    "                activity_prob_threshold,\n",
    "                usage_prob_threshold,\n",
    "                evaluation=evaluation,\n",
    "            )\n",
    "            recommendations_table = recommendations_table.append(\n",
    "                pd.DataFrame.from_dict(recommendations_by_device)\n",
    "            )\n",
    "        return recommendations_table"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_Recommendation_Agent.ipynb",
   "provenance": [
    {
     "file_id": "14_CTWNFpGdlXrPilsUe_TiJhs9Sq9Auv",
     "timestamp": 1607441655852
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}