{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*written by: Julia Knoblauch, Jana Vihs, Annika Boer, Kai Ingo Schewina*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Recommendation Systems for Energy-Efficient Smart Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- Annika: xgboost, lime liste von outputs\n",
    "- Jana: GAMMLI - output, recommendation after weather & xai\n",
    "- Julia: activity eval fertig machen, usage eval sobald hourly, shap liste outputs\n",
    "- Kai: usage hourly, weather dass pipeline runnt\n",
    "\n",
    "Weiteres:\n",
    "- EDA: auf germain paper anpassen und für weather updaten\n",
    "- evaluierung runnen für (alle) households\n",
    "- explanability agent erstellen: activity_agent & usage agent zusammenführen für recommendation\n",
    "- hyperparameter tuning: über performance evaluation agent\n",
    "- (other datasets)\n",
    "\n",
    "Sammlung Hinweise:\n",
    "- warning bei usage/load wenn man evaluierung runnt zu format --> beheben?\n",
    "(warning bei auc/load: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().)\n",
    "- xgb: use_label_encoder=False eingestellt damit warning weg geht: problem?\n",
    "- explainability evaluation atm only test first 3 rows (set to len(X_test)) for final eval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abstract\n",
    "should actually be before TOC and not numbered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of contents - cleaned\n",
    "1. Introduction\n",
    "2. Literature Review\n",
    "    2.1 Explainable AI:\n",
    "    2.2 Recommender Systems in SH\n",
    "    2.3 Explainability in RS\n",
    "3. Methodology\n",
    "    3.1 Recommender System\n",
    "    3.2 Algorithms XAI\n",
    "        - Taxonomy\n",
    "        - Model-intrinsic: logit, knn, rf, gam\n",
    "        - Model-agnostic\n",
    "            Feature Importance: LIME, SHAP, plus evtl. CIU, DALEX, MAPLE\n",
    "    3.3 Algorithms prediction:\n",
    "    adaboost, xgboost\n",
    "\n",
    "4. Experimental Design\n",
    "    4.1 data organization\n",
    "    4.2 Evaluation\n",
    "        - Performance\n",
    "        - Explainability\n",
    "5. Results\n",
    "    5.1 Performance\n",
    "    5.2 Explainability\n",
    "    5.3 Decision for model & XAI model\n",
    "    5.4 Final result of RS: Recommendation + Explanation\n",
    "6. Discussion\n",
    "    - Contributions\n",
    "    - Limitations\n",
    "    - Implications\n",
    "    - Recommendations\n",
    "\n",
    "7. Conclusion\n",
    "    - Future Research\n",
    "References"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Introduction:\n",
    "    - Problem setting: ?\n",
    "    - Current Recommender System --> agents + Bild (Jana)\n",
    "\n",
    "2. Literature Review (Kai) --> Tabelle\n",
    "    - Explainable AI (XAI): review zusammenfassen und aktualisieren\n",
    "    - Recommender Systems in SH: Fokus\n",
    "    - Explainability in RS: Fokus\n",
    "3. Methodology (i.e., the ML techniques that you apply)\n",
    "    - Recommender System??\n",
    "    - Algorithms XAI\n",
    "        - Taxonomy (Bild) (Annika)\n",
    "        - Model-intrinsic (Annika)\n",
    "        - Model-agnostic (Machine Learning Interpretability: A Survey on Methods and Metrics)\n",
    "            - LIME (Annika)\n",
    "            - SHAP (Julia)\n",
    "            - other approaches?\n",
    "        - vorgestellte Tabelle aus Präsi formaler --> Vergleich\n",
    "    - Algorithms prediction (Jana, Formel für finales Modell, mit paper)\n",
    "        xgboost, adaboost, knn, logit, rf, glm?\n",
    "\n",
    "4. Experimental Design (including EDA, data organization, performance measures, etc.)\n",
    "    - data organization (+ EDA von denen auf Wetterdaten + NAs Wetterdaten)\n",
    "        - agents system (Bild) (Jana)\n",
    "        - restriction to 2 households\n",
    "        - with & without weather data (EDA, Struktur, Herkunft) (Kai)\n",
    "    - Evaluation\n",
    "        - Performance (Kai?)\n",
    "        - Explainability (feature importance) (Julia, Annika)\n",
    "5. Results\n",
    "    - Performance (Kai?)\n",
    "    - Explainability (Julia, Annika)\n",
    "    - Decision for model & XAI model\n",
    "    - Final result of RS: Recommendation + Explanation (Jana)\n",
    "6. Discussion\n",
    "    - ...\n",
    "    - Contributions\n",
    "    - Limitations\n",
    "    - Implications\n",
    "    - Recommendations\n",
    "    - Future Research: Neural Networks\n",
    "\n",
    "e.g. include:\n",
    "    - no user\n",
    "    - runtime problems\n",
    "        - api weather data \n",
    "        - 4 models, 20 households\n",
    "        - Beispiel, wie lange es dauern würde, eine recommendation zu laden (aber nur 1x am Tag)\n",
    "\n",
    "7. Conclusion\n",
    "References"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Literature Review\n",
    "https://christophm.github.io/interpretable-ml-book/\n",
    "https://github.com/wangyongjie-ntu/Awesome-explainable-AI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explainable AI (XAI): review zusammenfassen und aktualisieren\n",
    "4 Richtungen der local approaches kurz zusammenfassen:\n",
    "**Feature Importance (FI)**\n",
    "**Rule Based (RB)**\n",
    "**Prototypes (PR)**\n",
    "**Counterfactuals (CF)**\n",
    "\n",
    "-->+/- der appraoches nennen damit man in 3. erklären kann dass man FI nimmt und die Vorteile von RB\n",
    "durch den Output als Liste."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recommender Systems in SH: Fokus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explainability in RS: Fokus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Methodology"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Recommender System"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 XAI Alogorithms\n",
    "### Taxonomy?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model intrinsic\n",
    "paper:\n",
    "https://arxiv.org/pdf/1811.10154.pdf\n",
    "https://arxiv.org/abs/2006.06466\n",
    "\n",
    "Logit as benchmark?, GAMMLI, Random Forest\n",
    "### Model Agnostic Approaches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LIME\n",
    "To do:\n",
    "Is mode= 'regression' correct?\n",
    "\n",
    "xgboost: resources to implement\n",
    "https://github.com/marcotcr/lime/issues/334\n",
    "https://github.com/adriamoya/xgboost_default_companies/blob/master/main.py\n",
    "mention current approaches (no implementation as far as I know except SP-LIME for global)\n",
    "https://arxiv.org/abs/2106.07875\n",
    "\n",
    "Benchmarking and Survey of Explanation Methods for\n",
    "Black Box Models - see chapter 4.1 for variants of LIME\n",
    "\n",
    "https://arxiv.org/abs/2012.00093"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SHAP\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fstrumbelj10a%2Fstrumbelj10a.pdf&clen=690800&chunk=true\n",
    "--> AnEfficient Explanation of Individual Classifications using Game Theory (2010)\n",
    "\n",
    "Aktueller Stand:\n",
    "logit: kernel (linearExplainer möglich aber dann anderes array als output für shapley values)\n",
    "ada: kernel (eigene implementierung in treee shap möglich aber internetversion hat bei mir auf die schnelle nicht funktioniert)\n",
    "knn: kernel, mask doesnt work\n",
    "rf: tree without predict proba\n",
    "xgb: tree\n",
    "\n",
    "Möglichkeit:\n",
    "X_test ganz berechnen: aber glaub gar nicht so viel schneller und\n",
    "Zeit wäre verzerrt\n",
    "\n",
    "Aktuell implementiert für KernelExplainer: vorher kmeans weil viel schneller\n",
    "aber keine info zu tradeoff und keine wirklichen recommendations wie k= gesetzt werden sollte\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other approaches to research:\n",
    "https://www.linkedin.com/posts/hima-lakkaraju-b130217_himachilpptx-activity-6814735906092703744-fOYe/\n",
    "https://interpretable-ml-class.github.io/\n",
    "https://github.com/anguyen8/XAI-papers\n",
    "https://towardsdatascience.com/the-how-of-explainable-ai-explainable-modelling-55c8c43d7bed\n",
    "https://github.com/SeldonIO/alibi\n",
    "https://github.com/pbiecek/xai_resources\n",
    "https://github.com/interpretml/interpret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MUSE**\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fcs.stanford.edu%2Fpeople%2Fjure%2Fpubs%2Fexplanations-aies19.pdf&clen=1929781&chunk=true\n",
    "\"Faithful and Customizable Explanations of Black Box Models\"\n",
    "--> give concrete measures for xai other than fidelity\n",
    "but: not open source package?\n",
    "\n",
    "slides:\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Finterpretable-ml-class.github.io%2Fslides%2FLecture_11.pdf&clen=1064587&chunk=true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BETA**\n",
    "https://arxiv.org/pdf/1707.01154.pdf\n",
    "Interpretable & Explorable Approximations of Black Box Models\n",
    "--> global approach, no package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anchors (2018)**\n",
    "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf\n",
    "https://github.com/marcotcr/anchor\n",
    "\n",
    "if- then rules\n",
    "\n",
    "\"An anchor explanation is a rule that sufficiently “anchors” the\n",
    "prediction locally – such that changes to the rest of the feature\n",
    "values of the instance do not matter. In other words, for instances\n",
    "on which the anchor holds, the prediction is (almost)\n",
    "always the same.\" (paper, p. 1)\n",
    "\n",
    "\n",
    "--> humans should be able to forecast the model's behavior on unseen instances\n",
    "(human) precicion: share of correctly classified/predicted instances by human\n",
    "advantage of anchors: humans are more confident compared to e.g. LIME which only applies to specfic instance\n",
    "and *coverge* (are of application) is unclear\n",
    "\n",
    "\"Anchors are intuitive, easy to comprehend, and\n",
    "have extremely clear coverage – they only apply when all the\n",
    "conditions in the rule are met, and if they apply the precision\n",
    "is high (by design).\" (paper)\n",
    "\n",
    "--> tabular, text, images\n",
    "\n",
    "--> also based on pertubation: fix rules (A) and set as condition\n",
    "--> advantage: short rules are easier to interpret than multi-layered decicion list\n",
    "--> local since easier to represent complex models, advantage over additive models\n",
    "as even local instances can be non-linenear and complex and lead to wrong conclusions from user,\n",
    "less mental strain than additive models --> coverage problem\n",
    "--> generalization to other instances via anchors, capture non-linearity+\n",
    "\n",
    "\"Anchors are intuitive, easy to comprehend, and\n",
    "have extremely clear coverage – they only apply when all the\n",
    "conditions in the rule are met, and if they apply the precision\n",
    "is high (by design).\"\n",
    "\n",
    "- similar logic than Probabilistic Inductive Logic\n",
    "Programming (ILP) but no dataset a priori; instead \"estimate precision and coverage bounds under D\" with permutation and black-box model\n",
    "\n",
    "Experiments:\n",
    "simulated data calculates precicion (what fraction\n",
    "of the instances they predict after seeing explanations)\n",
    "and precision (what fraction of the predictions were correct)\n",
    "--> also uses coverage per explanation (plot)\n",
    "\n",
    "--> simulated experiment comaring LIME and Anchors (we could also apply this\n",
    "on our dataset) (https://github.com/marcotcr/anchor-experiments)\n",
    "--> real study however only few participants and they are part of ML course so not\n",
    "that great generalizability to normal users\n",
    "--> real study: easier to predict, higher precicion and confidence\n",
    "but for our case: not as important to predict unknown cases but rather this case\n",
    "\n",
    "\n",
    "concept also used in:\n",
    "https://github.com/viadee/magie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Counterfactual Instances**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from paper Bodria et. a. (2021):\n",
    "see table 1 for overview on what else we could look at\n",
    "\n",
    "**DALEX**\n",
    "variable attribution approach\n",
    "\n",
    "and also EDA options\n",
    "https://github.com/ModelOriented/DALEX\n",
    "e.g. ceteris paribus plots (What-if analysis) for different features and predictive models\n",
    "\n",
    "https://github.com/ModelOriented/DALEXtra\n",
    "ROC if expaliner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**py-ciu**\n",
    "https://arxiv.org/abs/2006.00199\n",
    "Explanations of Black-Box Model Predictionsby Contextual Importance and Utility\n",
    "https://github.com/TimKam/py-ciu\n",
    "--> also nice because working textual output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MAPLE**\n",
    "https://github.com/GDPlumb/MAPLE\n",
    "https://arxiv.org/abs/1807.02910\n",
    "aber kein (schöner) output?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Prediction Algorithms\n",
    "**XGBoost**\n",
    "\n",
    "**AdaBoost**\n",
    "\n",
    "**K-Nearest-Neighbour**\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "**Random Forest**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost\n",
    "\n",
    "Achtung: depreciation warning for XGB\n",
    "--> label encoder = False eingestellt\n",
    "--> noch mehr verändern?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Experimental Design (including EDA, data organization, performance measures, etc.)\n",
    "EDA: https://github.com/EthicalML/xai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### data organization (+ EDA von denen auf Wetterdaten + NAs Wetterdaten)\n",
    "        - agents system (Bild) (Jana)\n",
    "        - restriction to 2 households\n",
    "        - with & without weather data (EDA, Struktur, Herkunft) (Kai)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance (Kai?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####Explainability (feature importance) (Julia, Annika)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "42. F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. arXiv\n",
    "preprint arXiv:1702.08608, 2017.\n",
    "43. F. Doshi-Velez and B. Kim. Considerations for evaluation and generalization in interpretable machine\n",
    "learning. In Explainable and interpretable models in computer vision and machine learning, pages 3{\n",
    "17. Springer, 2018.\n",
    "--> functionally grounded\n",
    "--> also propose other qualitative metrics\n",
    "\n",
    "Problem: MSEE eher für regression\n",
    "--> Accuracy (also AUC and ROC)\n",
    "--> Precicion (What proportion of positive identifications was actually correct?)\n",
    "--> Recall (What proportion of actual positives was identified correctly?)\n",
    "\n",
    "ausrechnen? aber dann brauchen wir einen treshold\n",
    "\n",
    "faithfulness: s. AIX360\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Discussion and Limitations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "paper: 'Human-in-the-Loop Interpretability Prior'\n",
    "paper: https://dl.acm.org/doi/abs/10.1145/3351095.3375624"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}