{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*written by: Julia Knoblauch, Jana Vihs, Annika Boer, Kai Ingo Schewina*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Recommendation Systems for Energy-Efficient Smart Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- Annika: usage agent evaluierung von activity agent übernehmen, --> xai evaluierung: Tabelle erstellen ähnlich zu agent_scores_to_summary/ oder anders eval werte hier als Tabelle einzufügen , lime überprüfen in eval: regression ersetzt durch classification parameter?\n",
    "- Jana: recommendation\n",
    "- Julia: SHAP chapter (SHAP eval nochmal prüfen)\n",
    "- Kai: literature\n",
    "\n",
    "Weiteres:\n",
    "- decide on citation approach (vlt einfach so einfügen in text)\n",
    "- hyperparameter tuning: über performance evaluation agent\n",
    "- Eval allgemein schonmal laufen lassen und eventuell modelle ausschließen für xai-evaluation\n",
    "- measure for interpretability/understandability: disregard because we just apply the same appraoches on all to get recommendation?\n",
    "- weitere packages checken: DALEX,MAPLE (siehe unten)\n",
    "- EDA: auf germain paper anpassen und für weather updaten\n",
    "- evaluierung runnen für (alle) households\n",
    "- explanability agent erstellen: outcome explainability des activity_agents & usage agents zusammenführen für recommendation\n",
    "- (other datasets)\n",
    "\n",
    "Sammlung Hinweise:\n",
    "- warning bei usage/load wenn man evaluierung runnt zu format --> beheben?\n",
    "(warning bei auc/load: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().)\n",
    "- xgb: use_label_encoder=False eingestellt damit warning weg geht: problem?\n",
    "- explainability evaluation atm only test first 3 rows (set to len(X_test)) for final eval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abstract\n",
    "should actually be before TOC and not numbered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of contents - cleaned\n",
    "1. Introduction\n",
    "2. Literature Review\n",
    "    2.1 Explainable AI:\n",
    "    2.2 Recommender Systems in SH\n",
    "    2.3 Explainability in RS\n",
    "3. Methodology\n",
    "    3.1 Recommender System\n",
    "    3.2 Algorithms XAI\n",
    "        - Taxonomy\n",
    "        - Model-intrinsic: logit, knn, rf, gam\n",
    "        - Model-agnostic\n",
    "            Feature Importance: LIME, SHAP, plus evtl. CIU, DALEX, MAPLE\n",
    "    3.3 Algorithms prediction:\n",
    "    adaboost, xgboost\n",
    "\n",
    "4. Experimental Design\n",
    "    4.1 data organization\n",
    "    4.2 Evaluation\n",
    "        - Performance\n",
    "        - Explainability\n",
    "5. Results\n",
    "    5.1 Performance\n",
    "    5.2 Explainability\n",
    "    5.3 Umbenennen: Decision for model & XAI model\n",
    "    5.4 Final result of RS: Recommendation + Explanation\n",
    "6. Discussion\n",
    "    - Contributions\n",
    "    - Limitations\n",
    "    - Implications\n",
    "    - Recommendations\n",
    "    - Future Research\n",
    "\n",
    "7. Conclusion\n",
    "\n",
    "References"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Introduction:\n",
    "    - Problem setting: ?\n",
    "    - Current Recommender System --> agents + Bild (Jana)\n",
    "\n",
    "2. Literature Review (Kai) --> Tabelle\n",
    "    - Explainable AI (XAI): review zusammenfassen und aktualisieren\n",
    "    - Recommender Systems in SH: Fokus\n",
    "    - Explainability in RS: Fokus\n",
    "3. Methodology (i.e., the ML techniques that you apply)\n",
    "    - Recommender System??\n",
    "    - Algorithms XAI\n",
    "        - Taxonomy (Bild) (Annika)\n",
    "        - Model-intrinsic (Annika)\n",
    "        - Model-agnostic (Machine Learning Interpretability: A Survey on Methods and Metrics)\n",
    "            - LIME (Annika)\n",
    "            - SHAP (Julia)\n",
    "            - other approaches?\n",
    "        - vorgestellte Tabelle aus Präsi formaler --> Vergleich\n",
    "    - Algorithms prediction (Jana, Formel für finales Modell, mit paper)\n",
    "        xgboost, adaboost, knn, logit, rf, glm?\n",
    "\n",
    "4. Experimental Design (including EDA, data organization, performance measures, etc.)\n",
    "    - data organization (+ EDA von denen auf Wetterdaten + NAs Wetterdaten)\n",
    "        - agents system (Bild) (Jana)\n",
    "        - restriction to 2 households\n",
    "        - with & without weather data (EDA, Struktur, Herkunft) (Kai)\n",
    "    - Evaluation\n",
    "        - Performance (Kai?)\n",
    "        - Explainability (feature importance) (Julia, Annika)\n",
    "5. Results\n",
    "    - Performance (Kai?)\n",
    "    - Explainability (Julia, Annika)\n",
    "    - umbennenen: Decision for model & XAI model\n",
    "    - Final result of RS: Recommendation + Explanation (Jana)\n",
    "6. Discussion\n",
    "    - ...\n",
    "    - Contributions\n",
    "    - Limitations: making usage daily\n",
    "    - Implications\n",
    "    - Recommendations\n",
    "    - Future Research: Neural Networks, mehrere recommendations pro Tag,\n",
    "\n",
    "e.g. include:\n",
    "    - no user\n",
    "    - runtime problems\n",
    "        - api weather data \n",
    "        - 4 models, 20 households\n",
    "        - Beispiel, wie lange es dauern würde, eine recommendation zu laden (aber nur 1x am Tag)\n",
    "\n",
    "7. Conclusion: zusammenfassung in 2 abschnitten\n",
    "References"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Necessary packages to run notebook:\n",
    "from IPython.display import Latex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Literature Review\n",
    "https://christophm.github.io/interpretable-ml-book/\n",
    "https://github.com/wangyongjie-ntu/Awesome-explainable-AI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explainable AI (XAI)\n",
    "4 Richtungen der local approaches kurz zusammenfassen:\n",
    "**Feature Importance (FI)**\n",
    "**Rule Based (RB)**\n",
    "**Prototypes (PR)**\n",
    "**Counterfactuals (CF)**\n",
    "\n",
    "-->+/- der appraoches nennen damit man in 3. erklären kann dass man FI nimmt und die Vorteile von RB\n",
    "durch den Output als Liste.\n",
    "\n",
    "### Criteria for XAI (Julia)\n",
    "\n",
    "Robnik-Šikonja and Bohanec (2018) suggested properties of the explainers that differentiate the appraoches:\n",
    "- Expressive Power: What logic does the explanation follow and which languge do they display?\n",
    "- Translucency: Does the model decompose the predictive model (decomposition), treat the model as blackbox (pedagocial) or mix these approaches?\n",
    "- Portability: Can the model be used on multiple predictive models i.e. is it model specific or agnostic?\n",
    "- Algorithmic complexity: How time consuming are the computations?\n",
    "\n",
    "Carvalho et. al (2019) propose additional properties that are useful in our situation:\n",
    "- Stability: How stable are the explanations?\n",
    "- Data Sampling: How is the data sampled e.g. for shapley value estimation?\n",
    "- Shuffling/Permutation: How does the algorithms shuffle the data to derive the feature importance?\n",
    "\n",
    "The explainability appproaches will be evaluated on these critera in section 3.\n",
    "\n",
    "Additionally, there are three options to evaluate the explainability approaches (Doshi-Valez & Kim, 2018).\n",
    "Firstly, *Application grounded Evaluation* involves using humans in the real setting i.e. we need subjects that use our\n",
    "recommender system in their smart home and evaluate the usefulness of the offered explanation. Secondly, another possibility\n",
    "would be to use *Human-grounded Metrics* that would involve humans but not necessarily include the explanability\n",
    "appraoch in a real task. In our case we could do experiments in which subjects have to choose between the explanability\n",
    "approach e.g. based on their preference.\n",
    "Thirdly, Doshi-Valez and Kim (2018) propose *Functionally-grounded Evaluation* that use proxies to evaluate the\n",
    "explainability approach.\n",
    "We will use the third approch as evaluations that involve the actual application of the recommender system are out of scope\n",
    "for this seminar and the evaluation with humans and simplified tasks might not offer much external validity to the real setting.\n",
    "*Could also be part of dicussion/ Future research to show different versions of our final output and what users like/disklike*\n",
    "\n",
    "We will introduce the proxies for evaluation of the explanations in chapter 4."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recommender Systems in SH: Fokus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explainability in RS: Fokus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Methodology"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Recommender System"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 XAI Alogorithms\n",
    "### Taxonomy?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model intrinsic\n",
    "paper:\n",
    "https://arxiv.org/pdf/1811.10154.pdf\n",
    "https://arxiv.org/abs/2006.06466\n",
    "\n",
    "Logit as benchmark?, GAMMLI, Random Forest (?)\n",
    "### Model Agnostic Approaches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LIME\n",
    "To do:\n",
    "Is mode= 'regression' correct?/ I changed it to classification but checking again if correct\n",
    "\n",
    "xgboost: resources to implement\n",
    "https://github.com/marcotcr/lime/issues/334\n",
    "https://github.com/adriamoya/xgboost_default_companies/blob/master/main.py\n",
    "mention current approaches (no implementation as far as I know except SP-LIME for global)\n",
    "https://arxiv.org/abs/2106.07875\n",
    "\n",
    "Benchmarking and Survey of Explanation Methods for\n",
    "Black Box Models - see chapter 4.1 for variants of LIME\n",
    "\n",
    "https://arxiv.org/abs/2012.00093"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SHAP (Julia)\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume11%2Fstrumbelj10a%2Fstrumbelj10a.pdf&clen=690800&chunk=true\n",
    "--> AnEfficient Explanation of Individual Classifications using Game Theory (2010)\n",
    "\n",
    "Aktuell implementiert für KernelExplainer: vorher kmeans weil viel schneller\n",
    "aber keine info zu tradeoff und keine wirklichen recommendations wie k= gesetzt werden sollte\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other approaches to research:\n",
    "https://www.linkedin.com/posts/hima-lakkaraju-b130217_himachilpptx-activity-6814735906092703744-fOYe/\n",
    "https://interpretable-ml-class.github.io/\n",
    "https://github.com/anguyen8/XAI-papers\n",
    "https://towardsdatascience.com/the-how-of-explainable-ai-explainable-modelling-55c8c43d7bed\n",
    "https://github.com/SeldonIO/alibi\n",
    "https://github.com/pbiecek/xai_resources\n",
    "https://github.com/interpretml/interpret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don't put in eval but maybe mention:\n",
    "**MUSE**\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fcs.stanford.edu%2Fpeople%2Fjure%2Fpubs%2Fexplanations-aies19.pdf&clen=1929781&chunk=true\n",
    "\"Faithful and Customizable Explanations of Black Box Models\"\n",
    "--> give concrete measures for xai other than fidelity\n",
    "but: not open source package?\n",
    "\n",
    "slides:\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Finterpretable-ml-class.github.io%2Fslides%2FLecture_11.pdf&clen=1064587&chunk=true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don't put in eval but maybe mention:\n",
    "**BETA**\n",
    "https://arxiv.org/pdf/1707.01154.pdf\n",
    "Interpretable & Explorable Approximations of Black Box Models\n",
    "--> global approach, no package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from paper Bodria et. a. (2021):\n",
    "see table 1 for overview on what else we could look at\n",
    "\n",
    "**DALEX**\n",
    "variable attribution approach --> check if they do surrogate model/ we would need to add to eval\n",
    "\n",
    "and also EDA options\n",
    "https://github.com/ModelOriented/DALEX\n",
    "e.g. ceteris paribus plots (What-if analysis) for different features and predictive models\n",
    "\n",
    "https://github.com/ModelOriented/DALEXtra\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don't put in eval but maybe mention:\n",
    "**py-ciu**\n",
    "https://arxiv.org/abs/2006.00199\n",
    "Explanations of Black-Box Model Predictionsby Contextual Importance and Utility\n",
    "https://github.com/TimKam/py-ciu\n",
    "--> also nice because working textual output\n",
    "--> aber: kein prediction model daraus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MAPLE**\n",
    "-> check if they do surrogate model/ we would need to add to eval\n",
    "https://github.com/GDPlumb/MAPLE\n",
    "https://arxiv.org/abs/1807.02910\n",
    "aber keine ahnung wie output ist & schlechte documentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Prediction Algorithms\n",
    "**XGBoost**\n",
    "\n",
    "**AdaBoost**\n",
    "\n",
    "**K-Nearest-Neighbour**\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "**Random Forest**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost\n",
    "\n",
    "Achtung: depreciation warning for XGB\n",
    "--> label encoder = False eingestellt\n",
    "--> noch mehr verändern?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Experimental Design (including EDA, data organization, performance measures, etc.)\n",
    "EDA: https://github.com/EthicalML/xai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### data  (+ EDA von denen auf Wetterdaten + NAs Wetterdaten)\n",
    "        - agents system (Bild) (Jana)\n",
    "        - restriction to 2 households\n",
    "        - with & without weather data (EDA, Struktur, Herkunft) (Kai)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance (Kai)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explainability (Julia)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*altueller stand: in literature research properties of explainability models und hier konkrete metrics*\n",
    "\n",
    "When is comes to our evaluation of the explainability approaches we want to quantitatively compare the\n",
    "individual explainations that the models offer. The literature does no propose one clear framework\n",
    " which metrics to use as it is also highly application depenednt.\n",
    "Nevertheless, we will use the following concepts that are widely applied to determine how well\n",
    " the explainability approaches work (Carvalho et. al, 2019):\n",
    "\n",
    "**Accuracy** Similar to predictive accurary, the accuracy of the explanation method refers to how well the explainable model\n",
    "predicts unseen instances compared to the real outcome. We will use the standard measure of accuracy, precicion and recall.\n",
    "Note: accuracy might be misleading since imbalanced?\n",
    "\n",
    "**Fidelity** This criteria determindes how close the prediction from the explainale model is to the black-box model.\n",
    "Therefore, fidelity describes how well the explainability model is able to imitate the prediction of the black-box model.\n",
    "We will again use the measure used for accuracy but replace the target label by the predictions from the black-box model.\n",
    "Note that Carvalho et. al (2019) mention an interrelation between these two concepts: black-box models with high predictive\n",
    "accuracy and explainations that offer a high fidelity are also highly accurate when it comes to the real predictions. This is\n",
    "the ideal that we would like to achieve.\n",
    "\n",
    "Additionally, we calculate the *Mean-Squared-Explainabilty Error* (MSEE) for every approach to measure not only if\n",
    "the decicion of the black-box model and the explainability approach is the same but also to measure how close they are to each other.\n",
    "This metrics represents how well the explainability approaches are calibrated. The formula of the traditionally used\n",
    "Mean-Squared-Error is adapted in the following way: *citation missing*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ MSEE_{} = \\frac{1}{n} (y_{pred} - y_{expl})^2 $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Interpretability/ Comprehensibility/ Understandability** This critera deals with how well humans actually\n",
    " understand the explanations. Since we have decided on functionally-grounded evaluations\n",
    "we have to rely on proxies like ...\n",
    "*hm entweder vlt das erst danach vergleichen z.b. fidelity für verschiedene anzahl and features?\n",
    "oder: ??\n",
    "oder: nicht vergleichen und sagen dass wir dadurch dass wir\n",
    "alles am ende auf den selben output bringen, dass das nicht nötig ist?*\n",
    "--> hier noch citation wenn was gefunden was wir machen könnten: am coolsten wäre ein plot iwie\n",
    "\n",
    "**Efficiency** Lastly, we will evaluate how efficient the explainability appraoches are in calculating\n",
    "the local explanations. To the end we will measure the time that each method needs to calculate all the\n",
    "local explanations for a day and average the values.\n",
    "\n",
    "Zur Info:\n",
    "--> Precicion (What proportion of positive identifications was actually correct?)\n",
    "--> Recall (What proportion of actual positives was identified correctly?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explainability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Umbenennen: Decision for model & XAI model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final result of RS: Recommendation + Explanation\n",
    "Note:  Carvalho et. al (2019) mention criteria for Human-friendly explanations that we could include here for reasoning\n",
    "our final explanation approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 6. Discussion and Limitations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "paper: 'Human-in-the-Loop Interpretability Prior'\n",
    "paper: https://dl.acm.org/doi/abs/10.1145/3351095.3375624"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}