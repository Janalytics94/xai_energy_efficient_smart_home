{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DRnXoeZJqNX"
   },
   "source": [
    "# **Usage Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwtXD6qiJx3r"
   },
   "source": [
    "The **Usage Agent's** functionnality is to estimate the probability that a particular device will be used on the following day. Within the general recommendation framework, this function is used in order to limit unnecessary recommendations that could irritate the user. Whenever a device is unlikely to be used on the next day (estimated likelihood below a certain threshold), no recommendation will be made.\n",
    "\n",
    "In the present notebook, we will describe how these probabilities are estimated in detail and define the **Usage Agent** class that will be integrated into the recommendation agent.\n",
    "\n",
    "The Usage Agent will use a ML-algorithm on features extracted from the household's electricity consumption data in order to predict the likelihood of use of devices on the next day. For instance, at a given day t-1, it will use all available consumption data until day t-1 in order to predict device usage on day t. The features we will use can be divided into 3 categories: \n",
    "1. Whether activity has been detected in the house in the preceding days (activity detected by electricity consumption).\n",
    "2. Whether the to-be-prediced-device has been used in the previous days.\n",
    "3. Time dummies.\n",
    "\n",
    "Given the limited number of observations for each household, we will need to restrict the complexity of the ML-Algorithm in use. This is the reason why we will use a logit model with a limited number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1xY9gp_4KqO"
   },
   "source": [
    "## **1. Load And Preprocess Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNx0bpISOLUE"
   },
   "source": [
    "This part's only purpose is to load the data used in the Usage Agent. This process is described in detail in the Preparation Agent. \n",
    "\n",
    "**Note: When computing the script with another Household than Household 1 you might need to adapt some parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17JxNkMHKa27"
   },
   "source": [
    "### **1.1 Initialize And Load Python Scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24993,
     "status": "ok",
     "timestamp": 1607623227444,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "XSEU7zeN3udE"
   },
   "outputs": [],
   "source": [
    "from helper_functions import Helper\n",
    "from agents import Preparation_Agent\n",
    "import numpy as np\n",
    "# More ML Models\n",
    "import xgboost as xgb\n",
    "import sklearn \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "helper = Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load household data\n",
    "household = helper.load_household(DATA_PATH, 1)\n",
    "active_appliances = ['Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuV9dYlD4Vkv"
   },
   "source": [
    "### **1.2 Set Parameters For Pre-processing Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24987,
     "status": "ok",
     "timestamp": 1607623227445,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "qw4N4Sq45663"
   },
   "outputs": [],
   "source": [
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "aggregate_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "aggregate_params24_H = {\n",
    "    'resample_param': '24H'\n",
    "}\n",
    "\n",
    "\n",
    "activity_params = {\n",
    "    'active_appliances': ['Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site'],\n",
    "    'threshold': .15\n",
    "}\n",
    "\n",
    "time_params = {\n",
    "    'features': ['hour', 'day_name']\n",
    "}\n",
    "\n",
    "activity_lag_params = {\n",
    "    'features': ['activity'],\n",
    "    'lags': [24, 48, 72]\n",
    "}\n",
    "\n",
    "shiftable_devices = {'Dishwasher', 'Washing Machine'}\n",
    "\n",
    "device = {\n",
    "    'threshold' : .15}\n",
    "\n",
    "activity_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'activity': activity_params,\n",
    "    'aggregate_hour': aggregate_params,\n",
    "    'aggregate_day': aggregate_params24_H,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params,\n",
    "    'shiftable_devices' : shiftable_devices,\n",
    "    'device': device\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsQY8RSZKsqL"
   },
   "source": [
    "### **1.3 Pre-process Data For Input In Device_Usage Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "executionInfo": {
     "elapsed": 73997,
     "status": "ok",
     "timestamp": 1607623276471,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "AOK_zrFr4aK9",
    "outputId": "239cbff3-d8d0-4a8e-a517-d8e30f8aae3e"
   },
   "outputs": [],
   "source": [
    "# calling the preparation pipeline\n",
    "prep = Preparation_Agent(household)\n",
    "df = prep.pipeline_usage(household, activity_pipe_params)\n",
    "df.to_pickle('../data/processed_pickle/trunctuated.pkl')\n",
    "\n",
    "#display all potential variables for predicting device usage likelihood\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL6F5tn6dTsf"
   },
   "source": [
    "## **2.  Constructing the Usage Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aU7AQfEYSS6"
   },
   "source": [
    "### **2.1 Initialize Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T09RUToXR93t"
   },
   "source": [
    "First we define the **Usage Agent class**. It takes as input the data generated by the prep.pipeline_usage function computed above, and the name of the device for which predictions should be made (e.g \"Washing Machine\", \"Dishwasher\"etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73988,
     "status": "ok",
     "timestamp": 1607623276472,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "hZgoXyrpdVfn"
   },
   "outputs": [],
   "source": [
    "class Usage_Agent:\n",
    "    import pandas as pd\n",
    "\n",
    "    def __init__(self, input_df, device):\n",
    "        self.input = input_df\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxZlevgn5nrr"
   },
   "source": [
    "Here we initialize the agent for the device \"Dishwasher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73985,
     "status": "ok",
     "timestamp": 1607623276473,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "Uh-a4Lmi5l_v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "usage = Usage_Agent(df, \"Dishwasher\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlu2qQ8BYXl9"
   },
   "source": [
    "### **2.2 Train_test_split function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ_GrCnOVc2X"
   },
   "source": [
    "The number of data points available to make a prediction for day t increases by one, each time t increases by one. Therefore, we define a custom train_test_split function that automatically puts all data available until day t-1 (incl.) into the training set. The Data for day t (= prediction day) comes into the test set.\n",
    "\n",
    "In order to limit over-fitting the function also filters out the number of features to be taken into account to train the model. Here these are the following:\n",
    "\n",
    "1. Indicator of device usage at day t-1.\n",
    "2. Indicator of device usage at day t-2.\n",
    "3. Indicator of activity in the household in the past two days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73981,
     "status": "ok",
     "timestamp": 1607623276474,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "UYOZSfdNVKCd"
   },
   "outputs": [],
   "source": [
    "#date: the day of prediction\n",
    "#train start: the day from which training starts\n",
    "def train_test_split(self, df, date, train_start='2013-11-01'):\n",
    "    #restrict number of variables\n",
    "    select_vars =  [self.device + '_usage', self.device+ '_usage_lag_1', self.device+ '_usage_lag_2',\t'active_last_2_days']\n",
    "    df = df[select_vars]\n",
    "    #spli train and test\n",
    "    X_train = df.loc[train_start:date, df.columns != self.device + '_usage']\n",
    "    y_train = df.loc[train_start:date, df.columns == self.device + '_usage']\n",
    "    X_test  = df.loc[date, df.columns != self.device + '_usage']\n",
    "    y_test  = df.loc[date , df.columns == self.device + '_usage']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# add to Usage agent\n",
    "setattr(Usage_Agent, 'train_test_split', train_test_split)\n",
    "del train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73978,
     "status": "ok",
     "timestamp": 1607623276474,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "1eQ_lkg_l9-C"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = usage.train_test_split(df, \"2014-11-01\", train_start='2013-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 73976,
     "status": "ok",
     "timestamp": 1607623276476,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "UCSNjGPUX3aT",
    "outputId": "8c49dba4-26f3-4a25-d1b6-0b69675816ba"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 73963,
     "status": "ok",
     "timestamp": 1607623276478,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "mcO6tljKX9wf",
    "outputId": "d42133da-e6be-457a-e269-7ef0d08f35e0"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73957,
     "status": "ok",
     "timestamp": 1607623276479,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "TIqYYx7AX_7c",
    "outputId": "6b956628-c21b-4aa7-fa70-a471a0fa1404"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp_Q2mbNq7SI"
   },
   "source": [
    "### **2.3 Fitting Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logit \n",
    "def fit_Logit(self, X, y):\n",
    "    return LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "#####ML Models ##################\n",
    "def fit_knn(self, X, y):\n",
    "     return KNeighborsClassifier(3).fit(X,y)\n",
    "\n",
    "\n",
    "def fit_random_forest(self, X,y):\n",
    "    return RandomForestClassifier(max_depth=5, n_estimators=240, max_features=1).fit(X,y)\n",
    "\n",
    "def fit_ADA(self,X,y):\n",
    "    return  AdaBoostClassifier().fit(X,y)\n",
    "\n",
    "def fit_XGB(self, X,y):\n",
    "    return xgb.XGBClassifier().fit(X,y)\n",
    "    \n",
    "\n",
    "# add to Usage agentt\n",
    "setattr(Usage_Agent, 'fit_Logit', fit_Logit)\n",
    "del fit_Logit \n",
    "\n",
    "setattr(Usage_Agent, 'fit_knn', fit_knn)\n",
    "del fit_knn\n",
    "\n",
    "setattr(Usage_Agent, 'fit_random_forest', fit_random_forest)\n",
    "del fit_random_forest\n",
    "\n",
    "setattr(Usage_Agent, 'fit_ADA', fit_ADA)\n",
    "del fit_ADA\n",
    "\n",
    "setattr(Usage_Agent, 'fit_XGB', fit_XGB)\n",
    "del fit_XGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, model_type):\n",
    "    model = None\n",
    "    if model_type == \"logit\":\n",
    "        model = self.fit_Logit(X, y)\n",
    "    elif model_type == \"ada\":\n",
    "        model = self.fit_ADA(X,y)\n",
    "    elif model_type == \"knn\":\n",
    "        model = self.fit_knn(X,y)\n",
    "    elif model_type == \"random forest\":\n",
    "        model = self.fit_random_forest(X,y)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = self.fit_XGB(X,y)\n",
    "    else:\n",
    "        raise InputError(\"Unknown model type.\")\n",
    "    return model\n",
    "\n",
    "setattr(Usage_Agent, 'fit', fit)\n",
    "del fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "901Rnq25Y58c"
   },
   "source": [
    "Now that we have the function to perform the split-sampling we can fit the model on training data. For that purpose, we define a Logit-fitting function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMxMWE5zZ1PE"
   },
   "source": [
    "Using this function on the training split, we can train our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74957,
     "status": "ok",
     "timestamp": 1607623277488,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "38OdZOcTrve-",
    "outputId": "d79773c9-60b9-4f27-8dba-aa67c27e07b5"
   },
   "outputs": [],
   "source": [
    "usage = Usage_Agent(df, \"Dishwasher\")\n",
    "model = usage.fit(X_train, y_train, 'logit') # Change it to the model you want to use\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =np.array(X_test).reshape(-1,3)\n",
    "model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, model, X):\n",
    "    import numpy as np\n",
    "    import pandas\n",
    "    res = 3\n",
    "    cols = [\"temp\", \"dwpt\", \"rhum\", \"wdir\", \"wspd\"]\n",
    "    for e in cols:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            if e in X.columns:\n",
    "                res += 1\n",
    "        if isinstance(X, pd.Series):\n",
    "            if e in X.index:\n",
    "                res += 1\n",
    "    X = np.array(X).reshape(-1, res)\n",
    "    if type(model) == sklearn.linear_model.LogisticRegression:\n",
    "        y_hat = model.predict_proba(X)[:,1]\n",
    "    elif type(model) == sklearn.neighbors._classification.KNeighborsClassifier:\n",
    "        y_hat = model.predict_proba(X)[:,1]\n",
    "    elif type(model) == sklearn.ensemble._forest.RandomForestClassifier:\n",
    "        y_hat = model.predict_proba(X)[:,1]\n",
    "    elif type(model) ==  sklearn.ensemble._weight_boosting.AdaBoostClassifier:\n",
    "        y_hat = model.predict_proba(X)[:,1]\n",
    "    elif type(model) == xgboost.sklearn.XGBClassifier:\n",
    "        y_hat = model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "        raise InputError(\"Unknown model type.\")\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "setattr(Usage_Agent, 'predict', predict)\n",
    "del predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(self, y_true, y_hat):\n",
    "    import sklearn.metrics\n",
    "    return sklearn.metrics.roc_auc_score(y_true, y_hat)\n",
    "\n",
    "# add to Activity agent\n",
    "setattr(Usage_Agent, 'auc', auc)\n",
    "del auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(self, df, date, train_start=\"2013-11-01\"):\n",
    "        df.columns = df.columns.map(str)\n",
    "        select_vars = [\n",
    "            self.device + \"_usage\",\n",
    "            self.device + \"_usage_lag_1\",\n",
    "            self.device + \"_usage_lag_2\",\n",
    "            \"active_last_2_days\",\n",
    "        ]\n",
    "        # Add weather possibly\n",
    "        if \"temp\" in df.columns:\n",
    "            select_vars.append(\"temp\")\n",
    "            df[\"temp\"].fillna(method=\"backfill\", inplace=True)\n",
    "        if \"dwpt\" in df.columns:\n",
    "            select_vars.append(\"dwpt\")\n",
    "            df[\"dwpt\"].fillna(method=\"backfill\", inplace=True)\n",
    "        if \"rhum\" in df.columns:\n",
    "            select_vars.append(\"rhum\")\n",
    "            df[\"rhum\"].fillna(method=\"backfill\", inplace=True)\n",
    "        if \"wdir\" in df.columns:\n",
    "            select_vars.append(\"wdir\")\n",
    "            df[\"wdir\"].fillna(method=\"backfill\", inplace=True)\n",
    "        if \"wspd\" in df.columns:\n",
    "            select_vars.append(\"wspd\")\n",
    "            df[\"wspd\"].fillna(method=\"backfill\", inplace=True)\n",
    "\n",
    "        df = df[select_vars]\n",
    "        X_train = df.loc[train_start:date, df.columns != self.device + \"_usage\"]\n",
    "        y_train = df.loc[train_start:date, df.columns == self.device + \"_usage\"]\n",
    "        X_test = df.loc[pd.to_datetime(date), df.columns != self.device + \"_usage\"]\n",
    "        y_test = df.loc[pd.to_datetime(date), df.columns == self.device + \"_usage\"]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "split_params = {\n",
    "            \"train_start\": \"2013-11-01\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_my8fyn4d-k"
   },
   "source": [
    "Once the model is fitted to the training data, a prediction can be made for the test day. This prediction function is defined in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for local in range(1):\n",
    "    print(X_test[local])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_start=\"2014-01-01\"\n",
    "predict_end = -1\n",
    "model_type = 'ada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/556 [00:00<?, ?it/s]The explainability approaches in the Usage Agent are being evaluated for model: logit\n",
      "Start evaluation with LIME and SHAP\n",
      "100%|██████████| 556/556 [14:16<00:00,  1.54s/it]Mean time nedded by appraoches: 0.796832178565238 0.5186920127561016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "xai= True\n",
    "weather_sel= False\n",
    "return_errors = True\n",
    "dates = pd.DataFrame(df.index)\n",
    "dates = dates.set_index(df.index)[\"Time\"]\n",
    "predict_start = pd.to_datetime(predict_start)\n",
    "predict_end = (\n",
    "    pd.to_datetime(dates.iloc[predict_end])\n",
    "    if type(predict_end) == int\n",
    "    else pd.to_datetime(predict_end)\n",
    ")\n",
    "dates = dates.loc[predict_start:predict_end]\n",
    "y_true = []\n",
    "y_hat_train = {}\n",
    "y_hat_test = []\n",
    "y_hat_lime = []\n",
    "y_hat_shap = []\n",
    "auc_train_dict = {}\n",
    "auc_test = []\n",
    "xai_time_lime = []\n",
    "xai_time_shap = []\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    " \n",
    "if xai:\n",
    "    print('The explainability approaches in the Usage Agent are being evaluated for model: ' + str     (model_type))\n",
    "    print('Start evaluation with LIME and SHAP')\n",
    "    import time\n",
    "    import lime\n",
    "    import shap as shap\n",
    "    from lime import lime_tabular\n",
    "\n",
    "    for date in tqdm(dates.index):\n",
    "        errors = {}\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = usage.train_test_split(\n",
    "                df, date, train_start=\"2013-11-01\"\n",
    "            )\n",
    "            # fit model\n",
    "            model = usage.fit(X_train, y_train, model_type)\n",
    "            # predict\n",
    "            y_hat_train.update({date: usage.predict(model, X_train)})\n",
    "            y_hat_test += list(usage.predict(model, X_test))\n",
    "            # evaluate train data\n",
    "            auc_train_dict.update(\n",
    "                {date: usage.auc(y_train, list(y_hat_train.values())[-1])}\n",
    "            )\n",
    "            y_true += list(y_test)\n",
    "            start_time = time.time()\n",
    "\n",
    "            explainer = lime_tabular.LimeTabularExplainer(training_data=np.array(X_train),\n",
    "                                                                mode=\"classification\",\n",
    "                                                                feature_names=X_train.columns,\n",
    "                                                                categorical_features=[0])\n",
    "\n",
    "            if model_type == \"xgboost\":\n",
    "                import warnings\n",
    "                warnings.filterwarnings('ignore')\n",
    "                exp = explainer.explain_instance(X_test.values, model.predict_proba)\n",
    "            else:\n",
    "                exp = explainer.explain_instance(data_row=X_test, predict_fn=model.predict_proba)\n",
    "\n",
    "           \n",
    "\n",
    "            y_hat_lime += list(exp.local_pred)\n",
    "\n",
    "            # take time for each day:\n",
    "            end_time = time.time()\n",
    "            difference_time = end_time - start_time\n",
    "\n",
    "            xai_time_lime.append(difference_time)\n",
    "\n",
    "             # SHAP\n",
    "            # =========================================================================\n",
    "            start_time = time.time()\n",
    "\n",
    "            if model_type == \"logit\":\n",
    "                #option: apply kmeans first for faster computation\n",
    "                X_train_summary = shap.kmeans(X_train, 10)\n",
    "                explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "                #without kmeans:\n",
    "                #explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "\n",
    "            elif model_type == \"ada\":\n",
    "                X_train_summary = shap.kmeans(X_train, 10)\n",
    "                explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "\n",
    "            elif model_type == \"knn\":\n",
    "                X_train_summary = shap.kmeans(X_train, 10)\n",
    "                explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "\n",
    "            elif model_type == \"random forest\":\n",
    "                explainer = shap.TreeExplainer(model, X_train)\n",
    "\n",
    "            elif model_type == \"xgboost\":\n",
    "                explainer = shap.TreeExplainer(model, X_train, model_output='predict_proba')\n",
    "            else:\n",
    "                raise InputError(\"Unknown model type.\")\n",
    "\n",
    "            base_value = explainer.expected_value[1]  # the mean prediction\n",
    "\n",
    "            \n",
    "            #for local in range(len(X_test)):  # same as above\n",
    "\n",
    "            shap_values = explainer.shap_values(\n",
    "                X_test)\n",
    "            # hier theoretisch ganzes test set prediction statt for loop möglich\n",
    "            contribution_to_class_1 = np.array(shap_values).sum(axis=1)[1]  # the red part of the diagram\n",
    "            shap_prediction = base_value + contribution_to_class_1\n",
    "            #print(shap_prediction)\n",
    "            # Prediction from XAI:\n",
    "            y_hat_shap.append(shap_prediction)\n",
    "            #print(y_hat_shap)\n",
    "\n",
    "\n",
    "            # take time for each day:\n",
    "            end_time = time.time()\n",
    "            difference_time = end_time - start_time\n",
    "            xai_time_shap.append(difference_time)\n",
    "            #print(xai_time_shap)  \n",
    "        except Exception as e:\n",
    "            errors[date] = e\n",
    "\n",
    "auc_test = usage.auc(y_true, y_hat_test)\n",
    "auc_train = np.mean(list(auc_train_dict.values()))\n",
    "predictions_list.append(y_true)\n",
    "predictions_list.append(y_hat_test)\n",
    "predictions_list.append(y_hat_lime)\n",
    "predictions_list.append(y_hat_shap)\n",
    "\n",
    "# Efficiency\n",
    "time_mean_lime = np.mean(xai_time_lime)\n",
    "time_mean_shap = np.mean(xai_time_shap)\n",
    "print('Mean time nedded by appraoches: ' + str(time_mean_lime) + ' ' + str(time_mean_shap))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance(data_row=X_test, predict_fn=model.predict_proba).local_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance(data_row=X_test,predict_fn=model.predict_proba).local_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_time_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, model_type, train_start, predict_start=\"2014-01-01\", predict_end=-1, return_errors=False, weather_sel=False, xai=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "            df, model_type, train_start, predict_start=\"2014-01-01\", predict_end=-1, return_errors=False,\n",
    "            weather_sel=False, xai=False\n",
    "    ):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        dates = pd.DataFrame(df.index)\n",
    "        dates = dates.set_index(df.index)[\"Time\"]\n",
    "        predict_start = pd.to_datetime(predict_start)\n",
    "        predict_end = (\n",
    "            pd.to_datetime(dates.iloc[predict_end])\n",
    "            if type(predict_end) == int\n",
    "            else pd.to_datetime(predict_end)\n",
    "        )\n",
    "        dates = dates.loc[predict_start:predict_end]\n",
    "        y_true = []\n",
    "        y_hat_train = {}\n",
    "        y_hat_test = []\n",
    "        y_hat_lime = []\n",
    "        y_hat_shap = []\n",
    "        auc_train_dict = {}\n",
    "        auc_test = []\n",
    "        xai_time_lime = []\n",
    "        xai_time_shap = []\n",
    "        \n",
    "        predictions_list = []\n",
    "\n",
    "        if weather_sel:\n",
    "            # Add Weather\n",
    "            ################################\n",
    "            from meteostat import Point, Daily\n",
    "            from datetime import datetime, timedelta\n",
    "\n",
    "            lough = Point(52.766593, -1.223511)\n",
    "            time = df.index.to_series(name=\"time\").tolist()\n",
    "            start = time[0]\n",
    "            end = time[len(time) - 1]\n",
    "            weather = Daily(lough, start, end)\n",
    "            weather = weather.fetch()\n",
    "\n",
    "            from sklearn.impute import KNNImputer\n",
    "            import numpy as np\n",
    "\n",
    "            headers = weather.columns.values\n",
    "\n",
    "            empty_train_columns = []\n",
    "            for col in weather.columns.values:\n",
    "                if sum(weather[col].isnull()) == weather.shape[0]:\n",
    "                    empty_train_columns.append(col)\n",
    "            headers = np.setdiff1d(headers, empty_train_columns)\n",
    "\n",
    "            imputer = KNNImputer(missing_values=np.nan, n_neighbors=7, weights=\"distance\")\n",
    "            weather = imputer.fit_transform(weather)\n",
    "            scaler = MinMaxScaler()\n",
    "            weather = scaler.fit_transform(weather)\n",
    "            weather = pd.DataFrame(weather)\n",
    "            weather[\"time\"] = time[0:len(weather)]\n",
    "            df[\"time\"] = time\n",
    "\n",
    "            weather.columns = np.append(headers, \"time\")\n",
    "\n",
    "            df = pd.merge(df, weather, how=\"right\", on=\"time\")\n",
    "            df = df.set_index(\"time\")\n",
    "            # df.drop(\"time\", axis=1, inplace=True)\n",
    "            ################################\n",
    "        if xai:\n",
    "            print('The explainability approaches are being evaluated for model: ' + str(model_type))\n",
    "\n",
    "        \n",
    "        for date in tqdm(dates.index):\n",
    "            errors = {}\n",
    "            try:\n",
    "                X_train, y_train, X_test, y_test = train_test_split(\n",
    "                    df, date, train_start\n",
    "                )\n",
    "                # fit model\n",
    "                model = fit(X_train, y_train, model_type)\n",
    "                # predict\n",
    "                y_hat_train.update({date: predict(model, X_train)})\n",
    "                y_hat_test += list(predict(model, X_test))\n",
    "                # evaluate train data\n",
    "                auc_train_dict.update(\n",
    "                    {date: sklearn.metrics.roc_auc_score(y_train, list(y_hat_train.values())[-1])}\n",
    "                )\n",
    "                y_true += list(y_test)\n",
    "                \n",
    "                if xai:\n",
    "                    import time\n",
    "                    import lime\n",
    "                    from lime import lime_tabular\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    if model_type == \"xgboost\":\n",
    "                        booster = model.get_booster()\n",
    "\n",
    "                        explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                                           feature_names=X_train.columns,\n",
    "                                                                           kernel_width=3, verbose=False)\n",
    "                        #print(explainer)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        explainer = lime_tabular.LimeTabularExplainer(training_data=np.array(X_train),\n",
    "                                                                      mode=\"classification\",\n",
    "                                                                      feature_names=X_train.columns,\n",
    "                                                                      categorical_features=[0])\n",
    "\n",
    "                    # optional to do: only select the instances that are predicted to be 1\n",
    "                    # for local in\n",
    "                    #to do:\n",
    "                    for local in range(2):  # replace 3 with when is works: len(X_test)\n",
    "                        # to do: hier weiter\n",
    "                        #print('Instance: ' + str(local))\n",
    "                        # still predict_proba since also used in other function: treshold dependent outcome\n",
    "                        if model_type == \"xgboost\":\n",
    "                            exp = explainer.explain_instance(X_test[local, :], model.predict_proba)\n",
    "                        else:\n",
    "                            exp = explainer.explain_instance(data_row=X_test[local], predict_fn=model.predict_proba)\n",
    "\n",
    "                        y_hat_lime += list(exp.local_pred)\n",
    "\n",
    "                    # take time for each day:\n",
    "                    end_time = time.time()\n",
    "                    difference_time = end_time - start_time\n",
    "\n",
    "                    xai_time_lime.append(difference_time)\n",
    "\n",
    "                    #print('SHAP: ')\n",
    "\n",
    "                    import shap as shap\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # to do: add for all models\n",
    "                    if model_type == \"logit\":\n",
    "                        #option: apply kmeans first for faster computation\n",
    "                        X_train_summary = shap.kmeans(X_train, 10)\n",
    "                        explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "                        #without kmeans:\n",
    "                        #explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "\n",
    "                    elif model_type == \"ada\":\n",
    "                        X_train_summary = shap.kmeans(X_train, 10)\n",
    "                        explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "\n",
    "                    elif model_type == \"knn\":\n",
    "                        X_train_summary = shap.kmeans(X_train, 10)\n",
    "                        explainer = shap.KernelExplainer(model.predict_proba, X_train_summary)\n",
    "\n",
    "                    elif model_type == \"random forest\":\n",
    "                        explainer = shap.TreeExplainer(model, X_train)\n",
    "\n",
    "                    elif model_type == \"xgboost\":\n",
    "                        explainer = shap.TreeExplainer(model, X_train, model_output='predict_proba')\n",
    "                        #print(explainer)\n",
    "                    else:\n",
    "                        raise InputError(\"Unknown model type.\")\n",
    "\n",
    "                    base_value = explainer.expected_value[1]  # the mean prediction\n",
    "\n",
    "                    #to do:\n",
    "                    for local in range(len(X_test)):  # replace 3 with when is works: len(X_test)\n",
    "\n",
    "                        shap_values = explainer.shap_values(\n",
    "                            X_test[local])\n",
    "                        # hier theoretisch ganzes test set prediction statt for loop möglich\n",
    "                        contribution_to_class_1 = np.array(shap_values).sum(axis=1)[1]  # the red part of the diagram\n",
    "                        shap_prediction = base_value + contribution_to_class_1\n",
    "                        #print(shap_prediction)\n",
    "                        # Prediction from XAI:\n",
    "                        y_hat_shap.append(shap_prediction)\n",
    "                        #print(y_hat_shap)\n",
    "\n",
    "\n",
    "                    # take time for each day:\n",
    "                    end_time = time.time()\n",
    "                    difference_time = end_time - start_time\n",
    "                    xai_time_shap.append(difference_time)\n",
    "                    #print(xai_time_shap)\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors[date] = e\n",
    "\n",
    "        auc_test = sklearn.metrics.roc_auc_score(y_true, y_hat_test)\n",
    "        auc_train = np.mean(list(auc_train_dict.values()))\n",
    "        predictions_list.append(y_true)\n",
    "        predictions_list.append(y_hat_test)\n",
    "        predictions_list.append(y_hat_lime)\n",
    "        predictions_list.append(y_hat_shap)\n",
    "        \n",
    "        # Efficiency\n",
    "        time_mean_lime = np.mean(xai_time_lime)\n",
    "        time_mean_shap = np.mean(xai_time_shap)\n",
    "        print('Mean time nedded by appraoches: ' + str(time_mean_lime) + str(time_mean_shap))\n",
    "        \n",
    "        if return_errors:\n",
    "            return auc_train, auc_test, auc_train_dict, time_mean_lime, time_mean_shap, predictions_list, errors\n",
    "        else:\n",
    "            return auc_train, auc_test, auc_train_dict, time_mean_lime, time_mean_shap, predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"logit\"\n",
    "train_start = \"2013-01-11\"\n",
    "evaluate(df, model_type, train_start, predict_start=\"2014-01-01\", predict_end=-1, return_errors=False,\n",
    "            weather_sel=False, xai=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLOHQvFv_GXI"
   },
   "source": [
    "### **2.4 Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuQrieip1S6X"
   },
   "source": [
    "Finally, we wrap up all the previously defined functions into the **pipeline** function. This allows to generate a prediction by simply inputting:\n",
    "* the pre-processed usage data\n",
    "* the prediction date\n",
    "* the model type (limited to logit for now)\n",
    "* the date at which the model has started to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74939,
     "status": "ok",
     "timestamp": 1607623277492,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "_i8kTixg_Nvu"
   },
   "outputs": [],
   "source": [
    "def pipeline(self, df, date, model_type, train_start):\n",
    "    X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n",
    "\n",
    "    # fit model\n",
    "    model = self.fit(X_train, y_train, model_type)\n",
    "\n",
    "    # predict\n",
    "    return self.predict(model, X_test)\n",
    "\n",
    "# add to Activity agent\n",
    "setattr(Usage_Agent, 'pipeline', pipeline)\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAexapts5BYv"
   },
   "source": [
    "A prediction for the \"2013-12-08\" based on the data starting on the '2013-11-01' can finally be made for the device with which we initialized the class (here: \"Dishwasher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74936,
     "status": "ok",
     "timestamp": 1607623277493,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "CXUmgFdK_Z03",
    "outputId": "dd3f42a0-89fe-437d-b00f-cde1b4bb9030"
   },
   "outputs": [],
   "source": [
    "date = \"2013-12-08\"\n",
    "train_start = '2013-11-01'\n",
    "usage.pipeline(df, date, 'logit', train_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5cE1DjO6ojI"
   },
   "source": [
    "### **2.5 Model Evaluation**\n",
    "\n",
    "Finally, we want to assess the accuracy of our model before using it in the Recommendation Agent. \n",
    "\n",
    "A drawback to our approach is that we are not able to apply conventional model evaluation techniques to our model. We will train our model for each day to account for newly available information. Hence, we have different train and test sets for each day and for each day different performance metric based on the respective data sets. Therefore, we created our own evaluation function. \n",
    "\n",
    "Our evaluation function will build a model, fit the model and predict the target for each day for a given prediction period. For each day and fitted model it will calculate a performance metric on the train data. We chose the Area Under the Receiver Operating Characteristic Curve (AUC) as performance metric for our binary classification task. As in our case the test data is only the current date to be predicted, we calculate the test AUC over the usage probabilities of all day after all days have been predicted. To summarize the train AUC in one score, we apply an average over all calculated train AUC scores (Note: This approach is the same as for the activity predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74932,
     "status": "ok",
     "timestamp": 1607623277494,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "GumQPIhDOM5m"
   },
   "outputs": [],
   "source": [
    "def auc(self, y_true, y_hat):\n",
    "    import sklearn.metrics\n",
    "    return sklearn.metrics.roc_auc_score(y_true, y_hat)\n",
    "\n",
    "# add to Activity agent\n",
    "setattr(Usage_Agent, 'auc', auc)\n",
    "del auc\n",
    "\n",
    "def evaluate(self, df, model_type, train_start, predict_start='2014-01-01', predict_end=-1):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dates = pd.DataFrame(df.index)\n",
    "    dates = dates.set_index(df.index)['time']\n",
    "    predict_start = pd.to_datetime(predict_start)\n",
    "    predict_end = pd.to_datetime(dates.iloc[predict_end]) if type(predict_end) == int else pd.to_datetime(predict_end)\n",
    "    dates = dates.loc[predict_start:predict_end]\n",
    "    y_true = []\n",
    "    y_hat_train = {}\n",
    "    y_hat_test = []\n",
    "    auc_train_dict = {}\n",
    "    auc_test = []\n",
    "\n",
    "    for date in dates.index:\n",
    "        # train test split\n",
    "        #train_test_split(self, df, date, train_start='2013-11-01', test_delta='all', target='activity')\n",
    "        X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n",
    "\n",
    "        # fit model\n",
    "        model = self.fit( X_train, y_train, model_type)\n",
    "\n",
    "        # predict\n",
    "        y_hat_train.update({date: self.predict(model, X_train)})\n",
    "        y_hat_test += list(self.predict(model, X_test))\n",
    "\n",
    "        # evaluate train data\n",
    "        auc_train_dict.update({date: self.auc(y_train, list(y_hat_train.values())[-1])})\n",
    "        \n",
    "        y_true += list(y_test)\n",
    "    \n",
    "    auc_test = self.auc(y_true, y_hat_test)\n",
    "    auc_train = np.mean(list(auc_train_dict.values()))\n",
    "\n",
    "    return auc_train, auc_test, auc_train_dict\n",
    "\n",
    "\n",
    "# add to Activity agent\n",
    "setattr(Usage_Agent, 'evaluate', evaluate)\n",
    "del evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYtHJpERPikJ"
   },
   "source": [
    "Finally, we can evaluate the simple Logit model for the \"Dishwasher\", for instance for all predictions after the \"2014-08-01\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79708,
     "status": "ok",
     "timestamp": 1607623282273,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "xqaxt7-b9SfE",
    "outputId": "86e00cfb-e9fc-4ba2-cc23-a33c7a1c2755"
   },
   "outputs": [],
   "source": [
    "auc_train, auc_test, auc_train_dict = usage.evaluate(df, \"logit\", '2013-11-01', predict_start='2014-08-01', predict_end= -1)\n",
    "print(\"mean_auc_on_train = \"+ str(auc_train) + \" | test_auc = \" + str(auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nf8rKaVRNW-"
   },
   "source": [
    "As can be seen above, the model's performance is quite disappointing. It is not surprising that we do not have a very high accuracy, given the little amount of data we have. However, there must be potential for improvment. A first step in that direction would be a proper feature selection methodology taking into account different devices and households. Moreover, there has been a large decrease in model accuracy after changing the pre-processing pipeline methodology. Therefore, it seems that the model is sensitive to the way we detect the devices' activity. In the next steps we should investigate how and why these pre-processing steps impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYF4hPeeDFtA"
   },
   "source": [
    "## **Appendix A1: Complete Usage Agent Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 79706,
     "status": "ok",
     "timestamp": 1607623282274,
     "user": {
      "displayName": "Felix Germaine",
      "photoUrl": "",
      "userId": "07033484937606028650"
     },
     "user_tz": -60
    },
    "id": "FllI0166G0lz"
   },
   "outputs": [],
   "source": [
    "class Usage_Agent:\n",
    "    import pandas as pd\n",
    "\n",
    "    def __init__(self, input_df, device):\n",
    "        self.input = input_df\n",
    "        self.device = device\n",
    "\n",
    "    # train test split\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def train_test_split(self, df, date, train_start=\"2013-11-01\"):\n",
    "        select_vars = [\n",
    "            self.device + \"_usage\",\n",
    "            self.device + \"_usage_lag_1\",\n",
    "            self.device + \"_usage_lag_2\",\n",
    "            \"active_last_2_days\",\n",
    "        ]\n",
    "        df = df[select_vars]\n",
    "        X_train = df.loc[train_start:date, df.columns != self.device + \"_usage\"]\n",
    "        y_train = df.loc[train_start:date, df.columns == self.device + \"_usage\"]\n",
    "        X_test = df.loc[date, df.columns != self.device + \"_usage\"]\n",
    "        y_test = df.loc[date, df.columns == self.device + \"_usage\"]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # model training and evaluation\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def fit_smLogit(self, X, y):\n",
    "        import statsmodels.api as sm\n",
    "\n",
    "        return sm.Logit(y, X).fit(disp=False)\n",
    "\n",
    "    #####ML Models ##################\n",
    "    def fit_knn(self, X, y):\n",
    "        return KNeighborsClassifier(3).fit(X,y)\n",
    "\n",
    "    def fit_random_forest(self, X,y):\n",
    "        return RandomForestClassifier(max_depth=5, n_estimators=240, max_features=1).fit(X,y)\n",
    "\n",
    "    def fit_ADA(self,X,y):\n",
    "        return  AdaBoostClassifier().fit(X,y)\n",
    "    def fit_XGB(self, X,y):\n",
    "        return xgb.XGBClassifier().fit(X,y)\n",
    "    ##############################################\n",
    "    def fit(self, X, y, model_type):\n",
    "        model = None\n",
    "        if model_type == \"logit\":\n",
    "            model = self.fit_smLogit(X, y)\n",
    "        elif model_type == \"ada\":\n",
    "            model = self.fit_ADA(X,y)\n",
    "        elif model_type == \"knn\":\n",
    "            model = self.fit_knn(X,y)\n",
    "        elif model_type == \"random forest\":\n",
    "            model = self.fit_random_forest(X,y)\n",
    "        elif model_type == \"xgboost\":\n",
    "           model = self.fit_XGB(X,y)\n",
    "        else:\n",
    "            raise InputError(\"Unknown model type.\")\n",
    "        return model\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    def predict(self, model, X):\n",
    "        import numpy as np\n",
    "        import pandas\n",
    "\n",
    "        if type(X) == pandas.core.series.Series:\n",
    "            X = pd.DataFrame(X).transpose() \n",
    "        else:\n",
    "            X=np.squeeze(np.asarray(X))      \n",
    "\n",
    "        if type(model) == sklearn.linear_model.LogisticRegression:\n",
    "            y_hat = model.predict(X)\n",
    "        elif type(model) == sklearn.neighbors._classification.KNeighborsClassifier:\n",
    "            y_hat = model.predict(X)\n",
    "        elif type(model) == sklearn.ensemble._forest.RandomForestClassifier:\n",
    "            y_hat = model.predict(X)\n",
    "        elif type(model) ==  sklearn.ensemble._weight_boosting.AdaBoostClassifier:\n",
    "            y_hat = model.predict(X)\n",
    "        elif type(model) == xgboost.sklearn.XGBClassifier:\n",
    "            y_hat = model.predict(X)\n",
    "        else:\n",
    "            raise InputError(\"Unknown model type.\")\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def auc(self, y_true, y_hat):\n",
    "        import sklearn.metrics\n",
    "        return sklearn.metrics.roc_auc_score(y_true, y_hat)\n",
    "    \n",
    "    def evaluate(\n",
    "        self, df, model_type, train_start, predict_start=\"2014-01-01\", predict_end=-1, return_errors=False\n",
    "    ):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        dates = pd.DataFrame(df.index)\n",
    "        dates = dates.set_index(df.index)[\"time\"]\n",
    "        predict_start = pd.to_datetime(predict_start)\n",
    "        predict_end = (\n",
    "            pd.to_datetime(dates.iloc[predict_end])\n",
    "            if type(predict_end) == int\n",
    "            else pd.to_datetime(predict_end)\n",
    "        )\n",
    "        dates = dates.loc[predict_start:predict_end]\n",
    "        y_true = []\n",
    "        y_hat_train = {}\n",
    "        y_hat_test = []\n",
    "        auc_train_dict = {}\n",
    "        auc_test = []\n",
    "\n",
    "        for date in tqdm(dates.index):\n",
    "            errors = {}\n",
    "            try:\n",
    "                X_train, y_train, X_test, y_test = self.train_test_split(\n",
    "                    df, date, train_start\n",
    "                )\n",
    "                # fit model\n",
    "                model = self.fit(X_train, y_train, model_type)\n",
    "                # predict\n",
    "                y_hat_train.update({date: self.predict(model, X_train)})\n",
    "                y_hat_test += list(self.predict(model, X_test))\n",
    "                # evaluate train data\n",
    "                auc_train_dict.update(\n",
    "                    {date: self.auc(y_train, list(y_hat_train.values())[-1])}\n",
    "                )\n",
    "                y_true += list(y_test)\n",
    "            except Exception as e:\n",
    "                errors[date] = e\n",
    "\n",
    "        auc_test = self.auc(y_true, y_hat_test)\n",
    "        auc_train = np.mean(list(auc_train_dict.values()))\n",
    "\n",
    "        if return_errors:\n",
    "            return auc_train, auc_test, auc_train_dict, errors\n",
    "        else:\n",
    "            return auc_train, auc_test, auc_train_dict\n",
    "        \n",
    "    # pipeline function: predicting device usage\n",
    "    # -------------------------------------------------------------------------------------------        \n",
    "    def pipeline(self, df, date, model_type, train_start):\n",
    "        X_train, y_train, X_test, y_test = self.train_test_split(df, date, train_start)\n",
    "        model = self.fit(X_train, y_train, model_type)\n",
    "        return self.predict(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03b_Usage_Agent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}